{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (8): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=True)\n",
       "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import xml.etree.ElementTree as ET\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Define paths for ImageNet validation images and annotations\n",
    "IMAGE_DIR = \"/home/kajm20/mnist/ILSVRC/Data/CLS-LOC/val\"  # Path to validation images\n",
    "ANNOTATION_DIR = \"/home/kajm20/mnist/ILSVRC/Annotations/CLS-LOC/val\"  # Path to XML annotations\n",
    "\n",
    "# 2. Define transformations for EfficientNet input (resize, crop, normalize)\n",
    "imagenet_transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # Resize the image to 256x256\n",
    "    transforms.CenterCrop(224),  # Crop the image to 224x224\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet mean/std\n",
    "])\n",
    "\n",
    "# 3. Load the synset mapping\n",
    "synset_mapping_path = \"/home/kajm20/mnist/ILSVRC/LOC_synset_mapping.txt\"\n",
    "wordnet_to_imagenet = {}\n",
    "\n",
    "# Load synset mapping from file\n",
    "with open(synset_mapping_path) as f:\n",
    "    for idx, line in enumerate(f.readlines()):\n",
    "        wordnet_id, _ = line.split(' ', 1)  # Get WordNet ID from the line (skip class name)\n",
    "        wordnet_to_imagenet[wordnet_id] = idx  # Map WordNet ID to class index\n",
    "\n",
    "# 4. Define the custom dataset class\n",
    "class ImageNetValDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Get all annotation file names\n",
    "        self.annotation_files = sorted(os.listdir(annotation_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotation_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get annotation file path\n",
    "        annotation_path = os.path.join(self.annotation_dir, self.annotation_files[idx])\n",
    "        \n",
    "        # Parse XML to extract class label\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "        wordnet_id = root.find(\"object\").find(\"name\").text  # WordNet ID, e.g., 'n01751748'\n",
    "\n",
    "        # Use the synset mapping to convert WordNet ID to ImageNet class index\n",
    "        class_idx = wordnet_to_imagenet.get(wordnet_id, -1)  # Default to -1 if not found (shouldn't happen)\n",
    "\n",
    "        # Get image filename from XML and construct image path\n",
    "        image_filename = root.find(\"filename\").text + \".JPEG\"\n",
    "        image_path = os.path.join(self.image_dir, image_filename)\n",
    "\n",
    "        # Load and transform image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, class_idx\n",
    "\n",
    "# 5. Initialize the dataset and dataloader\n",
    "imagenet_val_dataset = ImageNetValDataset(IMAGE_DIR, ANNOTATION_DIR, transform=imagenet_transform)\n",
    "imagenet_val_loader = DataLoader(imagenet_val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# 6. Define the model (EfficientNet-B0 with pre-trained weights)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.efficientnet_b0(weights='DEFAULT')  # Load pre-trained EfficientNet-B0 model\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 224, 224])\n",
      "torch.Size([2, 1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 8. Evaluate the model on ImageNet validation set\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimagenet_val_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEfficientNet-B0 Top-1 Accuracy on ImageNet: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     14\u001b[0m         correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# Count correct predictions\u001b[39;00m\n\u001b[1;32m     15\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m (\u001b[43mcorrect\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# 7. Define the evaluation function\n",
    "def evaluate_model(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        for images, labels in tqdm(dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images) \n",
    "            _, predicted = torch.max(outputs, 1)  # Get highest probability class\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = (correct / total) * 100\n",
    "    return accuracy\n",
    "\n",
    "# 8. Evaluate the model on ImageNet validation set\n",
    "accuracy = evaluate_model(model, imagenet_val_loader)\n",
    "print(f\"EfficientNet-B0 Top-1 Accuracy on ImageNet: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Forward Methods of All Layers ---\n",
      "\n",
      "\n",
      "---  (EfficientNet) ---\n",
      "\n",
      "    def forward(self, x: Tensor) -> Tensor:\n",
      "        return self._forward_impl(x)\n",
      "\n",
      "\n",
      "--- features (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.1 (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.1.0 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.1.0.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.1.0.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.1.0.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.1.0.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.1.0.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.1.0.block.1 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.1.0.block.1.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.1.0.block.1.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.1.0.block.1.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.1.0.block.1.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.1.0.block.1.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.1.0.block.2 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.1.0.block.2.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.1.0.block.2.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.1.0.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.2 (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.2.0 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.2.0.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.2.0.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.2.0.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.2.0.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.2.0.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.2.0.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.2.0.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.2.0.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.2.0.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.2.0.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.2.0.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.2.0.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.2.0.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.2.0.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.2.0.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.2.0.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.2.0.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.2.0.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.2.0.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.2.1 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.2.1.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.2.1.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.2.1.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.2.1.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.2.1.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.2.1.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.2.1.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.2.1.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.2.1.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.2.1.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.2.1.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.2.1.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.2.1.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.2.1.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.2.1.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.2.1.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.2.1.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.2.1.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.2.1.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.3 (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.3.0 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.3.0.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.3.0.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.3.0.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.3.0.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.3.0.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.3.0.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.3.0.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.3.0.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.3.0.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.3.0.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.3.0.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.3.0.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.3.0.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.3.0.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.3.0.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.3.0.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.3.0.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.3.0.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.3.0.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.3.1 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.3.1.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.3.1.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.3.1.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.3.1.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.3.1.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.3.1.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.3.1.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.3.1.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.3.1.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.3.1.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.3.1.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.3.1.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.3.1.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.3.1.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.3.1.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.3.1.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.3.1.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.3.1.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.3.1.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.4 (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.4.0 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.4.0.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.4.0.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.4.0.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.0.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.4.0.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.4.0.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.4.0.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.0.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.4.0.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.4.0.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.4.0.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.4.0.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.0.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.0.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.4.0.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.4.0.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.4.0.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.0.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.4.0.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.4.1 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.4.1.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.4.1.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.4.1.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.1.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.4.1.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.4.1.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.4.1.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.1.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.4.1.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.4.1.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.4.1.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.4.1.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.1.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.1.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.4.1.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.4.1.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.4.1.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.1.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.4.1.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.4.2 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.4.2.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.4.2.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.4.2.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.2.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.4.2.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.4.2.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.4.2.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.2.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.4.2.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.4.2.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.4.2.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.4.2.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.2.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.2.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.4.2.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.4.2.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.4.2.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.2.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.4.2.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.5 (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.5.0 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.5.0.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.5.0.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.5.0.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.0.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.5.0.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.5.0.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.5.0.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.0.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.5.0.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.5.0.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.5.0.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.5.0.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.0.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.0.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.5.0.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.5.0.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.5.0.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.0.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.5.0.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.5.1 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.5.1.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.5.1.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.5.1.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.1.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.5.1.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.5.1.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.5.1.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.1.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.5.1.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.5.1.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.5.1.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.5.1.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.1.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.1.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.5.1.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.5.1.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.5.1.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.1.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.5.1.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.5.2 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.5.2.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.5.2.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.5.2.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.2.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.5.2.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.5.2.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.5.2.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.2.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.5.2.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.5.2.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.5.2.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.5.2.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.2.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.2.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.5.2.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.5.2.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.5.2.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.2.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.5.2.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.6 (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.0 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.6.0.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.0.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.0.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.0.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.6.0.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.6.0.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.0.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.0.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.6.0.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.6.0.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.6.0.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.6.0.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.0.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.0.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.6.0.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.6.0.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.0.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.0.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.6.0.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.6.1 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.6.1.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.1.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.1.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.1.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.6.1.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.6.1.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.1.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.1.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.6.1.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.6.1.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.6.1.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.6.1.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.1.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.1.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.6.1.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.6.1.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.1.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.1.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.6.1.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.6.2 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.6.2.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.2.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.2.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.2.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.6.2.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.6.2.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.2.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.2.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.6.2.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.6.2.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.6.2.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.6.2.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.2.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.2.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.6.2.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.6.2.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.2.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.2.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.6.2.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.6.3 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.6.3.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.3.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.3.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.3.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.6.3.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.6.3.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.3.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.3.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.6.3.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.6.3.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.6.3.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.6.3.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.3.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.3.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.6.3.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.6.3.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.3.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.3.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.6.3.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.7 (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.7.0 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.7.0.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.7.0.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.7.0.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.7.0.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.7.0.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.7.0.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.7.0.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.7.0.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.7.0.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.7.0.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.7.0.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.7.0.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.7.0.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.7.0.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.7.0.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.7.0.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.7.0.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.7.0.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.7.0.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.8 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.8.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.8.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.8.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- classifier (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- classifier.0 (Dropout) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "\n",
      "--- classifier.1 (Linear) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.linear(input, self.weight, self.bias)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw Image Tensor Shape (Before Normalization): torch.Size([1, 3, 224, 224])\n",
      "\n",
      "--- Layer Activation Shapes ---\n",
      "Conv2d (139326499128336): torch.Size([1, 32, 112, 112])\n",
      "BatchNorm2d (139326559913920): torch.Size([1, 32, 112, 112])\n",
      "SiLU (139326516731472): torch.Size([1, 32, 112, 112])\n",
      "Conv2dNormActivation (139326516723792): torch.Size([1, 32, 112, 112])\n",
      "Conv2d (139326485821584): torch.Size([1, 32, 112, 112])\n",
      "BatchNorm2d (139326559913648): torch.Size([1, 32, 112, 112])\n",
      "SiLU (139326516730272): torch.Size([1, 32, 112, 112])\n",
      "Conv2dNormActivation (139326516731712): torch.Size([1, 32, 112, 112])\n",
      "AdaptiveAvgPool2d (139326516730512): torch.Size([1, 32, 1, 1])\n",
      "Conv2d (139326485822544): torch.Size([1, 8, 1, 1])\n",
      "SiLU (139326516730992): torch.Size([1, 8, 1, 1])\n",
      "Conv2d (139326485809104): torch.Size([1, 32, 1, 1])\n",
      "Sigmoid (139326443186096): torch.Size([1, 32, 1, 1])\n",
      "SqueezeExcitation (139326554502128): torch.Size([1, 32, 112, 112])\n",
      "Conv2d (139326485822224): torch.Size([1, 16, 112, 112])\n",
      "BatchNorm2d (139326559926704): torch.Size([1, 16, 112, 112])\n",
      "Conv2dNormActivation (139326516729552): torch.Size([1, 16, 112, 112])\n",
      "Sequential (139326443185424): torch.Size([1, 16, 112, 112])\n",
      "MBConv (139326516725232): torch.Size([1, 16, 112, 112])\n",
      "Sequential (139326569987696): torch.Size([1, 16, 112, 112])\n",
      "Conv2d (139326491122320): torch.Size([1, 96, 112, 112])\n",
      "BatchNorm2d (139326559912016): torch.Size([1, 96, 112, 112])\n",
      "SiLU (139326516736992): torch.Size([1, 96, 112, 112])\n",
      "Conv2dNormActivation (139326516733632): torch.Size([1, 96, 112, 112])\n",
      "Conv2d (139326506551568): torch.Size([1, 96, 56, 56])\n",
      "BatchNorm2d (139326559927792): torch.Size([1, 96, 56, 56])\n",
      "SiLU (139326559683600): torch.Size([1, 96, 56, 56])\n",
      "Conv2dNormActivation (139326516737232): torch.Size([1, 96, 56, 56])\n",
      "AdaptiveAvgPool2d (139326559683840): torch.Size([1, 96, 1, 1])\n",
      "Conv2d (139326554078928): torch.Size([1, 4, 1, 1])\n",
      "SiLU (139326559684320): torch.Size([1, 4, 1, 1])\n",
      "Conv2d (139326558045840): torch.Size([1, 96, 1, 1])\n",
      "Sigmoid (139326569982320): torch.Size([1, 96, 1, 1])\n",
      "SqueezeExcitation (139326569983216): torch.Size([1, 96, 56, 56])\n",
      "Conv2d (139326558046480): torch.Size([1, 24, 56, 56])\n",
      "BatchNorm2d (139326559926976): torch.Size([1, 24, 56, 56])\n",
      "Conv2dNormActivation (139326559685520): torch.Size([1, 24, 56, 56])\n",
      "Sequential (139326569981648): torch.Size([1, 24, 56, 56])\n",
      "MBConv (139326516732672): torch.Size([1, 24, 56, 56])\n",
      "Conv2d (139326558046800): torch.Size([1, 144, 56, 56])\n",
      "BatchNorm2d (139326516288224): torch.Size([1, 144, 56, 56])\n",
      "SiLU (139326559689360): torch.Size([1, 144, 56, 56])\n",
      "Conv2dNormActivation (139326559687440): torch.Size([1, 144, 56, 56])\n",
      "Conv2d (139326558047120): torch.Size([1, 144, 56, 56])\n",
      "BatchNorm2d (139326516287680): torch.Size([1, 144, 56, 56])\n",
      "SiLU (139326559689600): torch.Size([1, 144, 56, 56])\n",
      "Conv2dNormActivation (139326559686720): torch.Size([1, 144, 56, 56])\n",
      "AdaptiveAvgPool2d (139326559689840): torch.Size([1, 144, 1, 1])\n",
      "Conv2d (139326558047440): torch.Size([1, 6, 1, 1])\n",
      "SiLU (139326559691760): torch.Size([1, 6, 1, 1])\n",
      "Conv2d (139326558050640): torch.Size([1, 144, 1, 1])\n",
      "Sigmoid (139326473572368): torch.Size([1, 144, 1, 1])\n",
      "SqueezeExcitation (139326473571472): torch.Size([1, 144, 56, 56])\n",
      "Conv2d (139326558049680): torch.Size([1, 24, 56, 56])\n",
      "BatchNorm2d (139326516287408): torch.Size([1, 24, 56, 56])\n",
      "Conv2dNormActivation (139326559692000): torch.Size([1, 24, 56, 56])\n",
      "Sequential (139326473573936): torch.Size([1, 24, 56, 56])\n",
      "StochasticDepth (139326559694880): torch.Size([1, 24, 56, 56])\n",
      "MBConv (139326559686480): torch.Size([1, 24, 56, 56])\n",
      "Sequential (139326473574608): torch.Size([1, 24, 56, 56])\n",
      "Conv2d (139326558049360): torch.Size([1, 144, 56, 56])\n",
      "BatchNorm2d (139326516287136): torch.Size([1, 144, 56, 56])\n",
      "SiLU (139326559698240): torch.Size([1, 144, 56, 56])\n",
      "Conv2dNormActivation (139326559697520): torch.Size([1, 144, 56, 56])\n",
      "Conv2d (139326558049040): torch.Size([1, 144, 28, 28])\n",
      "BatchNorm2d (139326516284416): torch.Size([1, 144, 28, 28])\n",
      "SiLU (139326559698480): torch.Size([1, 144, 28, 28])\n",
      "Conv2dNormActivation (139326559692240): torch.Size([1, 144, 28, 28])\n",
      "AdaptiveAvgPool2d (139326559683120): torch.Size([1, 144, 1, 1])\n",
      "Conv2d (139326558045520): torch.Size([1, 6, 1, 1])\n",
      "SiLU (139326559683360): torch.Size([1, 6, 1, 1])\n",
      "Conv2d (139326558045200): torch.Size([1, 144, 1, 1])\n",
      "Sigmoid (139326473577968): torch.Size([1, 144, 1, 1])\n",
      "SqueezeExcitation (139326473577072): torch.Size([1, 144, 28, 28])\n",
      "Conv2d (139330892618064): torch.Size([1, 40, 28, 28])\n",
      "BatchNorm2d (139326516284688): torch.Size([1, 40, 28, 28])\n",
      "Conv2dNormActivation (139326559685280): torch.Size([1, 40, 28, 28])\n",
      "Sequential (139326473579536): torch.Size([1, 40, 28, 28])\n",
      "MBConv (139326559696800): torch.Size([1, 40, 28, 28])\n",
      "Conv2d (139330892617744): torch.Size([1, 240, 28, 28])\n",
      "BatchNorm2d (139326516284960): torch.Size([1, 240, 28, 28])\n",
      "SiLU (139326559687680): torch.Size([1, 240, 28, 28])\n",
      "Conv2dNormActivation (139326559689120): torch.Size([1, 240, 28, 28])\n",
      "Conv2d (139326559834576): torch.Size([1, 240, 28, 28])\n",
      "BatchNorm2d (139326516285232): torch.Size([1, 240, 28, 28])\n",
      "SiLU (139326559688400): torch.Size([1, 240, 28, 28])\n",
      "Conv2dNormActivation (139326559687920): torch.Size([1, 240, 28, 28])\n",
      "AdaptiveAvgPool2d (139326559686960): torch.Size([1, 240, 1, 1])\n",
      "Conv2d (139326559835216): torch.Size([1, 10, 1, 1])\n",
      "SiLU (139326559687200): torch.Size([1, 10, 1, 1])\n",
      "Conv2d (139326559834896): torch.Size([1, 240, 1, 1])\n",
      "Sigmoid (139326473583120): torch.Size([1, 240, 1, 1])\n",
      "SqueezeExcitation (139326473582224): torch.Size([1, 240, 28, 28])\n",
      "Conv2d (139326559833296): torch.Size([1, 40, 28, 28])\n",
      "BatchNorm2d (139326516283328): torch.Size([1, 40, 28, 28])\n",
      "Conv2dNormActivation (139326559690080): torch.Size([1, 40, 28, 28])\n",
      "Sequential (139326232428848): torch.Size([1, 40, 28, 28])\n",
      "StochasticDepth (139326559691040): torch.Size([1, 40, 28, 28])\n",
      "MBConv (139326559688880): torch.Size([1, 40, 28, 28])\n",
      "Sequential (139326232429520): torch.Size([1, 40, 28, 28])\n",
      "Conv2d (139326559833936): torch.Size([1, 240, 28, 28])\n",
      "BatchNorm2d (139326516283600): torch.Size([1, 240, 28, 28])\n",
      "SiLU (139326559693200): torch.Size([1, 240, 28, 28])\n",
      "Conv2dNormActivation (139326559694640): torch.Size([1, 240, 28, 28])\n",
      "Conv2d (139326559832336): torch.Size([1, 240, 14, 14])\n",
      "BatchNorm2d (139326516283872): torch.Size([1, 240, 14, 14])\n",
      "SiLU (139326559693920): torch.Size([1, 240, 14, 14])\n",
      "Conv2dNormActivation (139326559693440): torch.Size([1, 240, 14, 14])\n",
      "AdaptiveAvgPool2d (139326559692480): torch.Size([1, 240, 1, 1])\n",
      "Conv2d (139326559832976): torch.Size([1, 10, 1, 1])\n",
      "SiLU (139326559692720): torch.Size([1, 10, 1, 1])\n",
      "Conv2d (139326559834256): torch.Size([1, 240, 1, 1])\n",
      "Sigmoid (139326232432880): torch.Size([1, 240, 1, 1])\n",
      "SqueezeExcitation (139326232431984): torch.Size([1, 240, 14, 14])\n",
      "Conv2d (139326559833616): torch.Size([1, 80, 14, 14])\n",
      "BatchNorm2d (139326516284144): torch.Size([1, 80, 14, 14])\n",
      "Conv2dNormActivation (139326559698720): torch.Size([1, 80, 14, 14])\n",
      "Sequential (139326232434448): torch.Size([1, 80, 14, 14])\n",
      "MBConv (139326559694400): torch.Size([1, 80, 14, 14])\n",
      "Conv2d (139326560992656): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d (139326516282240): torch.Size([1, 480, 14, 14])\n",
      "SiLU (139326559695600): torch.Size([1, 480, 14, 14])\n",
      "Conv2dNormActivation (139326559695120): torch.Size([1, 480, 14, 14])\n",
      "Conv2d (139326560992336): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d (139326516282512): torch.Size([1, 480, 14, 14])\n",
      "SiLU (139326559698000): torch.Size([1, 480, 14, 14])\n",
      "Conv2dNormActivation (139326559697760): torch.Size([1, 480, 14, 14])\n",
      "AdaptiveAvgPool2d (139326559684560): torch.Size([1, 480, 1, 1])\n",
      "Conv2d (139326560992976): torch.Size([1, 20, 1, 1])\n",
      "SiLU (139326559684800): torch.Size([1, 20, 1, 1])\n",
      "Conv2d (139326560992016): torch.Size([1, 480, 1, 1])\n",
      "Sigmoid (139326232438032): torch.Size([1, 480, 1, 1])\n",
      "SqueezeExcitation (139326232437136): torch.Size([1, 480, 14, 14])\n",
      "Conv2d (139326560991696): torch.Size([1, 80, 14, 14])\n",
      "BatchNorm2d (139326516282784): torch.Size([1, 80, 14, 14])\n",
      "Conv2dNormActivation (139326559685040): torch.Size([1, 80, 14, 14])\n",
      "Sequential (139326232439600): torch.Size([1, 80, 14, 14])\n",
      "StochasticDepth (139326559682640): torch.Size([1, 80, 14, 14])\n",
      "MBConv (139326559695360): torch.Size([1, 80, 14, 14])\n",
      "Conv2d (139326560304528): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d (139326516283056): torch.Size([1, 480, 14, 14])\n",
      "SiLU (139326559686000): torch.Size([1, 480, 14, 14])\n",
      "Conv2dNormActivation (139326559685760): torch.Size([1, 480, 14, 14])\n",
      "Conv2d (139326560294608): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d (139326516281424): torch.Size([1, 480, 14, 14])\n",
      "SiLU (139326559690560): torch.Size([1, 480, 14, 14])\n",
      "Conv2dNormActivation (139326559690320): torch.Size([1, 480, 14, 14])\n",
      "AdaptiveAvgPool2d (139326559690800): torch.Size([1, 480, 1, 1])\n",
      "Conv2d (139326443571792): torch.Size([1, 20, 1, 1])\n",
      "SiLU (139326559688160): torch.Size([1, 20, 1, 1])\n",
      "Conv2d (139326443572112): torch.Size([1, 480, 1, 1])\n",
      "Sigmoid (139326232443184): torch.Size([1, 480, 1, 1])\n",
      "SqueezeExcitation (139326232442288): torch.Size([1, 480, 14, 14])\n",
      "Conv2d (139326443572432): torch.Size([1, 80, 14, 14])\n",
      "BatchNorm2d (139326516281696): torch.Size([1, 80, 14, 14])\n",
      "Conv2dNormActivation (139326559688640): torch.Size([1, 80, 14, 14])\n",
      "Sequential (139326232379472): torch.Size([1, 80, 14, 14])\n",
      "StochasticDepth (139326559691280): torch.Size([1, 80, 14, 14])\n",
      "MBConv (139326559682880): torch.Size([1, 80, 14, 14])\n",
      "Sequential (139326232380144): torch.Size([1, 80, 14, 14])\n",
      "Conv2d (139326443572752): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d (139326516281968): torch.Size([1, 480, 14, 14])\n",
      "SiLU (139326559696080): torch.Size([1, 480, 14, 14])\n",
      "Conv2dNormActivation (139326559695840): torch.Size([1, 480, 14, 14])\n",
      "Conv2d (139326443573072): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d (139326516286048): torch.Size([1, 480, 14, 14])\n",
      "SiLU (139326559693680): torch.Size([1, 480, 14, 14])\n",
      "Conv2dNormActivation (139326559696320): torch.Size([1, 480, 14, 14])\n",
      "AdaptiveAvgPool2d (139326559694160): torch.Size([1, 480, 1, 1])\n",
      "Conv2d (139326443573392): torch.Size([1, 20, 1, 1])\n",
      "SiLU (139326559697280): torch.Size([1, 20, 1, 1])\n",
      "Conv2d (139326443571472): torch.Size([1, 480, 1, 1])\n",
      "Sigmoid (139326232383504): torch.Size([1, 480, 1, 1])\n",
      "SqueezeExcitation (139326232382608): torch.Size([1, 480, 14, 14])\n",
      "Conv2d (139326443569872): torch.Size([1, 112, 14, 14])\n",
      "BatchNorm2d (139326516286320): torch.Size([1, 112, 14, 14])\n",
      "Conv2dNormActivation (139326559697040): torch.Size([1, 112, 14, 14])\n",
      "Sequential (139326232385072): torch.Size([1, 112, 14, 14])\n",
      "MBConv (139326559691520): torch.Size([1, 112, 14, 14])\n",
      "Conv2d (139326442896464): torch.Size([1, 672, 14, 14])\n",
      "BatchNorm2d (139326516286592): torch.Size([1, 672, 14, 14])\n",
      "SiLU (139326560013200): torch.Size([1, 672, 14, 14])\n",
      "Conv2dNormActivation (139326560012000): torch.Size([1, 672, 14, 14])\n",
      "Conv2d (139326442897104): torch.Size([1, 672, 14, 14])\n",
      "BatchNorm2d (139326516286864): torch.Size([1, 672, 14, 14])\n",
      "SiLU (139326560014160): torch.Size([1, 672, 14, 14])\n",
      "Conv2dNormActivation (139326560013920): torch.Size([1, 672, 14, 14])\n",
      "AdaptiveAvgPool2d (139326560017280): torch.Size([1, 672, 1, 1])\n",
      "Conv2d (139326442894224): torch.Size([1, 28, 1, 1])\n",
      "SiLU (139326560010800): torch.Size([1, 28, 1, 1])\n",
      "Conv2d (139326442895184): torch.Size([1, 672, 1, 1])\n",
      "Sigmoid (139326232388656): torch.Size([1, 672, 1, 1])\n",
      "SqueezeExcitation (139326232387760): torch.Size([1, 672, 14, 14])\n",
      "Conv2d (139326442896144): torch.Size([1, 112, 14, 14])\n",
      "BatchNorm2d (139326516285504): torch.Size([1, 112, 14, 14])\n",
      "Conv2dNormActivation (139326560011040): torch.Size([1, 112, 14, 14])\n",
      "Sequential (139326232390224): torch.Size([1, 112, 14, 14])\n",
      "StochasticDepth (139326560012960): torch.Size([1, 112, 14, 14])\n",
      "MBConv (139326560011520): torch.Size([1, 112, 14, 14])\n",
      "Conv2d (139326458883408): torch.Size([1, 672, 14, 14])\n",
      "BatchNorm2d (139326516285776): torch.Size([1, 672, 14, 14])\n",
      "SiLU (139326560016800): torch.Size([1, 672, 14, 14])\n",
      "Conv2dNormActivation (139326560016560): torch.Size([1, 672, 14, 14])\n",
      "Conv2d (139326232475152): torch.Size([1, 672, 14, 14])\n",
      "BatchNorm2d (139330900379584): torch.Size([1, 672, 14, 14])\n",
      "SiLU (139326560015600): torch.Size([1, 672, 14, 14])\n",
      "Conv2dNormActivation (139326560015360): torch.Size([1, 672, 14, 14])\n",
      "AdaptiveAvgPool2d (139326560016080): torch.Size([1, 672, 1, 1])\n",
      "Conv2d (139326232475472): torch.Size([1, 28, 1, 1])\n",
      "SiLU (139326560014640): torch.Size([1, 28, 1, 1])\n",
      "Conv2d (139326232475792): torch.Size([1, 672, 1, 1])\n",
      "Sigmoid (139326232393808): torch.Size([1, 672, 1, 1])\n",
      "SqueezeExcitation (139326232392912): torch.Size([1, 672, 14, 14])\n",
      "Conv2d (139326232476112): torch.Size([1, 112, 14, 14])\n",
      "BatchNorm2d (139326606142432): torch.Size([1, 112, 14, 14])\n",
      "Conv2dNormActivation (139326560014880): torch.Size([1, 112, 14, 14])\n",
      "Sequential (139326232395376): torch.Size([1, 112, 14, 14])\n",
      "StochasticDepth (139326560012240): torch.Size([1, 112, 14, 14])\n",
      "MBConv (139326560011760): torch.Size([1, 112, 14, 14])\n",
      "Sequential (139326243193360): torch.Size([1, 112, 14, 14])\n",
      "Conv2d (139326232476432): torch.Size([1, 672, 14, 14])\n",
      "BatchNorm2d (139326443142912): torch.Size([1, 672, 14, 14])\n",
      "SiLU (139326560010320): torch.Size([1, 672, 14, 14])\n",
      "Conv2dNormActivation (139326560012720): torch.Size([1, 672, 14, 14])\n",
      "Conv2d (139326232476752): torch.Size([1, 672, 7, 7])\n",
      "BatchNorm2d (139326443133392): torch.Size([1, 672, 7, 7])\n",
      "SiLU (139326560013440): torch.Size([1, 672, 7, 7])\n",
      "Conv2dNormActivation (139326560010560): torch.Size([1, 672, 7, 7])\n",
      "AdaptiveAvgPool2d (139326560013680): torch.Size([1, 672, 1, 1])\n",
      "Conv2d (139326232477072): torch.Size([1, 28, 1, 1])\n",
      "SiLU (139326560018000): torch.Size([1, 28, 1, 1])\n",
      "Conv2d (139326232477392): torch.Size([1, 672, 1, 1])\n",
      "Sigmoid (139326243196720): torch.Size([1, 672, 1, 1])\n",
      "SqueezeExcitation (139326243195824): torch.Size([1, 672, 7, 7])\n",
      "Conv2d (139326242832464): torch.Size([1, 192, 7, 7])\n",
      "BatchNorm2d (139326443130672): torch.Size([1, 192, 7, 7])\n",
      "Conv2dNormActivation (139326560018240): torch.Size([1, 192, 7, 7])\n",
      "Sequential (139326243198288): torch.Size([1, 192, 7, 7])\n",
      "MBConv (139326560012480): torch.Size([1, 192, 7, 7])\n",
      "Conv2d (139326242832784): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (139326443140736): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (139326554319008): torch.Size([1, 1152, 7, 7])\n",
      "Conv2dNormActivation (139326560016320): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (139326242833104): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (139326443141008): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (139326554319488): torch.Size([1, 1152, 7, 7])\n",
      "Conv2dNormActivation (139326554319728): torch.Size([1, 1152, 7, 7])\n",
      "AdaptiveAvgPool2d (139326554320928): torch.Size([1, 1152, 1, 1])\n",
      "Conv2d (139326242833424): torch.Size([1, 48, 1, 1])\n",
      "SiLU (139326554323088): torch.Size([1, 48, 1, 1])\n",
      "Conv2d (139326242833744): torch.Size([1, 1152, 1, 1])\n",
      "Sigmoid (139326243201872): torch.Size([1, 1152, 1, 1])\n",
      "SqueezeExcitation (139326243200976): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (139326242834064): torch.Size([1, 192, 7, 7])\n",
      "BatchNorm2d (139326443141280): torch.Size([1, 192, 7, 7])\n",
      "Conv2dNormActivation (139326554324768): torch.Size([1, 192, 7, 7])\n",
      "Sequential (139326243203440): torch.Size([1, 192, 7, 7])\n",
      "StochasticDepth (139326554323328): torch.Size([1, 192, 7, 7])\n",
      "MBConv (139326560015840): torch.Size([1, 192, 7, 7])\n",
      "Conv2d (139326242834384): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (139326443141552): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (139326554324528): torch.Size([1, 1152, 7, 7])\n",
      "Conv2dNormActivation (139326554324288): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (139326242834704): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (139326443139648): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (139326554319248): torch.Size([1, 1152, 7, 7])\n",
      "Conv2dNormActivation (139326554321888): torch.Size([1, 1152, 7, 7])\n",
      "AdaptiveAvgPool2d (139326554321168): torch.Size([1, 1152, 1, 1])\n",
      "Conv2d (139326242835024): torch.Size([1, 48, 1, 1])\n",
      "SiLU (139326554321408): torch.Size([1, 48, 1, 1])\n",
      "Conv2d (139326242835344): torch.Size([1, 1152, 1, 1])\n",
      "Sigmoid (139326243207024): torch.Size([1, 1152, 1, 1])\n",
      "SqueezeExcitation (139326243206128): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (139326242835664): torch.Size([1, 192, 7, 7])\n",
      "BatchNorm2d (139326443139920): torch.Size([1, 192, 7, 7])\n",
      "Conv2dNormActivation (139326554322128): torch.Size([1, 192, 7, 7])\n",
      "Sequential (139326243208592): torch.Size([1, 192, 7, 7])\n",
      "StochasticDepth (139326554322608): torch.Size([1, 192, 7, 7])\n",
      "MBConv (139326554320208): torch.Size([1, 192, 7, 7])\n",
      "Conv2d (139326242835984): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (139326443140192): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (139326554321648): torch.Size([1, 1152, 7, 7])\n",
      "Conv2dNormActivation (139326554323568): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (139326242836304): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (139326443140464): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (139326554319968): torch.Size([1, 1152, 7, 7])\n",
      "Conv2dNormActivation (139326554324048): torch.Size([1, 1152, 7, 7])\n",
      "AdaptiveAvgPool2d (139326554320688): torch.Size([1, 1152, 1, 1])\n",
      "Conv2d (139326242836624): torch.Size([1, 48, 1, 1])\n",
      "SiLU (139326554322848): torch.Size([1, 48, 1, 1])\n",
      "Conv2d (139326242836944): torch.Size([1, 1152, 1, 1])\n",
      "Sigmoid (139326242819216): torch.Size([1, 1152, 1, 1])\n",
      "SqueezeExcitation (139326242818320): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (139326242837264): torch.Size([1, 192, 7, 7])\n",
      "BatchNorm2d (139326443138560): torch.Size([1, 192, 7, 7])\n",
      "Conv2dNormActivation (139326554323808): torch.Size([1, 192, 7, 7])\n",
      "Sequential (139326242820784): torch.Size([1, 192, 7, 7])\n",
      "StochasticDepth (139326554320448): torch.Size([1, 192, 7, 7])\n",
      "MBConv (139326554322368): torch.Size([1, 192, 7, 7])\n",
      "Sequential (139326242821456): torch.Size([1, 192, 7, 7])\n",
      "Conv2d (139326242837584): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (139326443138832): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (139326485999648): torch.Size([1, 1152, 7, 7])\n",
      "Conv2dNormActivation (139326486000848): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (139326242837904): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (139326443139104): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (139326485999168): torch.Size([1, 1152, 7, 7])\n",
      "Conv2dNormActivation (139326485997488): torch.Size([1, 1152, 7, 7])\n",
      "AdaptiveAvgPool2d (139326486002288): torch.Size([1, 1152, 1, 1])\n",
      "Conv2d (139326242838224): torch.Size([1, 48, 1, 1])\n",
      "SiLU (139326486000128): torch.Size([1, 48, 1, 1])\n",
      "Conv2d (139326242838544): torch.Size([1, 1152, 1, 1])\n",
      "Sigmoid (139326242824816): torch.Size([1, 1152, 1, 1])\n",
      "SqueezeExcitation (139326242823920): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (139326242838864): torch.Size([1, 320, 7, 7])\n",
      "BatchNorm2d (139326443139376): torch.Size([1, 320, 7, 7])\n",
      "Conv2dNormActivation (139326485999888): torch.Size([1, 320, 7, 7])\n",
      "Sequential (139326242826384): torch.Size([1, 320, 7, 7])\n",
      "MBConv (139326486002528): torch.Size([1, 320, 7, 7])\n",
      "Sequential (139326242827056): torch.Size([1, 320, 7, 7])\n",
      "Conv2d (139326242839184): torch.Size([1, 1280, 7, 7])\n",
      "BatchNorm2d (139326443137472): torch.Size([1, 1280, 7, 7])\n",
      "SiLU (139326485998208): torch.Size([1, 1280, 7, 7])\n",
      "Conv2dNormActivation (139326485996768): torch.Size([1, 1280, 7, 7])\n",
      "Sequential (139326242828400): torch.Size([1, 1280, 7, 7])\n",
      "AdaptiveAvgPool2d (139326485997728): torch.Size([1, 1280, 1, 1])\n",
      "Dropout (139326486003008): torch.Size([1, 1280])\n",
      "Linear (139326486002768): torch.Size([1, 1000])\n",
      "Sequential (139326242829520): torch.Size([1, 1000])\n",
      "EfficientNet (139326485952144): torch.Size([1, 1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNet-B0 Top-1 Accuracy on ImageNet: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import xml.etree.ElementTree as ET\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import inspect  # Import inspect to get source code of forward methods\n",
    "\n",
    "# 1. Define paths for ImageNet validation images and annotations\n",
    "IMAGE_DIR = \"/home/kajm20/mnist/ILSVRC/Data/CLS-LOC/val\"  \n",
    "ANNOTATION_DIR = \"/home/kajm20/mnist/ILSVRC/Annotations/CLS-LOC/val\"  \n",
    "\n",
    "# 2. Define transformations for EfficientNet input\n",
    "imagenet_transform = transforms.Compose([\n",
    "    transforms.Resize(256),  \n",
    "    transforms.CenterCrop(224),  \n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n",
    "])\n",
    "\n",
    "# 3. Load the synset mapping\n",
    "synset_mapping_path = \"/home/kajm20/mnist/ILSVRC/LOC_synset_mapping.txt\"\n",
    "wordnet_to_imagenet = {}\n",
    "\n",
    "with open(synset_mapping_path) as f:\n",
    "    for idx, line in enumerate(f.readlines()):\n",
    "        wordnet_id, _ = line.split(' ', 1)\n",
    "        wordnet_to_imagenet[wordnet_id] = idx  \n",
    "\n",
    "# 4. Define the custom dataset class\n",
    "class ImageNetValDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.transform = transform\n",
    "        self.annotation_files = sorted(os.listdir(annotation_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotation_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation_path = os.path.join(self.annotation_dir, self.annotation_files[idx])\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "        wordnet_id = root.find(\"object\").find(\"name\").text  \n",
    "\n",
    "        class_idx = wordnet_to_imagenet.get(wordnet_id, -1)  \n",
    "        image_filename = root.find(\"filename\").text + \".JPEG\"\n",
    "        image_path = os.path.join(self.image_dir, image_filename)\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, class_idx\n",
    "\n",
    "# 5. Initialize the dataset and dataloader\n",
    "imagenet_val_dataset = ImageNetValDataset(IMAGE_DIR, ANNOTATION_DIR, transform=imagenet_transform)\n",
    "imagenet_val_loader = DataLoader(imagenet_val_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "# 6. Define the model (EfficientNet-B0 with pre-trained weights)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.efficientnet_b0(weights='DEFAULT')  \n",
    "model.to(device)\n",
    "model.eval()  \n",
    "\n",
    "# 7. Dictionaries to store activations\n",
    "activation_shapes = {}  # Stores activation shapes\n",
    "activation_values = {}  # Stores activation values\n",
    "\n",
    "# 8. Hook function to store activations for **ALL** layers\n",
    "def hook_fn(module, input, output):\n",
    "    layer_name = f\"{module.__class__.__name__} ({id(module)})\" \n",
    "    activation_shapes[layer_name] = output.shape  # Store shape\n",
    "    activation_values[layer_name] = output.detach().cpu()  # Store values (moved to CPU)\n",
    "\n",
    "# 9. Recursively register hooks for **all** relevant layers\n",
    "for name, layer in model.named_modules():\n",
    "    layer.register_forward_hook(hook_fn)\n",
    "\n",
    "# 10. Print the forward methods of all layers\n",
    "def print_forward_methods(model):\n",
    "    print(\"\\n--- Forward Methods of All Layers ---\\n\")\n",
    "    for name, layer in model.named_modules():\n",
    "        if hasattr(layer, \"forward\"):  # Ensure the layer has a forward method\n",
    "            try:\n",
    "                forward_code = inspect.getsource(layer.forward)\n",
    "                print(f\"\\n--- {name} ({layer.__class__.__name__}) ---\\n\")\n",
    "                print(forward_code)\n",
    "            except Exception as e:\n",
    "                print(f\"\\n--- {name} ({layer.__class__.__name__}) ---\\n\")\n",
    "                print(f\"Could not retrieve forward method: {e}\")\n",
    "\n",
    "# Call the function to print forward methods\n",
    "print_forward_methods(model)\n",
    "\n",
    "# 11. Define the evaluation function\n",
    "def evaluate_model(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for images, labels in tqdm(dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            activation_shapes.clear()  \n",
    "            activation_values.clear()  \n",
    "            \n",
    "            print(f\"\\nRaw Image Tensor Shape (Before Normalization): {images.shape}\")  \n",
    "            \n",
    "            outputs = model(images)  \n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)  \n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Print activation shapes\n",
    "            print(\"\\n--- Layer Activation Shapes ---\")\n",
    "            for layer, shape in activation_shapes.items():\n",
    "                print(f\"{layer}: {shape}\")\n",
    "\n",
    "            break  \n",
    "\n",
    "    accuracy = (correct / total) * 100\n",
    "    return accuracy\n",
    "\n",
    "# 12. Evaluate the model on ImageNet validation set\n",
    "accuracy = evaluate_model(model, imagenet_val_loader)\n",
    "print(f\"EfficientNet-B0 Top-1 Accuracy on ImageNet: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw Image Tensor Shape (Before Normalization): torch.Size([1, 3, 224, 224])\n",
      "\n",
      "--- Layer Activation Shapes ---\n",
      "Conv2d (139326242846224): torch.Size([1, 32, 112, 112])\n",
      "BatchNorm2d (139326516285232): torch.Size([1, 32, 112, 112])\n",
      "SiLU (139326474465792): torch.Size([1, 32, 112, 112])\n",
      "Conv2dNormActivation (139326474464352): torch.Size([1, 32, 112, 112])\n",
      "Conv2d (139326242845584): torch.Size([1, 32, 112, 112])\n",
      "BatchNorm2d (139326516284960): torch.Size([1, 32, 112, 112])\n",
      "SiLU (139326474466512): torch.Size([1, 32, 112, 112])\n",
      "Conv2dNormActivation (139326474466272): torch.Size([1, 32, 112, 112])\n",
      "AdaptiveAvgPool2d (139326474466752): torch.Size([1, 32, 1, 1])\n",
      "Conv2d (139326242846544): torch.Size([1, 8, 1, 1])\n",
      "SiLU (139326474466992): torch.Size([1, 8, 1, 1])\n",
      "Conv2d (139326242846864): torch.Size([1, 32, 1, 1])\n",
      "Sigmoid (139326242829520): torch.Size([1, 32, 1, 1])\n",
      "SqueezeExcitation (139326569985008): torch.Size([1, 32, 112, 112])\n",
      "Conv2d (139326242847184): torch.Size([1, 16, 112, 112])\n",
      "BatchNorm2d (139326516287136): torch.Size([1, 16, 112, 112])\n",
      "Conv2dNormActivation (139326474467232): torch.Size([1, 16, 112, 112])\n",
      "Sequential (139326242827504): torch.Size([1, 16, 112, 112])\n",
      "MBConv (139326474466032): torch.Size([1, 16, 112, 112])\n",
      "Sequential (139326242826384): torch.Size([1, 16, 112, 112])\n",
      "Conv2d (139326242847504): torch.Size([1, 96, 112, 112])\n",
      "BatchNorm2d (139326516287408): torch.Size([1, 96, 112, 112])\n",
      "SiLU (139326474468192): torch.Size([1, 96, 112, 112])\n",
      "Conv2dNormActivation (139326474467952): torch.Size([1, 96, 112, 112])\n",
      "Conv2d (139326242847824): torch.Size([1, 96, 56, 56])\n",
      "BatchNorm2d (139326516287680): torch.Size([1, 96, 56, 56])\n",
      "SiLU (139326474468672): torch.Size([1, 96, 56, 56])\n",
      "Conv2dNormActivation (139326474468432): torch.Size([1, 96, 56, 56])\n",
      "AdaptiveAvgPool2d (139326474468912): torch.Size([1, 96, 1, 1])\n",
      "Conv2d (139326458883408): torch.Size([1, 4, 1, 1])\n",
      "SiLU (139326474401216): torch.Size([1, 4, 1, 1])\n",
      "Conv2d (139326232477392): torch.Size([1, 96, 1, 1])\n",
      "Sigmoid (139326443186320): torch.Size([1, 96, 1, 1])\n",
      "SqueezeExcitation (139326242824144): torch.Size([1, 96, 56, 56])\n",
      "Conv2d (139326232477072): torch.Size([1, 24, 56, 56])\n",
      "BatchNorm2d (139326559913648): torch.Size([1, 24, 56, 56])\n",
      "Conv2dNormActivation (139326474400496): torch.Size([1, 24, 56, 56])\n",
      "Sequential (139326569980976): torch.Size([1, 24, 56, 56])\n",
      "MBConv (139326474467712): torch.Size([1, 24, 56, 56])\n",
      "Conv2d (139326232475472): torch.Size([1, 144, 56, 56])\n",
      "BatchNorm2d (139326559926704): torch.Size([1, 144, 56, 56])\n",
      "SiLU (139326558328768): torch.Size([1, 144, 56, 56])\n",
      "Conv2dNormActivation (139326558334768): torch.Size([1, 144, 56, 56])\n",
      "Conv2d (139326242844944): torch.Size([1, 144, 56, 56])\n",
      "BatchNorm2d (139326559926976): torch.Size([1, 144, 56, 56])\n",
      "SiLU (139326485959440): torch.Size([1, 144, 56, 56])\n",
      "Conv2dNormActivation (139326516729792): torch.Size([1, 144, 56, 56])\n",
      "AdaptiveAvgPool2d (139326232544272): torch.Size([1, 144, 1, 1])\n",
      "Conv2d (139326242844624): torch.Size([1, 6, 1, 1])\n",
      "SiLU (139326559689120): torch.Size([1, 6, 1, 1])\n",
      "Conv2d (139326242843664): torch.Size([1, 144, 1, 1])\n",
      "Sigmoid (139326473575952): torch.Size([1, 144, 1, 1])\n",
      "SqueezeExcitation (139326473575504): torch.Size([1, 144, 56, 56])\n",
      "Conv2d (139326242845264): torch.Size([1, 24, 56, 56])\n",
      "BatchNorm2d (139326559927792): torch.Size([1, 24, 56, 56])\n",
      "Conv2dNormActivation (139326559684080): torch.Size([1, 24, 56, 56])\n",
      "Sequential (139326473575728): torch.Size([1, 24, 56, 56])\n",
      "StochasticDepth (139326560017280): torch.Size([1, 24, 56, 56])\n",
      "MBConv (139326474388976): torch.Size([1, 24, 56, 56])\n",
      "Sequential (139326473576848): torch.Size([1, 24, 56, 56])\n",
      "Conv2d (139326242845904): torch.Size([1, 144, 56, 56])\n",
      "BatchNorm2d (139326559926432): torch.Size([1, 144, 56, 56])\n",
      "SiLU (139326554322848): torch.Size([1, 144, 56, 56])\n",
      "Conv2dNormActivation (139326554322368): torch.Size([1, 144, 56, 56])\n",
      "Conv2d (139326242842704): torch.Size([1, 144, 28, 28])\n",
      "BatchNorm2d (139326559912016): torch.Size([1, 144, 28, 28])\n",
      "SiLU (139326486000848): torch.Size([1, 144, 28, 28])\n",
      "Conv2dNormActivation (139326485997728): torch.Size([1, 144, 28, 28])\n",
      "AdaptiveAvgPool2d (139326474461472): torch.Size([1, 144, 1, 1])\n",
      "Conv2d (139326242843024): torch.Size([1, 6, 1, 1])\n",
      "SiLU (139326474462192): torch.Size([1, 6, 1, 1])\n",
      "Conv2d (139326242843344): torch.Size([1, 144, 1, 1])\n",
      "Sigmoid (139326473580208): torch.Size([1, 144, 1, 1])\n",
      "SqueezeExcitation (139326473582000): torch.Size([1, 144, 28, 28])\n",
      "Conv2d (139326242842384): torch.Size([1, 40, 28, 28])\n",
      "BatchNorm2d (139326606142432): torch.Size([1, 40, 28, 28])\n",
      "Conv2dNormActivation (139326474463392): torch.Size([1, 40, 28, 28])\n",
      "Sequential (139326473583568): torch.Size([1, 40, 28, 28])\n",
      "MBConv (139326560012000): torch.Size([1, 40, 28, 28])\n",
      "Conv2d (139326242841744): torch.Size([1, 240, 28, 28])\n",
      "BatchNorm2d (139330900379584): torch.Size([1, 240, 28, 28])\n",
      "SiLU (139326476190256): torch.Size([1, 240, 28, 28])\n",
      "Conv2dNormActivation (139326476190016): torch.Size([1, 240, 28, 28])\n",
      "Conv2d (139326242841104): torch.Size([1, 240, 28, 28])\n",
      "BatchNorm2d (139326443133392): torch.Size([1, 240, 28, 28])\n",
      "SiLU (139326476190736): torch.Size([1, 240, 28, 28])\n",
      "Conv2dNormActivation (139326476190496): torch.Size([1, 240, 28, 28])\n",
      "AdaptiveAvgPool2d (139326476190976): torch.Size([1, 240, 1, 1])\n",
      "Conv2d (139326242842064): torch.Size([1, 10, 1, 1])\n",
      "SiLU (139326476191216): torch.Size([1, 10, 1, 1])\n",
      "Conv2d (139326242841424): torch.Size([1, 240, 1, 1])\n",
      "Sigmoid (139326242820112): torch.Size([1, 240, 1, 1])\n",
      "SqueezeExcitation (139326242821456): torch.Size([1, 240, 28, 28])\n",
      "Conv2d (139326242834064): torch.Size([1, 40, 28, 28])\n",
      "BatchNorm2d (139326443140736): torch.Size([1, 40, 28, 28])\n",
      "Conv2dNormActivation (139326476191456): torch.Size([1, 40, 28, 28])\n",
      "Sequential (139326242818992): torch.Size([1, 40, 28, 28])\n",
      "StochasticDepth (139326476191696): torch.Size([1, 40, 28, 28])\n",
      "MBConv (139326476189776): torch.Size([1, 40, 28, 28])\n",
      "Sequential (139326242817648): torch.Size([1, 40, 28, 28])\n",
      "Conv2d (139326242840464): torch.Size([1, 240, 28, 28])\n",
      "BatchNorm2d (139326443130672): torch.Size([1, 240, 28, 28])\n",
      "SiLU (139326476192416): torch.Size([1, 240, 28, 28])\n",
      "Conv2dNormActivation (139326476192176): torch.Size([1, 240, 28, 28])\n",
      "Conv2d (139326242839824): torch.Size([1, 240, 14, 14])\n",
      "BatchNorm2d (139326443142912): torch.Size([1, 240, 14, 14])\n",
      "SiLU (139326476192896): torch.Size([1, 240, 14, 14])\n",
      "Conv2dNormActivation (139326476192656): torch.Size([1, 240, 14, 14])\n",
      "AdaptiveAvgPool2d (139326476193136): torch.Size([1, 240, 1, 1])\n",
      "Conv2d (139326242840784): torch.Size([1, 10, 1, 1])\n",
      "SiLU (139326476193376): torch.Size([1, 10, 1, 1])\n",
      "Conv2d (139326242833744): torch.Size([1, 240, 1, 1])\n",
      "Sigmoid (139326232195856): torch.Size([1, 240, 1, 1])\n",
      "SqueezeExcitation (139326242821680): torch.Size([1, 240, 14, 14])\n",
      "Conv2d (139326242833104): torch.Size([1, 80, 14, 14])\n",
      "BatchNorm2d (139326443139648): torch.Size([1, 80, 14, 14])\n",
      "Conv2dNormActivation (139326476193616): torch.Size([1, 80, 14, 14])\n",
      "Sequential (139326232194064): torch.Size([1, 80, 14, 14])\n",
      "MBConv (139326476191936): torch.Size([1, 80, 14, 14])\n",
      "Conv2d (139326242840144): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d (139326443141280): torch.Size([1, 480, 14, 14])\n",
      "SiLU (139326476194576): torch.Size([1, 480, 14, 14])\n",
      "Conv2dNormActivation (139326476194336): torch.Size([1, 480, 14, 14])\n",
      "Conv2d (139326242833424): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d (139326443141552): torch.Size([1, 480, 14, 14])\n",
      "SiLU (139326476195056): torch.Size([1, 480, 14, 14])\n",
      "Conv2dNormActivation (139326476194816): torch.Size([1, 480, 14, 14])\n",
      "AdaptiveAvgPool2d (139326476195296): torch.Size([1, 480, 1, 1])\n",
      "Conv2d (139326242834704): torch.Size([1, 20, 1, 1])\n",
      "SiLU (139326476195536): torch.Size([1, 20, 1, 1])\n",
      "Conv2d (139326242848144): torch.Size([1, 480, 1, 1])\n",
      "Sigmoid (139326554498320): torch.Size([1, 480, 1, 1])\n",
      "SqueezeExcitation (139326554501008): torch.Size([1, 480, 14, 14])\n",
      "Conv2d (139326242848464): torch.Size([1, 80, 14, 14])\n",
      "BatchNorm2d (139326443141008): torch.Size([1, 80, 14, 14])\n",
      "Conv2dNormActivation (139326476195776): torch.Size([1, 80, 14, 14])\n",
      "Sequential (139326487202064): torch.Size([1, 80, 14, 14])\n",
      "StochasticDepth (139326476196016): torch.Size([1, 80, 14, 14])\n",
      "MBConv (139326476194096): torch.Size([1, 80, 14, 14])\n",
      "Conv2d (139326558683216): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d (139326443139920): torch.Size([1, 480, 14, 14])\n",
      "SiLU (139326476196736): torch.Size([1, 480, 14, 14])\n",
      "Conv2dNormActivation (139326476196496): torch.Size([1, 480, 14, 14])\n",
      "Conv2d (139326558683536): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d (139326443140192): torch.Size([1, 480, 14, 14])\n",
      "SiLU (139326476197216): torch.Size([1, 480, 14, 14])\n",
      "Conv2dNormActivation (139326476196976): torch.Size([1, 480, 14, 14])\n",
      "AdaptiveAvgPool2d (139326476197456): torch.Size([1, 480, 1, 1])\n",
      "Conv2d (139326558683856): torch.Size([1, 20, 1, 1])\n",
      "SiLU (139326476197696): torch.Size([1, 20, 1, 1])\n",
      "Conv2d (139326558684176): torch.Size([1, 480, 1, 1])\n",
      "Sigmoid (139326487205648): torch.Size([1, 480, 1, 1])\n",
      "SqueezeExcitation (139326487204752): torch.Size([1, 480, 14, 14])\n",
      "Conv2d (139326558684496): torch.Size([1, 80, 14, 14])\n",
      "BatchNorm2d (139326443138832): torch.Size([1, 80, 14, 14])\n",
      "Conv2dNormActivation (139326476197936): torch.Size([1, 80, 14, 14])\n",
      "Sequential (139326487207216): torch.Size([1, 80, 14, 14])\n",
      "StochasticDepth (139326476198176): torch.Size([1, 80, 14, 14])\n",
      "MBConv (139326476196256): torch.Size([1, 80, 14, 14])\n",
      "Sequential (139326487207888): torch.Size([1, 80, 14, 14])\n",
      "Conv2d (139326558684816): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d (139326443139376): torch.Size([1, 480, 14, 14])\n",
      "SiLU (139326476198896): torch.Size([1, 480, 14, 14])\n",
      "Conv2dNormActivation (139326476198656): torch.Size([1, 480, 14, 14])\n",
      "Conv2d (139326558685136): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d (139326443137744): torch.Size([1, 480, 14, 14])\n",
      "SiLU (139326476199376): torch.Size([1, 480, 14, 14])\n",
      "Conv2dNormActivation (139326476199136): torch.Size([1, 480, 14, 14])\n",
      "AdaptiveAvgPool2d (139326476199616): torch.Size([1, 480, 1, 1])\n",
      "Conv2d (139326558685456): torch.Size([1, 20, 1, 1])\n",
      "SiLU (139326476199856): torch.Size([1, 20, 1, 1])\n",
      "Conv2d (139326558685776): torch.Size([1, 480, 1, 1])\n",
      "Sigmoid (139326487211248): torch.Size([1, 480, 1, 1])\n",
      "SqueezeExcitation (139326487210352): torch.Size([1, 480, 14, 14])\n",
      "Conv2d (139326558686096): torch.Size([1, 112, 14, 14])\n",
      "BatchNorm2d (139326516284144): torch.Size([1, 112, 14, 14])\n",
      "Conv2dNormActivation (139326476200096): torch.Size([1, 112, 14, 14])\n",
      "Sequential (139326487212816): torch.Size([1, 112, 14, 14])\n",
      "MBConv (139326476198416): torch.Size([1, 112, 14, 14])\n",
      "Conv2d (139326558686416): torch.Size([1, 672, 14, 14])\n",
      "BatchNorm2d (139326516283600): torch.Size([1, 672, 14, 14])\n",
      "SiLU (139326476201056): torch.Size([1, 672, 14, 14])\n",
      "Conv2dNormActivation (139326476200816): torch.Size([1, 672, 14, 14])\n",
      "Conv2d (139326558686736): torch.Size([1, 672, 14, 14])\n",
      "BatchNorm2d (139326516283872): torch.Size([1, 672, 14, 14])\n",
      "SiLU (139326476201536): torch.Size([1, 672, 14, 14])\n",
      "Conv2dNormActivation (139326476201296): torch.Size([1, 672, 14, 14])\n",
      "AdaptiveAvgPool2d (139326476201776): torch.Size([1, 672, 1, 1])\n",
      "Conv2d (139326558687056): torch.Size([1, 28, 1, 1])\n",
      "SiLU (139326476202016): torch.Size([1, 28, 1, 1])\n",
      "Conv2d (139326558687376): torch.Size([1, 672, 1, 1])\n",
      "Sigmoid (139326473224720): torch.Size([1, 672, 1, 1])\n",
      "SqueezeExcitation (139326487215504): torch.Size([1, 672, 14, 14])\n",
      "Conv2d (139326558687696): torch.Size([1, 112, 14, 14])\n",
      "BatchNorm2d (139326516283328): torch.Size([1, 112, 14, 14])\n",
      "Conv2dNormActivation (139326476202256): torch.Size([1, 112, 14, 14])\n",
      "Sequential (139326473226288): torch.Size([1, 112, 14, 14])\n",
      "StochasticDepth (139326476202496): torch.Size([1, 112, 14, 14])\n",
      "MBConv (139326476200576): torch.Size([1, 112, 14, 14])\n",
      "Conv2d (139326558688016): torch.Size([1, 672, 14, 14])\n",
      "BatchNorm2d (139326516283056): torch.Size([1, 672, 14, 14])\n",
      "SiLU (139326476203216): torch.Size([1, 672, 14, 14])\n",
      "Conv2dNormActivation (139326476202976): torch.Size([1, 672, 14, 14])\n",
      "Conv2d (139326558688336): torch.Size([1, 672, 14, 14])\n",
      "BatchNorm2d (139326516281424): torch.Size([1, 672, 14, 14])\n",
      "SiLU (139326476203696): torch.Size([1, 672, 14, 14])\n",
      "Conv2dNormActivation (139326476203456): torch.Size([1, 672, 14, 14])\n",
      "AdaptiveAvgPool2d (139326476203936): torch.Size([1, 672, 1, 1])\n",
      "Conv2d (139326558688656): torch.Size([1, 28, 1, 1])\n",
      "SiLU (139326476204176): torch.Size([1, 28, 1, 1])\n",
      "Conv2d (139326558688976): torch.Size([1, 672, 1, 1])\n",
      "Sigmoid (139326473229872): torch.Size([1, 672, 1, 1])\n",
      "SqueezeExcitation (139326473228976): torch.Size([1, 672, 14, 14])\n",
      "Conv2d (139326558689296): torch.Size([1, 112, 14, 14])\n",
      "BatchNorm2d (139326516281696): torch.Size([1, 112, 14, 14])\n",
      "Conv2dNormActivation (139326476204416): torch.Size([1, 112, 14, 14])\n",
      "Sequential (139326473231440): torch.Size([1, 112, 14, 14])\n",
      "StochasticDepth (139326476204656): torch.Size([1, 112, 14, 14])\n",
      "MBConv (139326476202736): torch.Size([1, 112, 14, 14])\n",
      "Sequential (139326473232112): torch.Size([1, 112, 14, 14])\n",
      "Conv2d (139326558689616): torch.Size([1, 672, 14, 14])\n",
      "BatchNorm2d (139326516286048): torch.Size([1, 672, 14, 14])\n",
      "SiLU (139326476205376): torch.Size([1, 672, 14, 14])\n",
      "Conv2dNormActivation (139326476205136): torch.Size([1, 672, 14, 14])\n",
      "Conv2d (139326558689936): torch.Size([1, 672, 7, 7])\n",
      "BatchNorm2d (139326516286320): torch.Size([1, 672, 7, 7])\n",
      "SiLU (139326476205856): torch.Size([1, 672, 7, 7])\n",
      "Conv2dNormActivation (139326476205616): torch.Size([1, 672, 7, 7])\n",
      "AdaptiveAvgPool2d (139326473502800): torch.Size([1, 672, 1, 1])\n",
      "Conv2d (139326558690256): torch.Size([1, 28, 1, 1])\n",
      "SiLU (139326473503040): torch.Size([1, 28, 1, 1])\n",
      "Conv2d (139326558690576): torch.Size([1, 672, 1, 1])\n",
      "Sigmoid (139326473235472): torch.Size([1, 672, 1, 1])\n",
      "SqueezeExcitation (139326473234576): torch.Size([1, 672, 7, 7])\n",
      "Conv2d (139326558690896): torch.Size([1, 192, 7, 7])\n",
      "BatchNorm2d (139326516286592): torch.Size([1, 192, 7, 7])\n",
      "Conv2dNormActivation (139326473503280): torch.Size([1, 192, 7, 7])\n",
      "Sequential (139326473237040): torch.Size([1, 192, 7, 7])\n",
      "MBConv (139326476204896): torch.Size([1, 192, 7, 7])\n",
      "Conv2d (139326558691216): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (139326516285504): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (139326473504240): torch.Size([1, 1152, 7, 7])\n",
      "Conv2dNormActivation (139326473504000): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (139326558691536): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (139326516285776): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (139326473504720): torch.Size([1, 1152, 7, 7])\n",
      "Conv2dNormActivation (139326473504480): torch.Size([1, 1152, 7, 7])\n",
      "AdaptiveAvgPool2d (139326473504960): torch.Size([1, 1152, 1, 1])\n",
      "Conv2d (139326558691856): torch.Size([1, 48, 1, 1])\n",
      "SiLU (139326473505200): torch.Size([1, 48, 1, 1])\n",
      "Conv2d (139326558692176): torch.Size([1, 1152, 1, 1])\n",
      "Sigmoid (139326242898224): torch.Size([1, 1152, 1, 1])\n",
      "SqueezeExcitation (139326473239728): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (139326558692496): torch.Size([1, 192, 7, 7])\n",
      "BatchNorm2d (139326516288224): torch.Size([1, 192, 7, 7])\n",
      "Conv2dNormActivation (139326473505440): torch.Size([1, 192, 7, 7])\n",
      "Sequential (139326242899792): torch.Size([1, 192, 7, 7])\n",
      "StochasticDepth (139326473505680): torch.Size([1, 192, 7, 7])\n",
      "MBConv (139326473503760): torch.Size([1, 192, 7, 7])\n",
      "Conv2d (139326558692816): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (139326516288768): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (139326473506400): torch.Size([1, 1152, 7, 7])\n",
      "Conv2dNormActivation (139326473506160): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (139326558693136): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (139326505223856): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (139326473506880): torch.Size([1, 1152, 7, 7])\n",
      "Conv2dNormActivation (139326473506640): torch.Size([1, 1152, 7, 7])\n",
      "AdaptiveAvgPool2d (139326473507120): torch.Size([1, 1152, 1, 1])\n",
      "Conv2d (139326558693456): torch.Size([1, 48, 1, 1])\n",
      "SiLU (139326473507360): torch.Size([1, 48, 1, 1])\n",
      "Conv2d (139326558693776): torch.Size([1, 1152, 1, 1])\n",
      "Sigmoid (139326242903376): torch.Size([1, 1152, 1, 1])\n",
      "SqueezeExcitation (139326242902480): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (139326558694096): torch.Size([1, 192, 7, 7])\n",
      "BatchNorm2d (139326498467600): torch.Size([1, 192, 7, 7])\n",
      "Conv2dNormActivation (139326473507600): torch.Size([1, 192, 7, 7])\n",
      "Sequential (139326242904944): torch.Size([1, 192, 7, 7])\n",
      "StochasticDepth (139326473507840): torch.Size([1, 192, 7, 7])\n",
      "MBConv (139326473505920): torch.Size([1, 192, 7, 7])\n",
      "Conv2d (139326558694416): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (139326498464880): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (139326473508560): torch.Size([1, 1152, 7, 7])\n",
      "Conv2dNormActivation (139326473508320): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (139326558694736): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (139326491539888): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (139326473509040): torch.Size([1, 1152, 7, 7])\n",
      "Conv2dNormActivation (139326473508800): torch.Size([1, 1152, 7, 7])\n",
      "AdaptiveAvgPool2d (139326473509280): torch.Size([1, 1152, 1, 1])\n",
      "Conv2d (139326558695056): torch.Size([1, 48, 1, 1])\n",
      "SiLU (139326473509520): torch.Size([1, 48, 1, 1])\n",
      "Conv2d (139326558695376): torch.Size([1, 1152, 1, 1])\n",
      "Sigmoid (139326242908528): torch.Size([1, 1152, 1, 1])\n",
      "SqueezeExcitation (139326242907632): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (139326558695696): torch.Size([1, 192, 7, 7])\n",
      "BatchNorm2d (139326491537984): torch.Size([1, 192, 7, 7])\n",
      "Conv2dNormActivation (139326473509760): torch.Size([1, 192, 7, 7])\n",
      "Sequential (139326242910096): torch.Size([1, 192, 7, 7])\n",
      "StochasticDepth (139326473510000): torch.Size([1, 192, 7, 7])\n",
      "MBConv (139326473508080): torch.Size([1, 192, 7, 7])\n",
      "Sequential (139326242910768): torch.Size([1, 192, 7, 7])\n",
      "Conv2d (139326558696016): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (139326491539616): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (139326473510720): torch.Size([1, 1152, 7, 7])\n",
      "Conv2dNormActivation (139326473510480): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (139326558696336): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (139326516206848): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (139326473511200): torch.Size([1, 1152, 7, 7])\n",
      "Conv2dNormActivation (139326473510960): torch.Size([1, 1152, 7, 7])\n",
      "AdaptiveAvgPool2d (139326473511440): torch.Size([1, 1152, 1, 1])\n",
      "Conv2d (139326558696656): torch.Size([1, 48, 1, 1])\n",
      "SiLU (139326473511680): torch.Size([1, 48, 1, 1])\n",
      "Conv2d (139326558696976): torch.Size([1, 1152, 1, 1])\n",
      "Sigmoid (139326243373136): torch.Size([1, 1152, 1, 1])\n",
      "SqueezeExcitation (139326242913232): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (139326558697296): torch.Size([1, 320, 7, 7])\n",
      "BatchNorm2d (139326516207120): torch.Size([1, 320, 7, 7])\n",
      "Conv2dNormActivation (139326473511920): torch.Size([1, 320, 7, 7])\n",
      "Sequential (139326243374704): torch.Size([1, 320, 7, 7])\n",
      "MBConv (139326473510240): torch.Size([1, 320, 7, 7])\n",
      "Sequential (139326243375376): torch.Size([1, 320, 7, 7])\n",
      "Conv2d (139326558697616): torch.Size([1, 1280, 7, 7])\n",
      "BatchNorm2d (139326516207392): torch.Size([1, 1280, 7, 7])\n",
      "SiLU (139326473512640): torch.Size([1, 1280, 7, 7])\n",
      "Conv2dNormActivation (139326473512400): torch.Size([1, 1280, 7, 7])\n",
      "Sequential (139326243376720): torch.Size([1, 1280, 7, 7])\n",
      "AdaptiveAvgPool2d (139326473512880): torch.Size([1, 1280, 1, 1])\n",
      "Dropout (139326473513120): torch.Size([1, 1280])\n",
      "Linear (139326473513360): torch.Size([1, 1000])\n",
      "Sequential (139326243377840): torch.Size([1, 1000])\n",
      "EfficientNet (139326569980528): torch.Size([1, 1000])\n",
      "EfficientNet-B0 Top-1 Accuracy on ImageNet: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import xml.etree.ElementTree as ET\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Define paths for ImageNet validation images and annotations\n",
    "IMAGE_DIR = \"/home/kajm20/mnist/ILSVRC/Data/CLS-LOC/val\"  \n",
    "ANNOTATION_DIR = \"/home/kajm20/mnist/ILSVRC/Annotations/CLS-LOC/val\"  \n",
    "\n",
    "# 2. Define transformations for EfficientNet input\n",
    "imagenet_transform = transforms.Compose([\n",
    "    transforms.Resize(256),  \n",
    "    transforms.CenterCrop(224),  \n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n",
    "])\n",
    "\n",
    "# 3. Load the synset mapping\n",
    "synset_mapping_path = \"/home/kajm20/mnist/ILSVRC/LOC_synset_mapping.txt\"\n",
    "wordnet_to_imagenet = {}\n",
    "\n",
    "with open(synset_mapping_path) as f:\n",
    "    for idx, line in enumerate(f.readlines()):\n",
    "        wordnet_id, _ = line.split(' ', 1)\n",
    "        wordnet_to_imagenet[wordnet_id] = idx  \n",
    "\n",
    "# 4. Define the custom dataset class\n",
    "class ImageNetValDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.transform = transform\n",
    "        self.annotation_files = sorted(os.listdir(annotation_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotation_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation_path = os.path.join(self.annotation_dir, self.annotation_files[idx])\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "        wordnet_id = root.find(\"object\").find(\"name\").text  \n",
    "\n",
    "        class_idx = wordnet_to_imagenet.get(wordnet_id, -1)  \n",
    "        image_filename = root.find(\"filename\").text + \".JPEG\"\n",
    "        image_path = os.path.join(self.image_dir, image_filename)\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, class_idx\n",
    "\n",
    "# 5. Initialize the dataset and dataloader\n",
    "imagenet_val_dataset = ImageNetValDataset(IMAGE_DIR, ANNOTATION_DIR, transform=imagenet_transform)\n",
    "imagenet_val_loader = DataLoader(imagenet_val_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "# 6. Define the model (EfficientNet-B0 with pre-trained weights)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.efficientnet_b0(weights='DEFAULT')  \n",
    "model.to(device)\n",
    "model.eval()  \n",
    "\n",
    "# 7. Dictionaries to store activations\n",
    "activation_shapes = {}  # Stores activation shapes\n",
    "activation_values = {}  # Stores activation values\n",
    "\n",
    "# 8. Hook function to store activations for **ALL** layers\n",
    "def hook_fn(module, input, output):\n",
    "    layer_name = f\"{module.__class__.__name__} ({id(module)})\" \n",
    "    activation_shapes[layer_name] = output.shape  # Store shape\n",
    "    activation_values[layer_name] = output.detach().cpu()  # Store values (moved to CPU)\n",
    "\n",
    "# 9. Recursively register hooks for **all** relevant layers, including `AdaptiveAvgPool2d`\n",
    "for name, layer in model.named_modules():\n",
    "    layer.register_forward_hook(hook_fn)\n",
    "\n",
    "# 10. Define the evaluation function\n",
    "def evaluate_model(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for images, labels in tqdm(dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            activation_shapes.clear()  \n",
    "            activation_values.clear()  \n",
    "            \n",
    "            print(f\"\\nRaw Image Tensor Shape (Before Normalization): {images.shape}\")  \n",
    "            \n",
    "            outputs = model(images)  \n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)  \n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Print activation shapes\n",
    "            print(\"\\n--- Layer Activation Shapes ---\")\n",
    "            for layer, shape in activation_shapes.items():\n",
    "                print(f\"{layer}: {shape}\")\n",
    "\n",
    "            break  \n",
    "\n",
    "    accuracy = (correct / total) * 100\n",
    "    return accuracy\n",
    "\n",
    "# 11. Evaluate the model on ImageNet validation set\n",
    "accuracy = evaluate_model(model, imagenet_val_loader)\n",
    "print(f\"EfficientNet-B0 Top-1 Accuracy on ImageNet: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (8): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=True)\n",
       "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.efficientnet_b0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
