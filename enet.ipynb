{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (8): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=True)\n",
       "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import xml.etree.ElementTree as ET\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Define paths for ImageNet validation images and annotations\n",
    "IMAGE_DIR = \"/home/kajm20/mnist/ILSVRC/Data/CLS-LOC/val\"  # Path to validation images\n",
    "ANNOTATION_DIR = \"/home/kajm20/mnist/ILSVRC/Annotations/CLS-LOC/val\"  # Path to XML annotations\n",
    "\n",
    "# 2. Define transformations for EfficientNet input (resize, crop, normalize)\n",
    "imagenet_transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # Resize the image to 256x256\n",
    "    transforms.CenterCrop(224),  # Crop the image to 224x224\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet mean/std\n",
    "])\n",
    "\n",
    "# 3. Load the synset mapping\n",
    "synset_mapping_path = \"/home/kajm20/mnist/ILSVRC/LOC_synset_mapping.txt\"\n",
    "wordnet_to_imagenet = {}\n",
    "\n",
    "# Load synset mapping from file\n",
    "with open(synset_mapping_path) as f:\n",
    "    for idx, line in enumerate(f.readlines()):\n",
    "        wordnet_id, _ = line.split(' ', 1)  # Get WordNet ID from the line (skip class name)\n",
    "        wordnet_to_imagenet[wordnet_id] = idx  # Map WordNet ID to class index\n",
    "\n",
    "# 4. Define the custom dataset class\n",
    "class ImageNetValDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Get all annotation file names\n",
    "        self.annotation_files = sorted(os.listdir(annotation_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotation_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get annotation file path\n",
    "        annotation_path = os.path.join(self.annotation_dir, self.annotation_files[idx])\n",
    "        \n",
    "        # Parse XML to extract class label\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "        wordnet_id = root.find(\"object\").find(\"name\").text  # WordNet ID, e.g., 'n01751748'\n",
    "\n",
    "        # Use the synset mapping to convert WordNet ID to ImageNet class index\n",
    "        class_idx = wordnet_to_imagenet.get(wordnet_id, -1)  # Default to -1 if not found (shouldn't happen)\n",
    "\n",
    "        # Get image filename from XML and construct image path\n",
    "        image_filename = root.find(\"filename\").text + \".JPEG\"\n",
    "        image_path = os.path.join(self.image_dir, image_filename)\n",
    "\n",
    "        # Load and transform image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, class_idx\n",
    "\n",
    "# 5. Initialize the dataset and dataloader\n",
    "imagenet_val_dataset = ImageNetValDataset(IMAGE_DIR, ANNOTATION_DIR, transform=imagenet_transform)\n",
    "imagenet_val_loader = DataLoader(imagenet_val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# 6. Define the model (EfficientNet-B0 with pre-trained weights)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.efficientnet_b0(weights='DEFAULT')  # Load pre-trained EfficientNet-B0 model\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 224, 224])\n",
      "torch.Size([2, 1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 8. Evaluate the model on ImageNet validation set\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimagenet_val_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEfficientNet-B0 Top-1 Accuracy on ImageNet: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     14\u001b[0m         correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# Count correct predictions\u001b[39;00m\n\u001b[1;32m     15\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m (\u001b[43mcorrect\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# 7. Define the evaluation function\n",
    "def evaluate_model(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        for images, labels in tqdm(dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images) \n",
    "            _, predicted = torch.max(outputs, 1)  # Get highest probability class\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = (correct / total) * 100\n",
    "    return accuracy\n",
    "\n",
    "# 8. Evaluate the model on ImageNet validation set\n",
    "accuracy = evaluate_model(model, imagenet_val_loader)\n",
    "print(f\"EfficientNet-B0 Top-1 Accuracy on ImageNet: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Shapes Before and After Transformation ---\n",
      "Raw Image Tensor Shape (Before Normalization): torch.Size([1, 3, 375, 500])\n",
      "Transformed Image Tensor Shape (After Normalization): torch.Size([1, 3, 224, 224])\n",
      "\n",
      "--- Layer Output Shapes ---\n",
      "Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): torch.Size([1, 32, 112, 112])\n",
      "BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 32, 112, 112])\n",
      "Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False): torch.Size([1, 32, 112, 112])\n",
      "BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 32, 112, 112])\n",
      "Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 8, 1, 1])\n",
      "Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 32, 1, 1])\n",
      "Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 16, 112, 112])\n",
      "BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 16, 112, 112])\n",
      "Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 96, 112, 112])\n",
      "BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 96, 112, 112])\n",
      "Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False): torch.Size([1, 96, 56, 56])\n",
      "BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 96, 56, 56])\n",
      "Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 4, 1, 1])\n",
      "Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 96, 1, 1])\n",
      "Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 24, 56, 56])\n",
      "BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 24, 56, 56])\n",
      "Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 144, 56, 56])\n",
      "BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 144, 56, 56])\n",
      "Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False): torch.Size([1, 144, 56, 56])\n",
      "BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 144, 56, 56])\n",
      "Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 6, 1, 1])\n",
      "Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 144, 1, 1])\n",
      "Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 24, 56, 56])\n",
      "BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 24, 56, 56])\n",
      "Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 144, 56, 56])\n",
      "BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 144, 56, 56])\n",
      "Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False): torch.Size([1, 144, 28, 28])\n",
      "BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 144, 28, 28])\n",
      "Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 6, 1, 1])\n",
      "Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 144, 1, 1])\n",
      "Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 40, 28, 28])\n",
      "BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 40, 28, 28])\n",
      "Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 240, 28, 28])\n",
      "BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 240, 28, 28])\n",
      "Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False): torch.Size([1, 240, 28, 28])\n",
      "BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 240, 28, 28])\n",
      "Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 10, 1, 1])\n",
      "Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 240, 1, 1])\n",
      "Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 40, 28, 28])\n",
      "BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 40, 28, 28])\n",
      "Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 240, 28, 28])\n",
      "BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 240, 28, 28])\n",
      "Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False): torch.Size([1, 240, 14, 14])\n",
      "BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 240, 14, 14])\n",
      "Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 10, 1, 1])\n",
      "Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 240, 1, 1])\n",
      "Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 80, 14, 14])\n",
      "BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 80, 14, 14])\n",
      "Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 480, 14, 14])\n",
      "Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 480, 14, 14])\n",
      "Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 20, 1, 1])\n",
      "Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 480, 1, 1])\n",
      "Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 80, 14, 14])\n",
      "BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 80, 14, 14])\n",
      "Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 480, 14, 14])\n",
      "Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 480, 14, 14])\n",
      "Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 20, 1, 1])\n",
      "Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 480, 1, 1])\n",
      "Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 80, 14, 14])\n",
      "BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 80, 14, 14])\n",
      "Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 480, 14, 14])\n",
      "Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 480, 14, 14])\n",
      "Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 20, 1, 1])\n",
      "Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 480, 1, 1])\n",
      "Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 112, 14, 14])\n",
      "BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 112, 14, 14])\n",
      "Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 672, 14, 14])\n",
      "BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 672, 14, 14])\n",
      "Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False): torch.Size([1, 672, 14, 14])\n",
      "BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 672, 14, 14])\n",
      "Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 28, 1, 1])\n",
      "Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 672, 1, 1])\n",
      "Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 112, 14, 14])\n",
      "BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 112, 14, 14])\n",
      "Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 672, 14, 14])\n",
      "BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 672, 14, 14])\n",
      "Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False): torch.Size([1, 672, 14, 14])\n",
      "BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 672, 14, 14])\n",
      "Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 28, 1, 1])\n",
      "Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 672, 1, 1])\n",
      "Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 112, 14, 14])\n",
      "BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 112, 14, 14])\n",
      "Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 672, 14, 14])\n",
      "BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 672, 14, 14])\n",
      "Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False): torch.Size([1, 672, 7, 7])\n",
      "BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 672, 7, 7])\n",
      "Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 28, 1, 1])\n",
      "Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 672, 1, 1])\n",
      "Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 192, 7, 7])\n",
      "BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 192, 7, 7])\n",
      "Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 48, 1, 1])\n",
      "Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 1152, 1, 1])\n",
      "Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 192, 7, 7])\n",
      "BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 192, 7, 7])\n",
      "Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 48, 1, 1])\n",
      "Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 1152, 1, 1])\n",
      "Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 192, 7, 7])\n",
      "BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 192, 7, 7])\n",
      "Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 48, 1, 1])\n",
      "Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 1152, 1, 1])\n",
      "Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 192, 7, 7])\n",
      "BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 192, 7, 7])\n",
      "Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 48, 1, 1])\n",
      "Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1)): torch.Size([1, 1152, 1, 1])\n",
      "Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 320, 7, 7])\n",
      "BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 320, 7, 7])\n",
      "Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False): torch.Size([1, 1280, 7, 7])\n",
      "BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): torch.Size([1, 1280, 7, 7])\n",
      "Linear(in_features=1280, out_features=1000, bias=True): torch.Size([1, 1000])\n",
      "EfficientNet-B0 Top-1 Accuracy on ImageNet: 100.00%\n",
      "Saved input image and activations.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import xml.etree.ElementTree as ET\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Define paths for ImageNet validation images and annotations\n",
    "IMAGE_DIR = \"/home/kajm20/mnist/ILSVRC/Data/CLS-LOC/val\"  # Path to validation images\n",
    "ANNOTATION_DIR = \"/home/kajm20/mnist/ILSVRC/Annotations/CLS-LOC/val\"  # Path to XML annotations\n",
    "\n",
    "# 2. Define transformations for EfficientNet input (resize, crop, normalize)\n",
    "imagenet_transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # Resize the image to 256x256\n",
    "    transforms.CenterCrop(224),  # Crop the image to 224x224\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet mean/std\n",
    "])\n",
    "\n",
    "# 3. Load the synset mapping\n",
    "synset_mapping_path = \"/home/kajm20/mnist/ILSVRC/LOC_synset_mapping.txt\"\n",
    "wordnet_to_imagenet = {}\n",
    "\n",
    "# Load synset mapping from file\n",
    "with open(synset_mapping_path) as f:\n",
    "    for idx, line in enumerate(f.readlines()):\n",
    "        wordnet_id, _ = line.split(' ', 1)  # Get WordNet ID from the line (skip class name)\n",
    "        wordnet_to_imagenet[wordnet_id] = idx  # Map WordNet ID to class index\n",
    "\n",
    "# 4. Define the custom dataset class\n",
    "class ImageNetValDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Get all annotation file names\n",
    "        self.annotation_files = sorted(os.listdir(annotation_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotation_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get annotation file path\n",
    "        annotation_path = os.path.join(self.annotation_dir, self.annotation_files[idx])\n",
    "        \n",
    "        # Parse XML to extract class label\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "        wordnet_id = root.find(\"object\").find(\"name\").text  # WordNet ID, e.g., 'n01751748'\n",
    "\n",
    "        # Use the synset mapping to convert WordNet ID to ImageNet class index\n",
    "        class_idx = wordnet_to_imagenet.get(wordnet_id, -1)  # Default to -1 if not found (shouldn't happen)\n",
    "\n",
    "        # Get image filename from XML and construct image path\n",
    "        image_filename = root.find(\"filename\").text + \".JPEG\"\n",
    "        image_path = os.path.join(self.image_dir, image_filename)\n",
    "\n",
    "        # Load image without transformation (for raw tensor shape printing)\n",
    "        raw_image = Image.open(image_path).convert(\"RGB\")\n",
    "        raw_tensor = transforms.ToTensor()(raw_image)  # Convert to tensor before normalization\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(raw_image)\n",
    "        else:\n",
    "            image = raw_tensor  # Just in case transform is None\n",
    "\n",
    "        return raw_tensor, image, class_idx  # Return raw image tensor, transformed image tensor, and label\n",
    "\n",
    "# 5. Initialize the dataset and dataloader\n",
    "imagenet_val_dataset = ImageNetValDataset(IMAGE_DIR, ANNOTATION_DIR, transform=imagenet_transform)\n",
    "imagenet_val_loader = DataLoader(imagenet_val_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "# 6. Define the model (EfficientNet-B0 with pre-trained weights)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.efficientnet_b0(weights='DEFAULT')  # Load pre-trained EfficientNet-B0 model\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Dictionary to store layer activations\n",
    "activations = {}\n",
    "\n",
    "# Hook function to store the output of each layer\n",
    "def hook_fn(module, input, output):\n",
    "    activations[module] = output.detach().cpu()\n",
    "\n",
    "# Register hooks on all layers\n",
    "for name, layer in model.named_modules():\n",
    "    if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear) or isinstance(layer, nn.BatchNorm2d):\n",
    "        layer.register_forward_hook(hook_fn)\n",
    "\n",
    "# 7. Define the evaluation function\n",
    "def evaluate_model(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    first_layer_input = None  # Store input image before first layer\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        for raw_images, transformed_images, labels in tqdm(dataloader):\n",
    "            raw_images, transformed_images, labels = raw_images.to(device), transformed_images.to(device), labels.to(device)\n",
    "\n",
    "            # Store the transformed input image before it goes into the first layer\n",
    "            first_layer_input = transformed_images.cpu().detach().clone()\n",
    "\n",
    "            # Print the shapes instead of the tensors\n",
    "            print(\"\\n--- Shapes Before and After Transformation ---\")\n",
    "            print(f\"Raw Image Tensor Shape (Before Normalization): {raw_images.shape}\")  # Should be (1, 3, H, W)\n",
    "            print(f\"Transformed Image Tensor Shape (After Normalization): {transformed_images.shape}\")  # Should be (1, 3, 224, 224)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(transformed_images) \n",
    "            _, predicted = torch.max(outputs, 1)  # Get highest probability class\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Print stored activations for debugging\n",
    "            print(\"\\n--- Layer Output Shapes ---\")\n",
    "            for layer, activation in activations.items():\n",
    "                print(f\"{layer}: {activation.shape}\")\n",
    "\n",
    "            # Break after 1st batch for visualization\n",
    "            break  \n",
    "\n",
    "    accuracy = (correct / total) * 100\n",
    "    return accuracy, first_layer_input, activations\n",
    "\n",
    "# 8. Evaluate the model on ImageNet validation set\n",
    "accuracy, first_layer_input, activations = evaluate_model(model, imagenet_val_loader)\n",
    "print(f\"EfficientNet-B0 Top-1 Accuracy on ImageNet: {accuracy:.2f}%\")\n",
    "\n",
    "# Save the first layer input for later analysis\n",
    "torch.save(first_layer_input, \"first_layer_input.pt\")\n",
    "torch.save(activations, \"activations.pt\")\n",
    "\n",
    "print(\"Saved input image and activations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw Image Tensor Shape (Before Normalization): torch.Size([1, 3, 224, 224])\n",
      "\n",
      "--- Layer Activations ---\n",
      "Conv2d (139326491122640): torch.Size([1, 32, 112, 112])\n",
      "BatchNorm2d (139326499503328): torch.Size([1, 32, 112, 112])\n",
      "SiLU (139326473888512): torch.Size([1, 32, 112, 112])\n",
      "Conv2d (139326491123280): torch.Size([1, 32, 112, 112])\n",
      "BatchNorm2d (139326491538528): torch.Size([1, 32, 112, 112])\n",
      "SiLU (139326473886592): torch.Size([1, 32, 112, 112])\n",
      "Conv2d (139326491122960): torch.Size([1, 8, 1, 1])\n",
      "SiLU (139326473885392): torch.Size([1, 8, 1, 1])\n",
      "Conv2d (139326491120400): torch.Size([1, 32, 1, 1])\n",
      "Conv2d (139326491119440): torch.Size([1, 16, 112, 112])\n",
      "BatchNorm2d (139326491539344): torch.Size([1, 16, 112, 112])\n",
      "Conv2d (139326491118800): torch.Size([1, 96, 112, 112])\n",
      "BatchNorm2d (139326491537712): torch.Size([1, 96, 112, 112])\n",
      "SiLU (139326473884672): torch.Size([1, 96, 112, 112])\n",
      "Conv2d (139326491119120): torch.Size([1, 96, 56, 56])\n",
      "BatchNorm2d (139326491541248): torch.Size([1, 96, 56, 56])\n",
      "SiLU (139326473886832): torch.Size([1, 96, 56, 56])\n",
      "Conv2d (139326491118160): torch.Size([1, 4, 1, 1])\n",
      "SiLU (139326473887312): torch.Size([1, 4, 1, 1])\n",
      "Conv2d (139326476518096): torch.Size([1, 96, 1, 1])\n",
      "Conv2d (139326476519376): torch.Size([1, 24, 56, 56])\n",
      "BatchNorm2d (139326472360272): torch.Size([1, 24, 56, 56])\n",
      "Conv2d (139326476519056): torch.Size([1, 144, 56, 56])\n",
      "BatchNorm2d (139326472360000): torch.Size([1, 144, 56, 56])\n",
      "SiLU (139326473881792): torch.Size([1, 144, 56, 56])\n",
      "Conv2d (139326476519696): torch.Size([1, 144, 56, 56])\n",
      "BatchNorm2d (139326472364080): torch.Size([1, 144, 56, 56])\n",
      "SiLU (139326473883472): torch.Size([1, 144, 56, 56])\n",
      "Conv2d (139326476518736): torch.Size([1, 6, 1, 1])\n",
      "SiLU (139326473882992): torch.Size([1, 6, 1, 1])\n",
      "Conv2d (139326476520336): torch.Size([1, 144, 1, 1])\n",
      "Conv2d (139326476520016): torch.Size([1, 24, 56, 56])\n",
      "BatchNorm2d (139326472363808): torch.Size([1, 24, 56, 56])\n",
      "Conv2d (139326476520656): torch.Size([1, 144, 56, 56])\n",
      "BatchNorm2d (139326472361904): torch.Size([1, 144, 56, 56])\n",
      "SiLU (139326473879632): torch.Size([1, 144, 56, 56])\n",
      "Conv2d (139326476520976): torch.Size([1, 144, 28, 28])\n",
      "BatchNorm2d (139326472358096): torch.Size([1, 144, 28, 28])\n",
      "SiLU (139326473881312): torch.Size([1, 144, 28, 28])\n",
      "Conv2d (139326476521296): torch.Size([1, 6, 1, 1])\n",
      "SiLU (139326473880832): torch.Size([1, 6, 1, 1])\n",
      "Conv2d (139326476521616): torch.Size([1, 144, 1, 1])\n",
      "Conv2d (139326476521936): torch.Size([1, 40, 28, 28])\n",
      "BatchNorm2d (139326472356464): torch.Size([1, 40, 28, 28])\n",
      "Conv2d (139326476522256): torch.Size([1, 240, 28, 28])\n",
      "BatchNorm2d (139326472359184): torch.Size([1, 240, 28, 28])\n",
      "SiLU (139326477148208): torch.Size([1, 240, 28, 28])\n",
      "Conv2d (139326476522576): torch.Size([1, 240, 28, 28])\n",
      "BatchNorm2d (139326472368160): torch.Size([1, 240, 28, 28])\n",
      "SiLU (139326477156128): torch.Size([1, 240, 28, 28])\n",
      "Conv2d (139326476522896): torch.Size([1, 10, 1, 1])\n",
      "SiLU (139326477155648): torch.Size([1, 10, 1, 1])\n",
      "Conv2d (139326476523216): torch.Size([1, 240, 1, 1])\n",
      "Conv2d (139326476523536): torch.Size([1, 40, 28, 28])\n",
      "BatchNorm2d (139326472370336): torch.Size([1, 40, 28, 28])\n",
      "Conv2d (139326476523856): torch.Size([1, 240, 28, 28])\n",
      "BatchNorm2d (139326472368432): torch.Size([1, 240, 28, 28])\n",
      "SiLU (139326477154688): torch.Size([1, 240, 28, 28])\n",
      "Conv2d (139326476524176): torch.Size([1, 240, 14, 14])\n",
      "BatchNorm2d (139326472371968): torch.Size([1, 240, 14, 14])\n",
      "SiLU (139326477153968): torch.Size([1, 240, 14, 14])\n",
      "Conv2d (139326476524496): torch.Size([1, 10, 1, 1])\n",
      "SiLU (139326477153488): torch.Size([1, 10, 1, 1])\n",
      "Conv2d (139326476524816): torch.Size([1, 240, 1, 1])\n",
      "Conv2d (139326476525136): torch.Size([1, 80, 14, 14])\n",
      "BatchNorm2d (139326472367888): torch.Size([1, 80, 14, 14])\n",
      "Conv2d (139326476525456): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d (139326472368704): torch.Size([1, 480, 14, 14])\n",
      "SiLU (139326477152528): torch.Size([1, 480, 14, 14])\n",
      "Conv2d (139326476525776): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d (139326472369792): torch.Size([1, 480, 14, 14])\n",
      "SiLU (139326477151808): torch.Size([1, 480, 14, 14])\n",
      "Conv2d (139326476526096): torch.Size([1, 20, 1, 1])\n",
      "SiLU (139326477151328): torch.Size([1, 20, 1, 1])\n",
      "Conv2d (139326476526416): torch.Size([1, 480, 1, 1])\n",
      "Conv2d (139326476526736): torch.Size([1, 80, 14, 14])\n",
      "BatchNorm2d (139326472368976): torch.Size([1, 80, 14, 14])\n",
      "Conv2d (139326476527056): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d (139326472369520): torch.Size([1, 480, 14, 14])\n",
      "SiLU (139326477150368): torch.Size([1, 480, 14, 14])\n",
      "Conv2d (139326476527376): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d (139326472369248): torch.Size([1, 480, 14, 14])\n",
      "SiLU (139326477149408): torch.Size([1, 480, 14, 14])\n",
      "Conv2d (139326476527696): torch.Size([1, 20, 1, 1])\n",
      "SiLU (139326477148928): torch.Size([1, 20, 1, 1])\n",
      "Conv2d (139326476528016): torch.Size([1, 480, 1, 1])\n",
      "Conv2d (139326476528336): torch.Size([1, 80, 14, 14])\n",
      "BatchNorm2d (139326472370608): torch.Size([1, 80, 14, 14])\n",
      "Conv2d (139326476528656): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d (139326472370880): torch.Size([1, 480, 14, 14])\n",
      "SiLU (139326498866672): torch.Size([1, 480, 14, 14])\n",
      "Conv2d (139326476528976): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d (139326498463248): torch.Size([1, 480, 14, 14])\n",
      "SiLU (139326498866192): torch.Size([1, 480, 14, 14])\n",
      "Conv2d (139326476529296): torch.Size([1, 20, 1, 1])\n",
      "SiLU (139326498865952): torch.Size([1, 20, 1, 1])\n",
      "Conv2d (139326476529616): torch.Size([1, 480, 1, 1])\n",
      "Conv2d (139326476529936): torch.Size([1, 112, 14, 14])\n",
      "BatchNorm2d (139326498459712): torch.Size([1, 112, 14, 14])\n",
      "Conv2d (139326476530256): torch.Size([1, 672, 14, 14])\n",
      "BatchNorm2d (139326498459984): torch.Size([1, 672, 14, 14])\n",
      "SiLU (139326498871952): torch.Size([1, 672, 14, 14])\n",
      "Conv2d (139326476530576): torch.Size([1, 672, 14, 14])\n",
      "BatchNorm2d (139326498466240): torch.Size([1, 672, 14, 14])\n",
      "SiLU (139326499622736): torch.Size([1, 672, 14, 14])\n",
      "Conv2d (139326476530896): torch.Size([1, 28, 1, 1])\n",
      "SiLU (139326499634976): torch.Size([1, 28, 1, 1])\n",
      "Conv2d (139326476531216): torch.Size([1, 672, 1, 1])\n",
      "Conv2d (139326476531536): torch.Size([1, 112, 14, 14])\n",
      "BatchNorm2d (139326498460256): torch.Size([1, 112, 14, 14])\n",
      "Conv2d (139326476531856): torch.Size([1, 672, 14, 14])\n",
      "BatchNorm2d (139326498457808): torch.Size([1, 672, 14, 14])\n",
      "SiLU (139326499633776): torch.Size([1, 672, 14, 14])\n",
      "Conv2d (139326476532176): torch.Size([1, 672, 14, 14])\n",
      "BatchNorm2d (139326498460528): torch.Size([1, 672, 14, 14])\n",
      "SiLU (139326499633296): torch.Size([1, 672, 14, 14])\n",
      "Conv2d (139326476532496): torch.Size([1, 28, 1, 1])\n",
      "SiLU (139326503752224): torch.Size([1, 28, 1, 1])\n",
      "Conv2d (139326476532816): torch.Size([1, 672, 1, 1])\n",
      "Conv2d (139326476533136): torch.Size([1, 112, 14, 14])\n",
      "BatchNorm2d (139326498459168): torch.Size([1, 112, 14, 14])\n",
      "Conv2d (139326476533456): torch.Size([1, 672, 14, 14])\n",
      "BatchNorm2d (139326498458624): torch.Size([1, 672, 14, 14])\n",
      "SiLU (139326484736160): torch.Size([1, 672, 14, 14])\n",
      "Conv2d (139326486151248): torch.Size([1, 672, 7, 7])\n",
      "BatchNorm2d (139326498456720): torch.Size([1, 672, 7, 7])\n",
      "SiLU (139326484736880): torch.Size([1, 672, 7, 7])\n",
      "Conv2d (139326486151568): torch.Size([1, 28, 1, 1])\n",
      "SiLU (139326484737360): torch.Size([1, 28, 1, 1])\n",
      "Conv2d (139326486151888): torch.Size([1, 672, 1, 1])\n",
      "Conv2d (139326486152208): torch.Size([1, 192, 7, 7])\n",
      "BatchNorm2d (139326498465152): torch.Size([1, 192, 7, 7])\n",
      "Conv2d (139326486152528): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (139326498458080): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (139326484738560): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (139326486152848): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (139326498459440): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (139326484739040): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (139326486153168): torch.Size([1, 48, 1, 1])\n",
      "SiLU (139326484739520): torch.Size([1, 48, 1, 1])\n",
      "Conv2d (139326486153488): torch.Size([1, 1152, 1, 1])\n",
      "Conv2d (139326486153808): torch.Size([1, 192, 7, 7])\n",
      "BatchNorm2d (139326498467056): torch.Size([1, 192, 7, 7])\n",
      "Conv2d (139326486154128): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (139326498455632): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (139326484740720): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (139326486154448): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (139326498461616): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (139326484741920): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (139326486154768): torch.Size([1, 48, 1, 1])\n",
      "SiLU (139326484740960): torch.Size([1, 48, 1, 1])\n",
      "Conv2d (139326486155088): torch.Size([1, 1152, 1, 1])\n",
      "Conv2d (139326486155408): torch.Size([1, 192, 7, 7])\n",
      "BatchNorm2d (139326498455904): torch.Size([1, 192, 7, 7])\n",
      "Conv2d (139326486155728): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (139326498456992): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (139326485955120): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (139326486156048): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (139326498457264): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (139326485958480): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (139326486156368): torch.Size([1, 48, 1, 1])\n",
      "SiLU (139326485959680): torch.Size([1, 48, 1, 1])\n",
      "Conv2d (139326486156688): torch.Size([1, 1152, 1, 1])\n",
      "Conv2d (139326486157008): torch.Size([1, 192, 7, 7])\n",
      "BatchNorm2d (139326498462432): torch.Size([1, 192, 7, 7])\n",
      "Conv2d (139326486157328): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (139326498461888): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (139326485957280): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (139326486157648): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (139326498461344): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (139326485955840): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (139326486157968): torch.Size([1, 48, 1, 1])\n",
      "SiLU (139326485956080): torch.Size([1, 48, 1, 1])\n",
      "Conv2d (139326486158288): torch.Size([1, 1152, 1, 1])\n",
      "Conv2d (139326486158608): torch.Size([1, 320, 7, 7])\n",
      "BatchNorm2d (139326498462704): torch.Size([1, 320, 7, 7])\n",
      "Conv2d (139326485808784): torch.Size([1, 1280, 7, 7])\n",
      "BatchNorm2d (139326498462976): torch.Size([1, 1280, 7, 7])\n",
      "SiLU (139326485960160): torch.Size([1, 1280, 7, 7])\n",
      "Linear (139326476459856): torch.Size([1, 1000])\n",
      "EfficientNet-B0 Top-1 Accuracy on ImageNet: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import xml.etree.ElementTree as ET\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Define paths for ImageNet validation images and annotations\n",
    "IMAGE_DIR = \"/home/kajm20/mnist/ILSVRC/Data/CLS-LOC/val\"  # Path to validation images\n",
    "ANNOTATION_DIR = \"/home/kajm20/mnist/ILSVRC/Annotations/CLS-LOC/val\"  # Path to XML annotations\n",
    "\n",
    "# 2. Define transformations for EfficientNet input (resize, crop, normalize)\n",
    "imagenet_transform = transforms.Compose([\n",
    "    transforms.Resize(256),  \n",
    "    transforms.CenterCrop(224),  \n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n",
    "])\n",
    "\n",
    "# 3. Load the synset mapping\n",
    "synset_mapping_path = \"/home/kajm20/mnist/ILSVRC/LOC_synset_mapping.txt\"\n",
    "wordnet_to_imagenet = {}\n",
    "\n",
    "with open(synset_mapping_path) as f:\n",
    "    for idx, line in enumerate(f.readlines()):\n",
    "        wordnet_id, _ = line.split(' ', 1)\n",
    "        wordnet_to_imagenet[wordnet_id] = idx  \n",
    "\n",
    "# 4. Define the custom dataset class\n",
    "class ImageNetValDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.transform = transform\n",
    "        self.annotation_files = sorted(os.listdir(annotation_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotation_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation_path = os.path.join(self.annotation_dir, self.annotation_files[idx])\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "        wordnet_id = root.find(\"object\").find(\"name\").text  \n",
    "\n",
    "        class_idx = wordnet_to_imagenet.get(wordnet_id, -1)  \n",
    "        image_filename = root.find(\"filename\").text + \".JPEG\"\n",
    "        image_path = os.path.join(self.image_dir, image_filename)\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, class_idx\n",
    "\n",
    "# 5. Initialize the dataset and dataloader\n",
    "imagenet_val_dataset = ImageNetValDataset(IMAGE_DIR, ANNOTATION_DIR, transform=imagenet_transform)\n",
    "imagenet_val_loader = DataLoader(imagenet_val_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "# 6. Define the model (EfficientNet-B0 with pre-trained weights)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.efficientnet_b0(weights='DEFAULT')  \n",
    "model.to(device)\n",
    "model.eval()  \n",
    "\n",
    "# 7. Dictionary to store activations\n",
    "activations = {}\n",
    "\n",
    "# 8. Hook function to store activations for **ALL** layers\n",
    "def hook_fn(module, input, output):\n",
    "    layer_name = f\"{module.__class__.__name__} ({id(module)})\"\n",
    "    if isinstance(output, torch.Tensor):  \n",
    "        activations[layer_name] = output.shape  \n",
    "\n",
    "# 9. Recursively register hooks for **all** layers\n",
    "for name, layer in model.named_modules():\n",
    "    if isinstance(layer, (nn.Conv2d, nn.BatchNorm2d, nn.SiLU, nn.Linear)):\n",
    "        layer.register_forward_hook(hook_fn)\n",
    "\n",
    "# 10. Define the evaluation function\n",
    "def evaluate_model(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for images, labels in tqdm(dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            activations.clear()  \n",
    "            \n",
    "            print(f\"\\nRaw Image Tensor Shape (Before Normalization): {images.shape}\")  \n",
    "            \n",
    "            outputs = model(images)  \n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)  \n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Print activations for all layers\n",
    "            print(\"\\n--- Layer Activations ---\")\n",
    "            for layer, shape in activations.items():\n",
    "                print(f\"{layer}: {shape}\")\n",
    "            break  \n",
    "\n",
    "    accuracy = (correct / total) * 100\n",
    "    return accuracy\n",
    "\n",
    "# 11. Evaluate the model on ImageNet validation set\n",
    "accuracy = evaluate_model(model, imagenet_val_loader)\n",
    "print(f\"EfficientNet-B0 Top-1 Accuracy on ImageNet: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
