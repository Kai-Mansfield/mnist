{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (8): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=True)\n",
       "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import xml.etree.ElementTree as ET\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Define paths for ImageNet validation images and annotations\n",
    "IMAGE_DIR = \"/home/kajm20/mnist/ILSVRC/Data/CLS-LOC/val\"  # Path to validation images\n",
    "ANNOTATION_DIR = \"/home/kajm20/mnist/ILSVRC/Annotations/CLS-LOC/val\"  # Path to XML annotations\n",
    "\n",
    "imagenet_transform = transforms.Compose([\n",
    "    transforms.Resize(256),  \n",
    "    transforms.CenterCrop(224),  \n",
    "    transforms.ToTensor()  # Only convert to tensor (values in range [0,1])\n",
    "])\n",
    "\n",
    "# 3. Load the synset mapping\n",
    "synset_mapping_path = \"/home/kajm20/mnist/ILSVRC/LOC_synset_mapping.txt\"\n",
    "wordnet_to_imagenet = {}\n",
    "\n",
    "# Load synset mapping from file\n",
    "with open(synset_mapping_path) as f:\n",
    "    for idx, line in enumerate(f.readlines()):\n",
    "        wordnet_id, _ = line.split(' ', 1)  # Get WordNet ID from the line (skip class name)\n",
    "        wordnet_to_imagenet[wordnet_id] = idx  # Map WordNet ID to class index\n",
    "\n",
    "# 4. Define the custom dataset class\n",
    "class ImageNetValDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Get all annotation file names\n",
    "        self.annotation_files = sorted(os.listdir(annotation_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotation_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get annotation file path\n",
    "        annotation_path = os.path.join(self.annotation_dir, self.annotation_files[idx])\n",
    "        \n",
    "        # Parse XML to extract class label\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "        wordnet_id = root.find(\"object\").find(\"name\").text  # WordNet ID, e.g., 'n01751748'\n",
    "\n",
    "        # Use the synset mapping to convert WordNet ID to ImageNet class index\n",
    "        class_idx = wordnet_to_imagenet.get(wordnet_id, -1)  # Default to -1 if not found (shouldn't happen)\n",
    "\n",
    "        # Get image filename from XML and construct image path\n",
    "        image_filename = root.find(\"filename\").text + \".JPEG\"\n",
    "        image_path = os.path.join(self.image_dir, image_filename)\n",
    "\n",
    "        # Load and transform image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, class_idx\n",
    "\n",
    "# 5. Initialize the dataset and dataloader\n",
    "imagenet_val_dataset = ImageNetValDataset(IMAGE_DIR, ANNOTATION_DIR, transform=imagenet_transform)\n",
    "imagenet_val_loader = DataLoader(imagenet_val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# 6. Define the model (EfficientNet-B0 with pre-trained weights)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.efficientnet_b0(weights='DEFAULT')  # Load pre-trained EfficientNet-B0 model\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:52<00:00, 29.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "torch.Size([3, 224, 224])\n",
      "tensor([[[0.4840, 0.4845, 0.4852,  ..., 0.4854, 0.4854, 0.4853],\n",
      "         [0.4837, 0.4843, 0.4852,  ..., 0.4858, 0.4857, 0.4857],\n",
      "         [0.4842, 0.4848, 0.4855,  ..., 0.4859, 0.4856, 0.4856],\n",
      "         ...,\n",
      "         [0.4602, 0.4600, 0.4605,  ..., 0.4602, 0.4602, 0.4605],\n",
      "         [0.4600, 0.4599, 0.4602,  ..., 0.4599, 0.4598, 0.4601],\n",
      "         [0.4596, 0.4593, 0.4595,  ..., 0.4599, 0.4598, 0.4601]],\n",
      "\n",
      "        [[0.4811, 0.4815, 0.4820,  ..., 0.4824, 0.4825, 0.4824],\n",
      "         [0.4807, 0.4811, 0.4818,  ..., 0.4826, 0.4825, 0.4825],\n",
      "         [0.4807, 0.4812, 0.4818,  ..., 0.4825, 0.4822, 0.4823],\n",
      "         ...,\n",
      "         [0.4412, 0.4410, 0.4413,  ..., 0.4408, 0.4410, 0.4417],\n",
      "         [0.4413, 0.4411, 0.4411,  ..., 0.4407, 0.4408, 0.4413],\n",
      "         [0.4410, 0.4406, 0.4405,  ..., 0.4410, 0.4410, 0.4413]],\n",
      "\n",
      "        [[0.4426, 0.4430, 0.4432,  ..., 0.4434, 0.4436, 0.4437],\n",
      "         [0.4420, 0.4424, 0.4428,  ..., 0.4433, 0.4434, 0.4437],\n",
      "         [0.4419, 0.4423, 0.4426,  ..., 0.4429, 0.4428, 0.4431],\n",
      "         ...,\n",
      "         [0.3885, 0.3882, 0.3885,  ..., 0.3875, 0.3877, 0.3883],\n",
      "         [0.3888, 0.3885, 0.3885,  ..., 0.3874, 0.3875, 0.3881],\n",
      "         [0.3886, 0.3882, 0.3880,  ..., 0.3877, 0.3877, 0.3882]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_sum = torch.zeros(3, 224, 224).to(device)  # Ensure it's on the same device\n",
    "num_images = 0  # Track total images processed\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    global batch_sum, num_images  # Allow modification of global variables\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for images, labels in tqdm(dataloader):\n",
    "            images = images.to(device)  # Move to same device as batch_sum\n",
    "            batch_sum += torch.sum(images, dim=0)  # Accumulate sum across batches\n",
    "            num_images += images.shape[0]  # Update total image count\n",
    "\n",
    "evaluate_model(model, imagenet_val_loader)\n",
    "\n",
    "# Compute the average pixel value across all images\n",
    "print(num_images)\n",
    "batch_mean = batch_sum / num_images  # Element-wise division\n",
    "print(batch_mean.shape)\n",
    "print(batch_mean)  # Should be [3, 224, 224]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> <class 'torchvision.models.efficientnet.EfficientNet'>\n",
      "features -> <class 'torch.nn.modules.container.Sequential'>\n",
      "features.0 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.0.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.0.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.1 -> <class 'torch.nn.modules.container.Sequential'>\n",
      "features.1.0 -> <class 'torchvision.models.efficientnet.MBConv'>\n",
      "features.1.0.block -> <class 'torch.nn.modules.container.Sequential'>\n",
      "features.1.0.block.0 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.1.0.block.0.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.1.0.block.0.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.1.0.block.1 -> <class 'torchvision.ops.misc.SqueezeExcitation'>\n",
      "features.1.0.block.1.fc1 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.1.0.block.1.fc2 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.1.0.block.2 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.1.0.block.2.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.1.0.block.2.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.2 -> <class 'torch.nn.modules.container.Sequential'>\n",
      "features.2.0 -> <class 'torchvision.models.efficientnet.MBConv'>\n",
      "features.2.0.block -> <class 'torch.nn.modules.container.Sequential'>\n",
      "features.2.0.block.0 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.2.0.block.0.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.2.0.block.0.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.2.0.block.1 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.2.0.block.1.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.2.0.block.1.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.2.0.block.2 -> <class 'torchvision.ops.misc.SqueezeExcitation'>\n",
      "features.2.0.block.2.fc1 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.2.0.block.2.fc2 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.2.0.block.3 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.2.0.block.3.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.2.0.block.3.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.2.1 -> <class 'torchvision.models.efficientnet.MBConv'>\n",
      "features.2.1.block -> <class 'torch.nn.modules.container.Sequential'>\n",
      "features.2.1.block.0 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.2.1.block.0.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.2.1.block.0.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.2.1.block.1 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.2.1.block.1.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.2.1.block.1.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.2.1.block.2 -> <class 'torchvision.ops.misc.SqueezeExcitation'>\n",
      "features.2.1.block.2.fc1 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.2.1.block.2.fc2 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.2.1.block.3 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.2.1.block.3.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.2.1.block.3.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.3 -> <class 'torch.nn.modules.container.Sequential'>\n",
      "features.3.0 -> <class 'torchvision.models.efficientnet.MBConv'>\n",
      "features.3.0.block -> <class 'torch.nn.modules.container.Sequential'>\n",
      "features.3.0.block.0 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.3.0.block.0.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.3.0.block.0.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.3.0.block.1 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.3.0.block.1.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.3.0.block.1.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.3.0.block.2 -> <class 'torchvision.ops.misc.SqueezeExcitation'>\n",
      "features.3.0.block.2.fc1 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.3.0.block.2.fc2 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.3.0.block.3 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.3.0.block.3.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.3.0.block.3.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.3.1 -> <class 'torchvision.models.efficientnet.MBConv'>\n",
      "features.3.1.block -> <class 'torch.nn.modules.container.Sequential'>\n",
      "features.3.1.block.0 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.3.1.block.0.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.3.1.block.0.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.3.1.block.1 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.3.1.block.1.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.3.1.block.1.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.3.1.block.2 -> <class 'torchvision.ops.misc.SqueezeExcitation'>\n",
      "features.3.1.block.2.fc1 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.3.1.block.2.fc2 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.3.1.block.3 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.3.1.block.3.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.3.1.block.3.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.4 -> <class 'torch.nn.modules.container.Sequential'>\n",
      "features.4.0 -> <class 'torchvision.models.efficientnet.MBConv'>\n",
      "features.4.0.block -> <class 'torch.nn.modules.container.Sequential'>\n",
      "features.4.0.block.0 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.4.0.block.0.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.4.0.block.0.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.4.0.block.1 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.4.0.block.1.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.4.0.block.1.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.4.0.block.2 -> <class 'torchvision.ops.misc.SqueezeExcitation'>\n",
      "features.4.0.block.2.fc1 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.4.0.block.2.fc2 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.4.0.block.3 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.4.0.block.3.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.4.0.block.3.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.4.1 -> <class 'torchvision.models.efficientnet.MBConv'>\n",
      "features.4.1.block -> <class 'torch.nn.modules.container.Sequential'>\n",
      "features.4.1.block.0 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.4.1.block.0.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.4.1.block.0.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.4.1.block.1 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.4.1.block.1.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.4.1.block.1.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.4.1.block.2 -> <class 'torchvision.ops.misc.SqueezeExcitation'>\n",
      "features.4.1.block.2.fc1 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.4.1.block.2.fc2 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.4.1.block.3 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.4.1.block.3.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.4.1.block.3.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.4.2 -> <class 'torchvision.models.efficientnet.MBConv'>\n",
      "features.4.2.block -> <class 'torch.nn.modules.container.Sequential'>\n",
      "features.4.2.block.0 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.4.2.block.0.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.4.2.block.0.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.4.2.block.1 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.4.2.block.1.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.4.2.block.1.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.4.2.block.2 -> <class 'torchvision.ops.misc.SqueezeExcitation'>\n",
      "features.4.2.block.2.fc1 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.4.2.block.2.fc2 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.4.2.block.3 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.4.2.block.3.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.4.2.block.3.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.5 -> <class 'torch.nn.modules.container.Sequential'>\n",
      "features.5.0 -> <class 'torchvision.models.efficientnet.MBConv'>\n",
      "features.5.0.block -> <class 'torch.nn.modules.container.Sequential'>\n",
      "features.5.0.block.0 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.5.0.block.0.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.5.0.block.0.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.5.0.block.1 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.5.0.block.1.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.5.0.block.1.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.5.0.block.2 -> <class 'torchvision.ops.misc.SqueezeExcitation'>\n",
      "features.5.0.block.2.fc1 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.5.0.block.2.fc2 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.5.0.block.3 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.5.0.block.3.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.5.0.block.3.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.5.1 -> <class 'torchvision.models.efficientnet.MBConv'>\n",
      "features.5.1.block -> <class 'torch.nn.modules.container.Sequential'>\n",
      "features.5.1.block.0 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.5.1.block.0.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.5.1.block.0.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.5.1.block.1 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.5.1.block.1.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.5.1.block.1.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.5.1.block.2 -> <class 'torchvision.ops.misc.SqueezeExcitation'>\n",
      "features.5.1.block.2.fc1 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.5.1.block.2.fc2 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.5.1.block.3 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.5.1.block.3.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.5.1.block.3.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.5.2 -> <class 'torchvision.models.efficientnet.MBConv'>\n",
      "features.5.2.block -> <class 'torch.nn.modules.container.Sequential'>\n",
      "features.5.2.block.0 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.5.2.block.0.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.5.2.block.0.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.5.2.block.1 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.5.2.block.1.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.5.2.block.1.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.5.2.block.2 -> <class 'torchvision.ops.misc.SqueezeExcitation'>\n",
      "features.5.2.block.2.fc1 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.5.2.block.2.fc2 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.5.2.block.3 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.5.2.block.3.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.5.2.block.3.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.6 -> <class 'torch.nn.modules.container.Sequential'>\n",
      "features.6.0 -> <class 'torchvision.models.efficientnet.MBConv'>\n",
      "features.6.0.block -> <class 'torch.nn.modules.container.Sequential'>\n",
      "features.6.0.block.0 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.6.0.block.0.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.6.0.block.0.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.6.0.block.1 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.6.0.block.1.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.6.0.block.1.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.6.0.block.2 -> <class 'torchvision.ops.misc.SqueezeExcitation'>\n",
      "features.6.0.block.2.fc1 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.6.0.block.2.fc2 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.6.0.block.3 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.6.0.block.3.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.6.0.block.3.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.6.1 -> <class 'torchvision.models.efficientnet.MBConv'>\n",
      "features.6.1.block -> <class 'torch.nn.modules.container.Sequential'>\n",
      "features.6.1.block.0 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.6.1.block.0.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.6.1.block.0.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.6.1.block.1 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.6.1.block.1.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.6.1.block.1.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.6.1.block.2 -> <class 'torchvision.ops.misc.SqueezeExcitation'>\n",
      "features.6.1.block.2.fc1 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.6.1.block.2.fc2 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.6.1.block.3 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.6.1.block.3.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.6.1.block.3.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.6.2 -> <class 'torchvision.models.efficientnet.MBConv'>\n",
      "features.6.2.block -> <class 'torch.nn.modules.container.Sequential'>\n",
      "features.6.2.block.0 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.6.2.block.0.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.6.2.block.0.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.6.2.block.1 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.6.2.block.1.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.6.2.block.1.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.6.2.block.2 -> <class 'torchvision.ops.misc.SqueezeExcitation'>\n",
      "features.6.2.block.2.fc1 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.6.2.block.2.fc2 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.6.2.block.3 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.6.2.block.3.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.6.2.block.3.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.6.3 -> <class 'torchvision.models.efficientnet.MBConv'>\n",
      "features.6.3.block -> <class 'torch.nn.modules.container.Sequential'>\n",
      "features.6.3.block.0 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.6.3.block.0.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.6.3.block.0.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.6.3.block.1 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.6.3.block.1.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.6.3.block.1.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.6.3.block.2 -> <class 'torchvision.ops.misc.SqueezeExcitation'>\n",
      "features.6.3.block.2.fc1 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.6.3.block.2.fc2 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.6.3.block.3 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.6.3.block.3.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.6.3.block.3.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.7 -> <class 'torch.nn.modules.container.Sequential'>\n",
      "features.7.0 -> <class 'torchvision.models.efficientnet.MBConv'>\n",
      "features.7.0.block -> <class 'torch.nn.modules.container.Sequential'>\n",
      "features.7.0.block.0 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.7.0.block.0.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.7.0.block.0.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.7.0.block.1 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.7.0.block.1.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.7.0.block.1.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.7.0.block.2 -> <class 'torchvision.ops.misc.SqueezeExcitation'>\n",
      "features.7.0.block.2.fc1 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.7.0.block.2.fc2 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.7.0.block.3 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.7.0.block.3.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.7.0.block.3.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.8 -> <class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "features.8.0 -> <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.8.1 -> <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "classifier -> <class 'torch.nn.modules.container.Sequential'>\n",
      "classifier.1 -> <class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "for name, layer in model.named_modules():\n",
    "    if any(p.requires_grad for p in layer.parameters()):  # Default recurse=True\n",
    "        print(name, \"->\", type(layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Forward Methods of All Layers ---\n",
      "\n",
      "\n",
      "---  (EfficientNet) ---\n",
      "\n",
      "    def forward(self, x: Tensor) -> Tensor:\n",
      "        return self._forward_impl(x)\n",
      "\n",
      "\n",
      "--- features (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.1 (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.1.0 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.1.0.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.1.0.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.1.0.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.1.0.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.1.0.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.1.0.block.1 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.1.0.block.1.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.1.0.block.1.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.1.0.block.1.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.1.0.block.1.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.1.0.block.1.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.1.0.block.2 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.1.0.block.2.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.1.0.block.2.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.1.0.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.2 (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.2.0 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.2.0.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.2.0.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.2.0.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.2.0.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.2.0.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.2.0.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.2.0.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.2.0.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.2.0.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.2.0.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.2.0.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.2.0.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.2.0.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.2.0.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.2.0.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.2.0.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.2.0.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.2.0.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.2.0.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.2.1 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.2.1.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.2.1.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.2.1.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.2.1.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.2.1.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.2.1.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.2.1.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.2.1.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.2.1.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.2.1.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.2.1.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.2.1.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.2.1.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.2.1.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.2.1.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.2.1.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.2.1.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.2.1.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.2.1.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.3 (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.3.0 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.3.0.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.3.0.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.3.0.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.3.0.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.3.0.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.3.0.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.3.0.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.3.0.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.3.0.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.3.0.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.3.0.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.3.0.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.3.0.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.3.0.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.3.0.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.3.0.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.3.0.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.3.0.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.3.0.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.3.1 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.3.1.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.3.1.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.3.1.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.3.1.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.3.1.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.3.1.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.3.1.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.3.1.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.3.1.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.3.1.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.3.1.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.3.1.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.3.1.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.3.1.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.3.1.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.3.1.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.3.1.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.3.1.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.3.1.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.4 (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.4.0 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.4.0.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.4.0.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.4.0.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.0.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.4.0.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.4.0.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.4.0.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.0.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.4.0.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.4.0.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.4.0.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.4.0.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.0.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.0.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.4.0.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.4.0.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.4.0.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.0.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.4.0.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.4.1 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.4.1.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.4.1.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.4.1.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.1.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.4.1.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.4.1.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.4.1.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.1.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.4.1.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.4.1.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.4.1.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.4.1.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.1.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.1.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.4.1.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.4.1.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.4.1.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.1.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.4.1.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.4.2 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.4.2.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.4.2.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.4.2.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.2.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.4.2.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.4.2.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.4.2.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.2.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.4.2.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.4.2.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.4.2.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.4.2.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.2.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.2.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.4.2.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.4.2.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.4.2.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.4.2.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.4.2.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.5 (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.5.0 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.5.0.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.5.0.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.5.0.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.0.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.5.0.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.5.0.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.5.0.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.0.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.5.0.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.5.0.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.5.0.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.5.0.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.0.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.0.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.5.0.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.5.0.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.5.0.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.0.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.5.0.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.5.1 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.5.1.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.5.1.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.5.1.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.1.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.5.1.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.5.1.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.5.1.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.1.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.5.1.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.5.1.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.5.1.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.5.1.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.1.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.1.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.5.1.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.5.1.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.5.1.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.1.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.5.1.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.5.2 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.5.2.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.5.2.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.5.2.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.2.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.5.2.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.5.2.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.5.2.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.2.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.5.2.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.5.2.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.5.2.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.5.2.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.2.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.2.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.5.2.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.5.2.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.5.2.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.5.2.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.5.2.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.6 (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.0 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.6.0.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.0.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.0.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.0.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.6.0.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.6.0.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.0.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.0.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.6.0.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.6.0.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.6.0.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.6.0.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.0.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.0.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.6.0.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.6.0.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.0.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.0.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.6.0.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.6.1 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.6.1.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.1.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.1.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.1.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.6.1.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.6.1.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.1.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.1.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.6.1.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.6.1.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.6.1.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.6.1.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.1.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.1.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.6.1.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.6.1.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.1.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.1.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.6.1.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.6.2 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.6.2.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.2.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.2.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.2.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.6.2.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.6.2.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.2.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.2.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.6.2.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.6.2.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.6.2.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.6.2.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.2.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.2.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.6.2.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.6.2.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.2.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.2.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.6.2.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.6.3 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.6.3.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.3.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.3.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.3.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.6.3.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.6.3.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.3.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.3.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.6.3.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.6.3.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.6.3.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.6.3.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.3.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.3.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.6.3.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.6.3.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.6.3.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.6.3.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.6.3.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.7 (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.7.0 (MBConv) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        result = self.block(input)\n",
      "        if self.use_res_connect:\n",
      "            result = self.stochastic_depth(result)\n",
      "            result += input\n",
      "        return result\n",
      "\n",
      "\n",
      "--- features.7.0.block (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.7.0.block.0 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.7.0.block.0.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.7.0.block.0.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.7.0.block.0.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.7.0.block.1 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.7.0.block.1.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.7.0.block.1.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.7.0.block.1.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.7.0.block.2 (SqueezeExcitation) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        scale = self._scale(input)\n",
      "        return scale * input\n",
      "\n",
      "\n",
      "--- features.7.0.block.2.avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- features.7.0.block.2.fc1 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.7.0.block.2.fc2 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.7.0.block.2.activation (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- features.7.0.block.2.scale_activation (Sigmoid) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return torch.sigmoid(input)\n",
      "\n",
      "\n",
      "--- features.7.0.block.3 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.7.0.block.3.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.7.0.block.3.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.7.0.stochastic_depth (StochasticDepth) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
      "\n",
      "\n",
      "--- features.8 (Conv2dNormActivation) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- features.8.0 (Conv2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "\n",
      "--- features.8.1 (BatchNorm2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        self._check_input_dim(input)\n",
      "\n",
      "        # exponential_average_factor is set to self.momentum\n",
      "        # (when it is available) only so that it gets updated\n",
      "        # in ONNX graph when this node is exported to ONNX.\n",
      "        if self.momentum is None:\n",
      "            exponential_average_factor = 0.0\n",
      "        else:\n",
      "            exponential_average_factor = self.momentum\n",
      "\n",
      "        if self.training and self.track_running_stats:\n",
      "            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
      "            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n",
      "                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n",
      "                if self.momentum is None:  # use cumulative moving average\n",
      "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
      "                else:  # use exponential moving average\n",
      "                    exponential_average_factor = self.momentum\n",
      "\n",
      "        r\"\"\"\n",
      "        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n",
      "        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n",
      "        \"\"\"\n",
      "        if self.training:\n",
      "            bn_training = True\n",
      "        else:\n",
      "            bn_training = (self.running_mean is None) and (self.running_var is None)\n",
      "\n",
      "        r\"\"\"\n",
      "        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n",
      "        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n",
      "        used for normalization (i.e. in eval mode when buffers are not None).\n",
      "        \"\"\"\n",
      "        return F.batch_norm(\n",
      "            input,\n",
      "            # If buffers are not to be tracked, ensure that they won't be updated\n",
      "            self.running_mean\n",
      "            if not self.training or self.track_running_stats\n",
      "            else None,\n",
      "            self.running_var if not self.training or self.track_running_stats else None,\n",
      "            self.weight,\n",
      "            self.bias,\n",
      "            bn_training,\n",
      "            exponential_average_factor,\n",
      "            self.eps,\n",
      "        )\n",
      "\n",
      "\n",
      "--- features.8.2 (SiLU) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "\n",
      "--- avgpool (AdaptiveAvgPool2d) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.adaptive_avg_pool2d(input, self.output_size)\n",
      "\n",
      "\n",
      "--- classifier (Sequential) ---\n",
      "\n",
      "    def forward(self, input):\n",
      "        for module in self:\n",
      "            input = module(input)\n",
      "        return input\n",
      "\n",
      "\n",
      "--- classifier.0 (Dropout) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "\n",
      "--- classifier.1 (Linear) ---\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.linear(input, self.weight, self.bias)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw Image Tensor Shape (Before Normalization): torch.Size([1, 3, 224, 224])\n",
      "\n",
      "--- Layer Activation Shapes ---\n",
      "Conv2d (134778334259792): torch.Size([1, 32, 112, 112])\n",
      "BatchNorm2d (134778334260128): torch.Size([1, 32, 112, 112])\n",
      "SiLU (134778334260464): torch.Size([1, 32, 112, 112])\n",
      "Conv2dNormActivation (134778334259456): torch.Size([1, 32, 112, 112])\n",
      "Conv2d (134778334131088): torch.Size([1, 32, 112, 112])\n",
      "BatchNorm2d (134778334131408): torch.Size([1, 32, 112, 112])\n",
      "SiLU (134778334131728): torch.Size([1, 32, 112, 112])\n",
      "Conv2dNormActivation (134778334130768): torch.Size([1, 32, 112, 112])\n",
      "AdaptiveAvgPool2d (134778334251056): torch.Size([1, 32, 1, 1])\n",
      "Conv2d (134782382727696): torch.Size([1, 8, 1, 1])\n",
      "SiLU (134778334132048): torch.Size([1, 8, 1, 1])\n",
      "Conv2d (134778334126608): torch.Size([1, 32, 1, 1])\n",
      "Sigmoid (134778334261136): torch.Size([1, 32, 1, 1])\n",
      "SqueezeExcitation (134778334249712): torch.Size([1, 32, 112, 112])\n",
      "Conv2d (134778334132688): torch.Size([1, 16, 112, 112])\n",
      "BatchNorm2d (134778334133008): torch.Size([1, 16, 112, 112])\n",
      "Conv2dNormActivation (134778334132368): torch.Size([1, 16, 112, 112])\n",
      "Sequential (134778334261472): torch.Size([1, 16, 112, 112])\n",
      "MBConv (134778334260800): torch.Size([1, 16, 112, 112])\n",
      "Sequential (134778334133328): torch.Size([1, 16, 112, 112])\n",
      "Conv2d (134778334133968): torch.Size([1, 96, 112, 112])\n",
      "BatchNorm2d (134778334367072): torch.Size([1, 96, 112, 112])\n",
      "SiLU (134778334367376): torch.Size([1, 96, 112, 112])\n",
      "Conv2dNormActivation (134778334366768): torch.Size([1, 96, 112, 112])\n",
      "Conv2d (134778311442512): torch.Size([1, 96, 56, 56])\n",
      "BatchNorm2d (134778334367984): torch.Size([1, 96, 56, 56])\n",
      "SiLU (134778334368288): torch.Size([1, 96, 56, 56])\n",
      "Conv2dNormActivation (134778334367680): torch.Size([1, 96, 56, 56])\n",
      "AdaptiveAvgPool2d (134778311443152): torch.Size([1, 96, 1, 1])\n",
      "Conv2d (134778311443472): torch.Size([1, 4, 1, 1])\n",
      "SiLU (134782182584496): torch.Size([1, 4, 1, 1])\n",
      "Conv2d (134778311443792): torch.Size([1, 96, 1, 1])\n",
      "Sigmoid (134778311444112): torch.Size([1, 96, 1, 1])\n",
      "SqueezeExcitation (134778311442832): torch.Size([1, 96, 56, 56])\n",
      "Conv2d (134778311444432): torch.Size([1, 24, 56, 56])\n",
      "BatchNorm2d (134778333648592): torch.Size([1, 24, 56, 56])\n",
      "Conv2dNormActivation (134778337854960): torch.Size([1, 24, 56, 56])\n",
      "Sequential (134778311444752): torch.Size([1, 24, 56, 56])\n",
      "MBConv (134778334133648): torch.Size([1, 24, 56, 56])\n",
      "Conv2d (134778311445712): torch.Size([1, 144, 56, 56])\n",
      "BatchNorm2d (134778333972416): torch.Size([1, 144, 56, 56])\n",
      "SiLU (134778333970512): torch.Size([1, 144, 56, 56])\n",
      "Conv2dNormActivation (134778333972960): torch.Size([1, 144, 56, 56])\n",
      "Conv2d (134778311446032): torch.Size([1, 144, 56, 56])\n",
      "BatchNorm2d (134778333973504): torch.Size([1, 144, 56, 56])\n",
      "SiLU (134778333973776): torch.Size([1, 144, 56, 56])\n",
      "Conv2dNormActivation (134778333973232): torch.Size([1, 144, 56, 56])\n",
      "AdaptiveAvgPool2d (134778311446672): torch.Size([1, 144, 1, 1])\n",
      "Conv2d (134778311446992): torch.Size([1, 6, 1, 1])\n",
      "SiLU (134782213820240): torch.Size([1, 6, 1, 1])\n",
      "Conv2d (134778311447312): torch.Size([1, 144, 1, 1])\n",
      "Sigmoid (134778311447632): torch.Size([1, 144, 1, 1])\n",
      "SqueezeExcitation (134778311446352): torch.Size([1, 144, 56, 56])\n",
      "Conv2d (134778311447952): torch.Size([1, 24, 56, 56])\n",
      "BatchNorm2d (134778333974048): torch.Size([1, 24, 56, 56])\n",
      "Conv2dNormActivation (134778333485136): torch.Size([1, 24, 56, 56])\n",
      "Sequential (134778334368592): torch.Size([1, 24, 56, 56])\n",
      "StochasticDepth (134778311448272): torch.Size([1, 24, 56, 56])\n",
      "MBConv (134778311445392): torch.Size([1, 24, 56, 56])\n",
      "Sequential (134778334368896): torch.Size([1, 24, 56, 56])\n",
      "Conv2d (134778311448592): torch.Size([1, 144, 56, 56])\n",
      "BatchNorm2d (134778333974320): torch.Size([1, 144, 56, 56])\n",
      "SiLU (134778333488976): torch.Size([1, 144, 56, 56])\n",
      "Conv2dNormActivation (134778333487184): torch.Size([1, 144, 56, 56])\n",
      "Conv2d (134778311448912): torch.Size([1, 144, 28, 28])\n",
      "BatchNorm2d (134778333974592): torch.Size([1, 144, 28, 28])\n",
      "SiLU (134778338259936): torch.Size([1, 144, 28, 28])\n",
      "Conv2dNormActivation (134778338259216): torch.Size([1, 144, 28, 28])\n",
      "AdaptiveAvgPool2d (134778334369808): torch.Size([1, 144, 1, 1])\n",
      "Conv2d (134778311449232): torch.Size([1, 6, 1, 1])\n",
      "SiLU (134778338259696): torch.Size([1, 6, 1, 1])\n",
      "Conv2d (134778311449552): torch.Size([1, 144, 1, 1])\n",
      "Sigmoid (134778334370112): torch.Size([1, 144, 1, 1])\n",
      "SqueezeExcitation (134778334369504): torch.Size([1, 144, 28, 28])\n",
      "Conv2d (134778311449872): torch.Size([1, 40, 28, 28])\n",
      "BatchNorm2d (134778333974864): torch.Size([1, 40, 28, 28])\n",
      "Conv2dNormActivation (134778338260176): torch.Size([1, 40, 28, 28])\n",
      "Sequential (134778333648880): torch.Size([1, 40, 28, 28])\n",
      "MBConv (134778334369200): torch.Size([1, 40, 28, 28])\n",
      "Conv2d (134778311450192): torch.Size([1, 240, 28, 28])\n",
      "BatchNorm2d (134778333975136): torch.Size([1, 240, 28, 28])\n",
      "SiLU (134778338260656): torch.Size([1, 240, 28, 28])\n",
      "Conv2dNormActivation (134782204870976): torch.Size([1, 240, 28, 28])\n",
      "Conv2d (134778311450512): torch.Size([1, 240, 28, 28])\n",
      "BatchNorm2d (134778333975408): torch.Size([1, 240, 28, 28])\n",
      "SiLU (134778338261136): torch.Size([1, 240, 28, 28])\n",
      "Conv2dNormActivation (134778338260896): torch.Size([1, 240, 28, 28])\n",
      "AdaptiveAvgPool2d (134778334371328): torch.Size([1, 240, 1, 1])\n",
      "Conv2d (134778311450832): torch.Size([1, 10, 1, 1])\n",
      "SiLU (134778338257776): torch.Size([1, 10, 1, 1])\n",
      "Conv2d (134778311451152): torch.Size([1, 240, 1, 1])\n",
      "Sigmoid (134778334371632): torch.Size([1, 240, 1, 1])\n",
      "SqueezeExcitation (134778334371024): torch.Size([1, 240, 28, 28])\n",
      "Conv2d (134778311451472): torch.Size([1, 40, 28, 28])\n",
      "BatchNorm2d (134778333975680): torch.Size([1, 40, 28, 28])\n",
      "Conv2dNormActivation (134778338258976): torch.Size([1, 40, 28, 28])\n",
      "Sequential (134778333975952): torch.Size([1, 40, 28, 28])\n",
      "StochasticDepth (134778334371936): torch.Size([1, 40, 28, 28])\n",
      "MBConv (134778334370720): torch.Size([1, 40, 28, 28])\n",
      "Sequential (134778333976224): torch.Size([1, 40, 28, 28])\n",
      "Conv2d (134778311451792): torch.Size([1, 240, 28, 28])\n",
      "BatchNorm2d (134778333976496): torch.Size([1, 240, 28, 28])\n",
      "SiLU (134778338261616): torch.Size([1, 240, 28, 28])\n",
      "Conv2dNormActivation (134778338260416): torch.Size([1, 240, 28, 28])\n",
      "Conv2d (134778311452112): torch.Size([1, 240, 14, 14])\n",
      "BatchNorm2d (134778333976768): torch.Size([1, 240, 14, 14])\n",
      "SiLU (134778338262096): torch.Size([1, 240, 14, 14])\n",
      "Conv2dNormActivation (134778338261856): torch.Size([1, 240, 14, 14])\n",
      "AdaptiveAvgPool2d (134778333650320): torch.Size([1, 240, 1, 1])\n",
      "Conv2d (134778311452432): torch.Size([1, 10, 1, 1])\n",
      "SiLU (134778338262336): torch.Size([1, 10, 1, 1])\n",
      "Conv2d (134778311452752): torch.Size([1, 240, 1, 1])\n",
      "Sigmoid (134778333650608): torch.Size([1, 240, 1, 1])\n",
      "SqueezeExcitation (134778333650032): torch.Size([1, 240, 14, 14])\n",
      "Conv2d (134778311453072): torch.Size([1, 80, 14, 14])\n",
      "BatchNorm2d (134778333977040): torch.Size([1, 80, 14, 14])\n",
      "Conv2dNormActivation (134778338262576): torch.Size([1, 80, 14, 14])\n",
      "Sequential (134778333489232): torch.Size([1, 80, 14, 14])\n",
      "MBConv (134778333649744): torch.Size([1, 80, 14, 14])\n",
      "Conv2d (134778311453392): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d (134778333977584): torch.Size([1, 480, 14, 14])\n",
      "SiLU (134778311688272): torch.Size([1, 480, 14, 14])\n",
      "Conv2dNormActivation (134778338262816): torch.Size([1, 480, 14, 14])\n",
      "Conv2d (134778311453712): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d (134778333977856): torch.Size([1, 480, 14, 14])\n",
      "SiLU (134778311688752): torch.Size([1, 480, 14, 14])\n",
      "Conv2dNormActivation (134778311688512): torch.Size([1, 480, 14, 14])\n",
      "AdaptiveAvgPool2d (134778333978400): torch.Size([1, 480, 1, 1])\n",
      "Conv2d (134778311454032): torch.Size([1, 20, 1, 1])\n",
      "SiLU (134778311688992): torch.Size([1, 20, 1, 1])\n",
      "Conv2d (134778311454352): torch.Size([1, 480, 1, 1])\n",
      "Sigmoid (134778333978672): torch.Size([1, 480, 1, 1])\n",
      "SqueezeExcitation (134778333978128): torch.Size([1, 480, 14, 14])\n",
      "Conv2d (134778311454672): torch.Size([1, 80, 14, 14])\n",
      "BatchNorm2d (134778333978944): torch.Size([1, 80, 14, 14])\n",
      "Conv2dNormActivation (134778311689232): torch.Size([1, 80, 14, 14])\n",
      "Sequential (134778333489744): torch.Size([1, 80, 14, 14])\n",
      "StochasticDepth (134778333979216): torch.Size([1, 80, 14, 14])\n",
      "MBConv (134778333977312): torch.Size([1, 80, 14, 14])\n",
      "Conv2d (134778311454992): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d (134778333979760): torch.Size([1, 480, 14, 14])\n",
      "SiLU (134778311689712): torch.Size([1, 480, 14, 14])\n",
      "Conv2dNormActivation (134778311689472): torch.Size([1, 480, 14, 14])\n",
      "Conv2d (134778311455312): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d (134778333980032): torch.Size([1, 480, 14, 14])\n",
      "SiLU (134778311690192): torch.Size([1, 480, 14, 14])\n",
      "Conv2dNormActivation (134778311689952): torch.Size([1, 480, 14, 14])\n",
      "AdaptiveAvgPool2d (134778333980576): torch.Size([1, 480, 1, 1])\n",
      "Conv2d (134778311455632): torch.Size([1, 20, 1, 1])\n",
      "SiLU (134778311690432): torch.Size([1, 20, 1, 1])\n",
      "Conv2d (134778311455952): torch.Size([1, 480, 1, 1])\n",
      "Sigmoid (134778333980848): torch.Size([1, 480, 1, 1])\n",
      "SqueezeExcitation (134778333980304): torch.Size([1, 480, 14, 14])\n",
      "Conv2d (134778311456272): torch.Size([1, 80, 14, 14])\n",
      "BatchNorm2d (134778333981120): torch.Size([1, 80, 14, 14])\n",
      "Conv2dNormActivation (134778311690672): torch.Size([1, 80, 14, 14])\n",
      "Sequential (134778311690912): torch.Size([1, 80, 14, 14])\n",
      "StochasticDepth (134778333981392): torch.Size([1, 80, 14, 14])\n",
      "MBConv (134778333979488): torch.Size([1, 80, 14, 14])\n",
      "Sequential (134778311691152): torch.Size([1, 80, 14, 14])\n",
      "Conv2d (134778311456592): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d (134778333981664): torch.Size([1, 480, 14, 14])\n",
      "SiLU (134778311691632): torch.Size([1, 480, 14, 14])\n",
      "Conv2dNormActivation (134778311691392): torch.Size([1, 480, 14, 14])\n",
      "Conv2d (134778311456912): torch.Size([1, 480, 14, 14])\n",
      "BatchNorm2d (134778333981936): torch.Size([1, 480, 14, 14])\n",
      "SiLU (134778311692112): torch.Size([1, 480, 14, 14])\n",
      "Conv2dNormActivation (134778311691872): torch.Size([1, 480, 14, 14])\n",
      "AdaptiveAvgPool2d (134778333490256): torch.Size([1, 480, 1, 1])\n",
      "Conv2d (134778311457232): torch.Size([1, 20, 1, 1])\n",
      "SiLU (134778311692352): torch.Size([1, 20, 1, 1])\n",
      "Conv2d (134778311457552): torch.Size([1, 480, 1, 1])\n",
      "Sigmoid (134778333490512): torch.Size([1, 480, 1, 1])\n",
      "SqueezeExcitation (134778333489488): torch.Size([1, 480, 14, 14])\n",
      "Conv2d (134778311457872): torch.Size([1, 112, 14, 14])\n",
      "BatchNorm2d (134778333982208): torch.Size([1, 112, 14, 14])\n",
      "Conv2dNormActivation (134778311692592): torch.Size([1, 112, 14, 14])\n",
      "Sequential (134778311805424): torch.Size([1, 112, 14, 14])\n",
      "MBConv (134778333490000): torch.Size([1, 112, 14, 14])\n",
      "Conv2d (134778311458192): torch.Size([1, 672, 14, 14])\n",
      "BatchNorm2d (134778333982480): torch.Size([1, 672, 14, 14])\n",
      "SiLU (134778311693072): torch.Size([1, 672, 14, 14])\n",
      "Conv2dNormActivation (134778311692832): torch.Size([1, 672, 14, 14])\n",
      "Conv2d (134778311458512): torch.Size([1, 672, 14, 14])\n",
      "BatchNorm2d (134778333982752): torch.Size([1, 672, 14, 14])\n",
      "SiLU (134778311693552): torch.Size([1, 672, 14, 14])\n",
      "Conv2dNormActivation (134778311693312): torch.Size([1, 672, 14, 14])\n",
      "AdaptiveAvgPool2d (134778333491536): torch.Size([1, 672, 1, 1])\n",
      "Conv2d (134778311852112): torch.Size([1, 28, 1, 1])\n",
      "SiLU (134778311693792): torch.Size([1, 28, 1, 1])\n",
      "Conv2d (134778311852432): torch.Size([1, 672, 1, 1])\n",
      "Sigmoid (134778333491792): torch.Size([1, 672, 1, 1])\n",
      "SqueezeExcitation (134778333491280): torch.Size([1, 672, 14, 14])\n",
      "Conv2d (134778311852752): torch.Size([1, 112, 14, 14])\n",
      "BatchNorm2d (134778333983024): torch.Size([1, 112, 14, 14])\n",
      "Conv2dNormActivation (134778311694032): torch.Size([1, 112, 14, 14])\n",
      "Sequential (134778311810128): torch.Size([1, 112, 14, 14])\n",
      "StochasticDepth (134778333492304): torch.Size([1, 112, 14, 14])\n",
      "MBConv (134778333490768): torch.Size([1, 112, 14, 14])\n",
      "Conv2d (134778311853072): torch.Size([1, 672, 14, 14])\n",
      "BatchNorm2d (134778333983296): torch.Size([1, 672, 14, 14])\n",
      "SiLU (134778311694752): torch.Size([1, 672, 14, 14])\n",
      "Conv2dNormActivation (134778311694512): torch.Size([1, 672, 14, 14])\n",
      "Conv2d (134778311853392): torch.Size([1, 672, 14, 14])\n",
      "BatchNorm2d (134778333983568): torch.Size([1, 672, 14, 14])\n",
      "SiLU (134778311695232): torch.Size([1, 672, 14, 14])\n",
      "Conv2dNormActivation (134778311694992): torch.Size([1, 672, 14, 14])\n",
      "AdaptiveAvgPool2d (134778311695712): torch.Size([1, 672, 1, 1])\n",
      "Conv2d (134778311853712): torch.Size([1, 28, 1, 1])\n",
      "SiLU (134778311695952): torch.Size([1, 28, 1, 1])\n",
      "Conv2d (134778311854032): torch.Size([1, 672, 1, 1])\n",
      "Sigmoid (134778311696192): torch.Size([1, 672, 1, 1])\n",
      "SqueezeExcitation (134778311695472): torch.Size([1, 672, 14, 14])\n",
      "Conv2d (134778311854352): torch.Size([1, 112, 14, 14])\n",
      "BatchNorm2d (134778333983840): torch.Size([1, 112, 14, 14])\n",
      "Conv2dNormActivation (134778311696432): torch.Size([1, 112, 14, 14])\n",
      "Sequential (134778311814832): torch.Size([1, 112, 14, 14])\n",
      "StochasticDepth (134778311696672): torch.Size([1, 112, 14, 14])\n",
      "MBConv (134778311694272): torch.Size([1, 112, 14, 14])\n",
      "Sequential (134778311815504): torch.Size([1, 112, 14, 14])\n",
      "Conv2d (134778311854672): torch.Size([1, 672, 14, 14])\n",
      "BatchNorm2d (134778333984112): torch.Size([1, 672, 14, 14])\n",
      "SiLU (134778311697392): torch.Size([1, 672, 14, 14])\n",
      "Conv2dNormActivation (134778311697152): torch.Size([1, 672, 14, 14])\n",
      "Conv2d (134778311854992): torch.Size([1, 672, 7, 7])\n",
      "BatchNorm2d (134778333984384): torch.Size([1, 672, 7, 7])\n",
      "SiLU (134778311697872): torch.Size([1, 672, 7, 7])\n",
      "Conv2dNormActivation (134778311697632): torch.Size([1, 672, 7, 7])\n",
      "AdaptiveAvgPool2d (134778311698352): torch.Size([1, 672, 1, 1])\n",
      "Conv2d (134778311855312): torch.Size([1, 28, 1, 1])\n",
      "SiLU (134778311698592): torch.Size([1, 28, 1, 1])\n",
      "Conv2d (134778311855632): torch.Size([1, 672, 1, 1])\n",
      "Sigmoid (134778311698832): torch.Size([1, 672, 1, 1])\n",
      "SqueezeExcitation (134778311698112): torch.Size([1, 672, 7, 7])\n",
      "Conv2d (134778311855952): torch.Size([1, 192, 7, 7])\n",
      "BatchNorm2d (134778333984656): torch.Size([1, 192, 7, 7])\n",
      "Conv2dNormActivation (134778311699072): torch.Size([1, 192, 7, 7])\n",
      "Sequential (134778311951312): torch.Size([1, 192, 7, 7])\n",
      "MBConv (134778311696912): torch.Size([1, 192, 7, 7])\n",
      "Conv2d (134778311856272): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (134778333984928): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (134778311700032): torch.Size([1, 1152, 7, 7])\n",
      "Conv2dNormActivation (134778311699792): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (134778311856592): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (134778333985200): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (134778311700512): torch.Size([1, 1152, 7, 7])\n",
      "Conv2dNormActivation (134778311700272): torch.Size([1, 1152, 7, 7])\n",
      "AdaptiveAvgPool2d (134778311700752): torch.Size([1, 1152, 1, 1])\n",
      "Conv2d (134778311856912): torch.Size([1, 48, 1, 1])\n",
      "SiLU (134778311700992): torch.Size([1, 48, 1, 1])\n",
      "Conv2d (134778311857232): torch.Size([1, 1152, 1, 1])\n",
      "Sigmoid (134778311954896): torch.Size([1, 1152, 1, 1])\n",
      "SqueezeExcitation (134778311954000): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (134778311857552): torch.Size([1, 192, 7, 7])\n",
      "BatchNorm2d (134778333985472): torch.Size([1, 192, 7, 7])\n",
      "Conv2dNormActivation (134778311701232): torch.Size([1, 192, 7, 7])\n",
      "Sequential (134778311956464): torch.Size([1, 192, 7, 7])\n",
      "StochasticDepth (134778311701472): torch.Size([1, 192, 7, 7])\n",
      "MBConv (134778311699552): torch.Size([1, 192, 7, 7])\n",
      "Conv2d (134778311857872): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (134778333985744): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (134778311702192): torch.Size([1, 1152, 7, 7])\n",
      "Conv2dNormActivation (134778311701952): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (134778311858192): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (134778333986016): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (134778311702672): torch.Size([1, 1152, 7, 7])\n",
      "Conv2dNormActivation (134778311702432): torch.Size([1, 1152, 7, 7])\n",
      "AdaptiveAvgPool2d (134778311702912): torch.Size([1, 1152, 1, 1])\n",
      "Conv2d (134778311858512): torch.Size([1, 48, 1, 1])\n",
      "SiLU (134778311703152): torch.Size([1, 48, 1, 1])\n",
      "Conv2d (134778311858832): torch.Size([1, 1152, 1, 1])\n",
      "Sigmoid (134778311960048): torch.Size([1, 1152, 1, 1])\n",
      "SqueezeExcitation (134778311959152): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (134778311859152): torch.Size([1, 192, 7, 7])\n",
      "BatchNorm2d (134778333986288): torch.Size([1, 192, 7, 7])\n",
      "Conv2dNormActivation (134778311703392): torch.Size([1, 192, 7, 7])\n",
      "Sequential (134778311961616): torch.Size([1, 192, 7, 7])\n",
      "StochasticDepth (134778311703632): torch.Size([1, 192, 7, 7])\n",
      "MBConv (134778311701712): torch.Size([1, 192, 7, 7])\n",
      "Conv2d (134778311859472): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (134778333986560): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (134778311704352): torch.Size([1, 1152, 7, 7])\n",
      "Conv2dNormActivation (134778311704112): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (134778311859792): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (134778312097872): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (134778312081728): torch.Size([1, 1152, 7, 7])\n",
      "Conv2dNormActivation (134778312081488): torch.Size([1, 1152, 7, 7])\n",
      "AdaptiveAvgPool2d (134778312081968): torch.Size([1, 1152, 1, 1])\n",
      "Conv2d (134778311860112): torch.Size([1, 48, 1, 1])\n",
      "SiLU (134778312082208): torch.Size([1, 48, 1, 1])\n",
      "Conv2d (134778311860432): torch.Size([1, 1152, 1, 1])\n",
      "Sigmoid (134778311965200): torch.Size([1, 1152, 1, 1])\n",
      "SqueezeExcitation (134778311964304): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (134778311860752): torch.Size([1, 192, 7, 7])\n",
      "BatchNorm2d (134778312098144): torch.Size([1, 192, 7, 7])\n",
      "Conv2dNormActivation (134778312082448): torch.Size([1, 192, 7, 7])\n",
      "Sequential (134778312130864): torch.Size([1, 192, 7, 7])\n",
      "StochasticDepth (134778312082688): torch.Size([1, 192, 7, 7])\n",
      "MBConv (134778311703872): torch.Size([1, 192, 7, 7])\n",
      "Sequential (134778312131536): torch.Size([1, 192, 7, 7])\n",
      "Conv2d (134778311861072): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (134778312098416): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (134778312083408): torch.Size([1, 1152, 7, 7])\n",
      "Conv2dNormActivation (134778312083168): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (134778311861392): torch.Size([1, 1152, 7, 7])\n",
      "BatchNorm2d (134778312098688): torch.Size([1, 1152, 7, 7])\n",
      "SiLU (134778312083888): torch.Size([1, 1152, 7, 7])\n",
      "Conv2dNormActivation (134778312083648): torch.Size([1, 1152, 7, 7])\n",
      "AdaptiveAvgPool2d (134778312084128): torch.Size([1, 1152, 1, 1])\n",
      "Conv2d (134778311861712): torch.Size([1, 48, 1, 1])\n",
      "SiLU (134778312084368): torch.Size([1, 48, 1, 1])\n",
      "Conv2d (134778311862032): torch.Size([1, 1152, 1, 1])\n",
      "Sigmoid (134778312134896): torch.Size([1, 1152, 1, 1])\n",
      "SqueezeExcitation (134778312134000): torch.Size([1, 1152, 7, 7])\n",
      "Conv2d (134778311862352): torch.Size([1, 320, 7, 7])\n",
      "BatchNorm2d (134778312098960): torch.Size([1, 320, 7, 7])\n",
      "Conv2dNormActivation (134778312084608): torch.Size([1, 320, 7, 7])\n",
      "Sequential (134778312136464): torch.Size([1, 320, 7, 7])\n",
      "MBConv (134778312082928): torch.Size([1, 320, 7, 7])\n",
      "Sequential (134778312137136): torch.Size([1, 320, 7, 7])\n",
      "Conv2d (134778311862672): torch.Size([1, 1280, 7, 7])\n",
      "BatchNorm2d (134778312099232): torch.Size([1, 1280, 7, 7])\n",
      "SiLU (134778312085328): torch.Size([1, 1280, 7, 7])\n",
      "Conv2dNormActivation (134778312085088): torch.Size([1, 1280, 7, 7])\n",
      "Sequential (134778312138480): torch.Size([1, 1280, 7, 7])\n",
      "AdaptiveAvgPool2d (134778312085568): torch.Size([1, 1280, 1, 1])\n",
      "Dropout (134778334262144): torch.Size([1, 1280])\n",
      "Linear (134778334262480): torch.Size([1, 1000])\n",
      "Sequential (134778312139600): torch.Size([1, 1000])\n",
      "EfficientNet (134778334259120): torch.Size([1, 1000])\n",
      "EfficientNet-B0 Top-1 Accuracy on ImageNet: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import xml.etree.ElementTree as ET\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import inspect  # Import inspect to get source code of forward methods\n",
    "\n",
    "# 1. Define paths for ImageNet validation images and annotations\n",
    "IMAGE_DIR = \"/home/kajm20/mnist/ILSVRC/Data/CLS-LOC/val\"  \n",
    "ANNOTATION_DIR = \"/home/kajm20/mnist/ILSVRC/Annotations/CLS-LOC/val\"  \n",
    "\n",
    "imagenet_transform = transforms.Compose([\n",
    "    transforms.Resize(256),  \n",
    "    transforms.CenterCrop(224),  \n",
    "    transforms.ToTensor()  # Only convert to tensor (values in range [0,1])\n",
    "])\n",
    "\n",
    "# 3. Load the synset mapping\n",
    "synset_mapping_path = \"/home/kajm20/mnist/ILSVRC/LOC_synset_mapping.txt\"\n",
    "wordnet_to_imagenet = {}\n",
    "\n",
    "with open(synset_mapping_path) as f:\n",
    "    for idx, line in enumerate(f.readlines()):\n",
    "        wordnet_id, _ = line.split(' ', 1)\n",
    "        wordnet_to_imagenet[wordnet_id] = idx  \n",
    "\n",
    "# 4. Define the custom dataset class\n",
    "class ImageNetValDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.transform = transform\n",
    "        self.annotation_files = sorted(os.listdir(annotation_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotation_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation_path = os.path.join(self.annotation_dir, self.annotation_files[idx])\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "        wordnet_id = root.find(\"object\").find(\"name\").text  \n",
    "\n",
    "        class_idx = wordnet_to_imagenet.get(wordnet_id, -1)  \n",
    "        image_filename = root.find(\"filename\").text + \".JPEG\"\n",
    "        image_path = os.path.join(self.image_dir, image_filename)\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, class_idx\n",
    "\n",
    "# 5. Initialize the dataset and dataloader\n",
    "imagenet_val_dataset = ImageNetValDataset(IMAGE_DIR, ANNOTATION_DIR, transform=imagenet_transform)\n",
    "imagenet_val_loader = DataLoader(imagenet_val_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "# 6. Define the model (EfficientNet-B0 with pre-trained weights)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.efficientnet_b0(weights='DEFAULT')  \n",
    "model.to(device)\n",
    "model.eval()  \n",
    "\n",
    "# 7. Dictionaries to store activations\n",
    "activation_shapes = {}  # Stores activation shapes\n",
    "activation_values = {}  # Stores activation values\n",
    "\n",
    "# 8. Hook function to store activations for **ALL** layers\n",
    "def hook_fn(module, input, output):\n",
    "    layer_name = f\"{module.__class__.__name__} ({id(module)})\" \n",
    "    activation_shapes[layer_name] = output.shape  # Store shape\n",
    "    activation_values[layer_name] = output.detach().cpu()  # Store values (moved to CPU)\n",
    "\n",
    "# 9. Recursively register hooks for **all** relevant layers\n",
    "for name, layer in model.named_modules():\n",
    "    layer.register_forward_hook(hook_fn)\n",
    "\n",
    "# 10. Print the forward methods of all layers\n",
    "def print_forward_methods(model):\n",
    "    print(\"\\n--- Forward Methods of All Layers ---\\n\")\n",
    "    for name, layer in model.named_modules():\n",
    "        if hasattr(layer, \"forward\"):  # Ensure the layer has a forward method\n",
    "            try:\n",
    "                forward_code = inspect.getsource(layer.forward)\n",
    "                print(f\"\\n--- {name} ({layer.__class__.__name__}) ---\\n\")\n",
    "                print(forward_code)\n",
    "            except Exception as e:\n",
    "                print(f\"\\n--- {name} ({layer.__class__.__name__}) ---\\n\")\n",
    "                print(f\"Could not retrieve forward method: {e}\")\n",
    "\n",
    "# Call the function to print forward methods\n",
    "print_forward_methods(model)\n",
    "\n",
    "# 11. Define the evaluation function\n",
    "def evaluate_model(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for images, labels in tqdm(dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            activation_shapes.clear()  \n",
    "            activation_values.clear()  \n",
    "            \n",
    "            print(f\"\\nRaw Image Tensor Shape (Before Normalization): {images.shape}\")  \n",
    "            \n",
    "            outputs = model(images)  \n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)  \n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Print activation shapes\n",
    "            print(\"\\n--- Layer Activation Shapes ---\")\n",
    "            for layer, shape in activation_shapes.items():\n",
    "                print(f\"{layer}: {shape}\")\n",
    "\n",
    "            break  \n",
    "\n",
    "    accuracy = (correct / total) * 100\n",
    "    return accuracy\n",
    "\n",
    "# 12. Evaluate the model on ImageNet validation set\n",
    "accuracy = evaluate_model(model, imagenet_val_loader)\n",
    "print(f\"EfficientNet-B0 Top-1 Accuracy on ImageNet: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('features', Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): SiLU(inplace=True)\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "    )\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (0): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
      "    )\n",
      "    (1): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
      "    )\n",
      "  )\n",
      "  (3): Sequential(\n",
      "    (0): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
      "    )\n",
      "    (1): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
      "    )\n",
      "  )\n",
      "  (4): Sequential(\n",
      "    (0): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
      "    )\n",
      "    (1): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
      "    )\n",
      "    (2): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
      "    )\n",
      "  )\n",
      "  (5): Sequential(\n",
      "    (0): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
      "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
      "    )\n",
      "    (1): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
      "    )\n",
      "    (2): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
      "    )\n",
      "  )\n",
      "  (6): Sequential(\n",
      "    (0): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
      "    )\n",
      "    (1): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
      "    )\n",
      "    (2): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
      "    )\n",
      "    (3): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
      "    )\n",
      "  )\n",
      "  (7): Sequential(\n",
      "    (0): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
      "    )\n",
      "  )\n",
      "  (8): Conv2dNormActivation(\n",
      "    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): SiLU(inplace=True)\n",
      "  )\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "for child in model.named_children():\n",
    "    print(child)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import xml.etree.ElementTree as ET\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Define paths for ImageNet validation images and annotations\n",
    "IMAGE_DIR = \"/home/kajm20/mnist/ILSVRC/Data/CLS-LOC/val\"  \n",
    "ANNOTATION_DIR = \"/home/kajm20/mnist/ILSVRC/Annotations/CLS-LOC/val\"  \n",
    "\n",
    "imagenet_transform = transforms.Compose([\n",
    "    transforms.Resize(256),  \n",
    "    transforms.CenterCrop(224),  \n",
    "    transforms.ToTensor()  # Only convert to tensor (values in range [0,1])\n",
    "])\n",
    "\n",
    "# 3. Load the synset mapping\n",
    "synset_mapping_path = \"/home/kajm20/mnist/ILSVRC/LOC_synset_mapping.txt\"\n",
    "wordnet_to_imagenet = {}\n",
    "\n",
    "with open(synset_mapping_path) as f:\n",
    "    for idx, line in enumerate(f.readlines()):\n",
    "        wordnet_id, _ = line.split(' ', 1)\n",
    "        wordnet_to_imagenet[wordnet_id] = idx  \n",
    "\n",
    "# 4. Define the custom dataset class\n",
    "class ImageNetValDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.transform = transform\n",
    "        self.annotation_files = sorted(os.listdir(annotation_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotation_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation_path = os.path.join(self.annotation_dir, self.annotation_files[idx])\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "        wordnet_id = root.find(\"object\").find(\"name\").text  \n",
    "\n",
    "        class_idx = wordnet_to_imagenet.get(wordnet_id, -1)  \n",
    "        image_filename = root.find(\"filename\").text + \".JPEG\"\n",
    "        image_path = os.path.join(self.image_dir, image_filename)\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, class_idx\n",
    "\n",
    "# 5. Initialize the dataset and dataloader\n",
    "imagenet_val_dataset = ImageNetValDataset(IMAGE_DIR, ANNOTATION_DIR, transform=imagenet_transform)\n",
    "imagenet_val_loader = DataLoader(imagenet_val_dataset, batch_size=2, shuffle=False, num_workers=0)\n",
    "\n",
    "# 6. Define the model (EfficientNet-B0 with pre-trained weights)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.efficientnet_b0(weights='DEFAULT')  \n",
    "model.to(device)\n",
    "model.eval() \n",
    "\n",
    "# 8. Hook function to store activations for **ALL** layers\n",
    "def hook_fn(module, inputs, output):\n",
    "    layer_name = f\"{module.__class__.__name__} ({id(module)})\" \n",
    "    activation_values[layer_name] = activation_values.get(layer_name, 0) + output.detach().cpu()  # Store values (moved to CPU)\n",
    "    if layer_name == 'Conv2d (134778391833936)':\n",
    "        print('output: ', output.detach().cpu())\n",
    "        print('output shape: ', output.detach().cpu().shape)\n",
    "        print('act ', activation_values[layer_name])\n",
    "        print('act shape', activation_values[layer_name].shape)\n",
    "\n",
    "# 9. Recursively register hooks for **all** relevant layers, including `AdaptiveAvgPool2d`\n",
    "for name, layer in model.named_modules():\n",
    "    layer.register_forward_hook(hook_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sequential (140366547643184)': tensor([[[[-3.6564e-01, -3.4152e-01, -4.1903e-01,  ..., -5.1162e-01,\n",
       "            -5.4380e-01, -5.0046e-01],\n",
       "           [-3.3070e-01, -2.6602e-01, -1.8996e-01,  ...,  1.5716e+00,\n",
       "            -5.5197e-01, -4.7699e-01],\n",
       "           [-3.7843e-01, -4.6823e-01,  3.5615e+00,  ...,  8.3725e+00,\n",
       "             1.8761e+00, -3.8957e-01],\n",
       "           ...,\n",
       "           [-4.0972e-01, -5.5683e-01, -5.1612e-01,  ..., -4.7296e-01,\n",
       "            -3.5173e-01, -4.0662e-01],\n",
       "           [-4.8791e-01, -3.0020e-01, -1.8837e-01,  ..., -4.1283e-01,\n",
       "            -4.1146e-01, -3.9652e-01],\n",
       "           [-4.7560e-01, -4.7161e-01, -3.9992e-01,  ..., -5.1697e-01,\n",
       "            -4.9693e-01, -4.2005e-01]],\n",
       " \n",
       "          [[ 1.2508e+00,  1.3232e+00,  1.0446e+00,  ...,  5.5654e-01,\n",
       "            -3.6331e-01, -4.2324e-01],\n",
       "           [ 7.1243e-01,  6.5421e-01,  1.2681e+00,  ..., -1.0845e-01,\n",
       "            -2.0978e-01, -4.4098e-01],\n",
       "           [ 3.5119e-01,  1.3660e+00,  4.8305e-01,  ..., -4.1717e-01,\n",
       "             1.6030e+00, -2.8895e-01],\n",
       "           ...,\n",
       "           [-4.7070e-01,  3.7420e-01,  9.9089e-02,  ..., -4.5920e-01,\n",
       "             2.5242e+00, -3.1742e-01],\n",
       "           [ 1.5606e+00,  7.4819e-01,  4.2119e-01,  ..., -4.4621e-01,\n",
       "            -1.1953e-01, -4.1333e-01],\n",
       "           [ 1.0254e+00,  1.0057e+00,  1.1976e-01,  ..., -4.4636e-01,\n",
       "            -4.3275e-01, -3.6335e-01]],\n",
       " \n",
       "          [[-4.2912e-01, -4.3110e-01, -3.8388e-01,  ..., -5.1311e-01,\n",
       "            -4.0607e-01, -2.5078e-01],\n",
       "           [-3.9447e-01, -4.5262e-01, -2.2655e-01,  ..., -3.4398e-01,\n",
       "             1.8588e-01,  3.3610e-01],\n",
       "           [-4.1720e-01,  8.5645e-01,  1.1064e+00,  ..., -2.7727e-01,\n",
       "             6.1271e-01,  3.4438e-01],\n",
       "           ...,\n",
       "           [-3.5700e-01, -1.8619e-01, -1.5609e-01,  ..., -2.1375e-01,\n",
       "            -4.2175e-01, -4.4737e-01],\n",
       "           [-2.9725e-01, -2.7039e-01, -2.5752e-01,  ..., -3.2082e-01,\n",
       "            -4.0315e-01, -4.3519e-01],\n",
       "           [-2.9759e-01, -2.8927e-01, -2.9439e-01,  ..., -3.4034e-01,\n",
       "            -3.9443e-01, -3.6001e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.6490e-01, -1.9994e-01, -3.3228e-01,  ..., -3.8665e-01,\n",
       "            -3.1272e-01, -1.5261e-01],\n",
       "           [-1.4941e-01, -4.3446e-01, -4.6647e-01,  ..., -4.3030e-01,\n",
       "            -4.5326e-01, -2.6303e-01],\n",
       "           [-3.2980e-01,  1.3022e+00,  2.5874e-01,  ..., -2.6758e-01,\n",
       "            -5.0614e-01, -4.4879e-01],\n",
       "           ...,\n",
       "           [-2.6883e-01, -1.6741e-01, -2.1370e-01,  ..., -1.1192e-01,\n",
       "            -3.1891e-01, -3.5462e-01],\n",
       "           [-3.9160e-01, -1.5919e-01, -1.8606e-01,  ..., -2.1256e-01,\n",
       "            -2.1854e-01, -2.5797e-01],\n",
       "           [-4.5765e-01, -3.7226e-01, -2.8638e-01,  ..., -3.3338e-01,\n",
       "            -3.8276e-01, -3.5990e-01]],\n",
       " \n",
       "          [[ 1.0665e+00,  8.2902e-01,  6.6857e-01,  ...,  4.2448e-01,\n",
       "             2.8433e-01,  4.2898e-02],\n",
       "           [ 1.1784e+00,  1.2404e+00,  3.3804e-01,  ..., -4.6320e-01,\n",
       "            -4.1027e-01,  6.2933e-01],\n",
       "           [ 5.2815e-01, -1.5267e-01, -4.3137e-01,  ..., -3.0967e-01,\n",
       "            -3.0579e-01,  5.2942e-01],\n",
       "           ...,\n",
       "           [-4.4468e-03, -1.8553e-01, -9.2759e-02,  ..., -1.2944e-01,\n",
       "            -3.1684e-01,  6.7628e-01],\n",
       "           [-2.5123e-01, -1.6272e-01, -1.6993e-01,  ..., -1.5913e-01,\n",
       "            -1.4645e-01, -9.0349e-02],\n",
       "           [-1.9154e-01, -2.3287e-01, -3.1194e-01,  ..., -2.3069e-01,\n",
       "            -2.5131e-01, -1.9070e-01]],\n",
       " \n",
       "          [[-1.9475e-01, -2.0008e-01, -2.6879e-01,  ..., -3.8415e-01,\n",
       "            -3.5305e-01, -2.3406e-01],\n",
       "           [-2.1687e-01, -2.5068e-01, -3.7703e-01,  ..., -3.1933e-01,\n",
       "            -2.5888e-01, -2.5778e-01],\n",
       "           [-2.8237e-01, -4.0711e-01, -2.8267e-01,  ..., -6.3878e-02,\n",
       "            -1.7230e-01, -2.9257e-01],\n",
       "           ...,\n",
       "           [-2.4921e-01, -3.1238e-01, -2.6519e-01,  ..., -2.0156e-01,\n",
       "            -5.0093e-02, -2.0322e-02],\n",
       "           [-2.8846e-01, -2.8685e-01, -3.0963e-01,  ..., -1.9263e-01,\n",
       "            -1.6581e-01, -1.3017e-01],\n",
       "           [-2.5535e-01, -2.0749e-01, -2.2916e-01,  ..., -2.3037e-01,\n",
       "            -1.7428e-01, -1.3674e-01]]],\n",
       " \n",
       " \n",
       "         [[[-2.8359e-01, -3.0090e-01, -2.8778e-01,  ..., -2.9489e-01,\n",
       "            -3.9782e-01, -4.2869e-01],\n",
       "           [-2.1787e-01, -2.3288e-01, -1.2936e-01,  ..., -2.2093e-01,\n",
       "            -3.5641e-01, -3.3707e-01],\n",
       "           [-3.5024e-01, -2.4491e-01, -5.3019e-02,  ..., -5.9421e-02,\n",
       "            -8.7441e-02, -2.0895e-01],\n",
       "           ...,\n",
       "           [-4.7178e-01,  3.2385e-01, -2.5225e-01,  ...,  1.2402e+00,\n",
       "             1.5999e-02, -2.1782e-01],\n",
       "           [-2.5618e-01, -3.3672e-01, -2.3082e-01,  ..., -3.2560e-01,\n",
       "            -2.9172e-01, -2.1554e-01],\n",
       "           [-4.2588e-01, -4.0991e-01, -4.4329e-01,  ..., -4.5484e-01,\n",
       "            -4.2738e-01, -4.3387e-01]],\n",
       " \n",
       "          [[-2.3653e-01, -1.4925e-01, -1.2587e-01,  ..., -1.0257e-01,\n",
       "            -1.4093e-01, -2.0672e-01],\n",
       "           [-2.1623e-01, -6.6235e-02, -3.5994e-02,  ..., -1.1230e-02,\n",
       "            -1.5827e-02, -3.3590e-02],\n",
       "           [-3.8236e-01, -3.0233e-01, -1.5819e-01,  ..., -1.1657e-01,\n",
       "            -4.5970e-02, -7.7248e-02],\n",
       "           ...,\n",
       "           [-2.6669e-01, -1.8517e-01, -2.7452e-01,  ...,  8.5727e-01,\n",
       "             1.2969e-01, -2.7807e-01],\n",
       "           [-2.1638e-01, -2.0203e-01, -2.7287e-01,  ..., -2.9245e-01,\n",
       "            -1.8330e-01, -1.7635e-01],\n",
       "           [-2.2001e-01, -1.9873e-01, -2.4784e-01,  ..., -2.6405e-01,\n",
       "            -2.1759e-01, -2.9771e-01]],\n",
       " \n",
       "          [[-3.8456e-01, -4.1554e-01, -2.9305e-01,  ..., -3.0980e-01,\n",
       "             2.6150e+00,  2.4836e+00],\n",
       "           [-4.3581e-01, -4.3177e-01, -1.8597e-01,  ..., -4.5891e-01,\n",
       "            -2.9518e-01,  5.7791e-01],\n",
       "           [-4.3021e-01, -3.9398e-01, -3.1439e-01,  ..., -2.2501e-01,\n",
       "            -5.2344e-01, -4.0110e-01],\n",
       "           ...,\n",
       "           [-4.5734e-01, -1.0559e-01,  6.3243e-02,  ..., -5.3933e-01,\n",
       "            -3.0164e-01, -3.1963e-01],\n",
       "           [-4.8592e-01, -4.4920e-01, -4.3275e-01,  ..., -3.7775e-01,\n",
       "            -3.3321e-01, -3.9987e-01],\n",
       "           [-4.6677e-01, -5.0195e-01, -5.0736e-01,  ..., -5.5633e-01,\n",
       "            -5.1366e-01, -4.3687e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-4.2436e-01, -3.7566e-01, -3.4832e-01,  ..., -3.8647e-01,\n",
       "            -3.3300e-01, -3.4547e-01],\n",
       "           [-1.9365e-01, -9.2051e-02, -3.4893e-01,  ..., -3.3974e-01,\n",
       "            -1.8344e-01, -4.8992e-01],\n",
       "           [ 6.1586e-01,  4.5388e-01, -5.2004e-01,  ..., -4.4386e-01,\n",
       "             9.4037e-01,  8.5067e-01],\n",
       "           ...,\n",
       "           [-3.4409e-01, -1.8854e-01, -7.9769e-02,  ..., -3.9159e-01,\n",
       "            -4.2300e-01, -3.2277e-01],\n",
       "           [-3.8176e-01, -2.2978e-01, -1.8682e-01,  ..., -3.8978e-01,\n",
       "            -4.7605e-01, -3.5927e-01],\n",
       "           [-3.5144e-01, -3.3674e-01, -3.1363e-01,  ..., -4.6379e-01,\n",
       "            -5.2135e-01, -4.7601e-01]],\n",
       " \n",
       "          [[-1.6812e-02,  1.5002e+00,  9.5461e-01,  ..., -4.8602e-01,\n",
       "            -2.1510e-01,  4.1699e-02],\n",
       "           [-2.8428e-02,  5.3028e-01, -5.3739e-01,  ..., -1.5073e-01,\n",
       "            -3.1685e-01, -6.6836e-03],\n",
       "           [-3.6044e-01,  2.5141e-01, -8.6583e-02,  ..., -2.8971e-01,\n",
       "            -3.2104e-01, -3.4483e-01],\n",
       "           ...,\n",
       "           [-2.7539e-01, -2.8543e-01, -2.7984e-01,  ...,  8.0872e-02,\n",
       "            -2.7748e-01, -2.2704e-01],\n",
       "           [-1.9655e-01, -1.6198e-01, -1.2331e-01,  ..., -1.3154e-01,\n",
       "            -1.2111e-01, -2.0129e-01],\n",
       "           [-2.1105e-01, -2.3308e-01, -2.1485e-01,  ..., -2.0756e-01,\n",
       "            -2.3629e-01, -2.1044e-01]],\n",
       " \n",
       "          [[-2.8433e-01, -2.9768e-01, -2.0079e-01,  ..., -3.4727e-01,\n",
       "            -3.8769e-01, -3.3884e-01],\n",
       "           [-3.2983e-01, -3.2338e-01, -1.5065e-01,  ..., -2.9389e-01,\n",
       "            -2.6010e-01, -2.5923e-01],\n",
       "           [-2.0836e-01, -2.2030e-01, -3.2217e-01,  ..., -2.8774e-01,\n",
       "            -2.3617e-01, -1.5258e-01],\n",
       "           ...,\n",
       "           [-1.5428e-01, -2.9149e-02, -4.3839e-03,  ..., -9.5011e-03,\n",
       "            -3.3711e-02, -4.8710e-02],\n",
       "           [-2.2734e-01, -2.2157e-01, -1.1041e-01,  ..., -1.2883e-01,\n",
       "            -1.5480e-01, -2.0652e-01],\n",
       "           [-2.6385e-01, -3.0520e-01, -3.2338e-01,  ..., -3.1956e-01,\n",
       "            -3.6230e-01, -3.5750e-01]]]]),\n",
       " 'AdaptiveAvgPool2d (140366547928800)': tensor([[[[ 0.6004]],\n",
       " \n",
       "          [[ 0.3555]],\n",
       " \n",
       "          [[-0.2113]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.2559]],\n",
       " \n",
       "          [[ 0.0505]],\n",
       " \n",
       "          [[-0.1978]]],\n",
       " \n",
       " \n",
       "         [[[-0.1807]],\n",
       " \n",
       "          [[-0.1208]],\n",
       " \n",
       "          [[-0.1203]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.2278]],\n",
       " \n",
       "          [[-0.1196]],\n",
       " \n",
       "          [[-0.2041]]]]),\n",
       " 'Sequential (140366547644304)': tensor([[ 0.0593,  0.5122,  0.6411,  ..., -1.2749, -2.1134,  1.1105],\n",
       "         [-0.6226,  2.1095, -1.2012,  ..., -0.0357, -0.1671,  0.2872]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/25000 [00:00<55:34,  7.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Layer Activation Shapes ---\n",
      "Conv2d (140366556556944): torch.Size([2, 32, 112, 112])\n",
      "BatchNorm2d (140366551526368): torch.Size([2, 32, 112, 112])\n",
      "SiLU (140366556117136): torch.Size([2, 32, 112, 112])\n",
      "Conv2dNormActivation (140366556116896): torch.Size([2, 32, 112, 112])\n",
      "Conv2d (140366556556624): torch.Size([2, 32, 112, 112])\n",
      "BatchNorm2d (140366551526096): torch.Size([2, 32, 112, 112])\n",
      "SiLU (140366556116416): torch.Size([2, 32, 112, 112])\n",
      "Conv2dNormActivation (140366556117376): torch.Size([2, 32, 112, 112])\n",
      "AdaptiveAvgPool2d (140366556115936): torch.Size([2, 32, 1, 1])\n",
      "Conv2d (140366556556304): torch.Size([2, 8, 1, 1])\n",
      "SiLU (140366556116176): torch.Size([2, 8, 1, 1])\n",
      "Conv2d (140366556555984): torch.Size([2, 32, 1, 1])\n",
      "Sigmoid (140366556563504): torch.Size([2, 32, 1, 1])\n",
      "SqueezeExcitation (140366556571344): torch.Size([2, 32, 112, 112])\n",
      "Conv2d (140366556555664): torch.Size([2, 16, 112, 112])\n",
      "BatchNorm2d (140366551525280): torch.Size([2, 16, 112, 112])\n",
      "Conv2dNormActivation (140366556115456): torch.Size([2, 16, 112, 112])\n",
      "Sequential (140366556563952): torch.Size([2, 16, 112, 112])\n",
      "MBConv (140366556117856): torch.Size([2, 16, 112, 112])\n",
      "Sequential (140366556564624): torch.Size([2, 16, 112, 112])\n",
      "Conv2d (140366556555344): torch.Size([2, 96, 112, 112])\n",
      "BatchNorm2d (140366551525008): torch.Size([2, 96, 112, 112])\n",
      "SiLU (140366556114736): torch.Size([2, 96, 112, 112])\n",
      "Conv2dNormActivation (140366556114976): torch.Size([2, 96, 112, 112])\n",
      "Conv2d (140366556555024): torch.Size([2, 96, 56, 56])\n",
      "BatchNorm2d (140366551524736): torch.Size([2, 96, 56, 56])\n",
      "SiLU (140366556114256): torch.Size([2, 96, 56, 56])\n",
      "Conv2dNormActivation (140366556114496): torch.Size([2, 96, 56, 56])\n",
      "AdaptiveAvgPool2d (140366556113776): torch.Size([2, 96, 1, 1])\n",
      "Conv2d (140366556554704): torch.Size([2, 4, 1, 1])\n",
      "SiLU (140366556114016): torch.Size([2, 4, 1, 1])\n",
      "Conv2d (140366556554384): torch.Size([2, 96, 1, 1])\n",
      "Sigmoid (140366547646320): torch.Size([2, 96, 1, 1])\n",
      "SqueezeExcitation (140366547645424): torch.Size([2, 96, 56, 56])\n",
      "Conv2d (140366556554064): torch.Size([2, 24, 56, 56])\n",
      "BatchNorm2d (140366551524464): torch.Size([2, 24, 56, 56])\n",
      "Conv2dNormActivation (140366556113296): torch.Size([2, 24, 56, 56])\n",
      "Sequential (140366547647888): torch.Size([2, 24, 56, 56])\n",
      "MBConv (140366556113056): torch.Size([2, 24, 56, 56])\n",
      "Conv2d (140366556553744): torch.Size([2, 144, 56, 56])\n",
      "BatchNorm2d (140366551524192): torch.Size([2, 144, 56, 56])\n",
      "SiLU (140366556112576): torch.Size([2, 144, 56, 56])\n",
      "Conv2dNormActivation (140366556112816): torch.Size([2, 144, 56, 56])\n",
      "Conv2d (140366556553424): torch.Size([2, 144, 56, 56])\n",
      "BatchNorm2d (140366551523920): torch.Size([2, 144, 56, 56])\n",
      "SiLU (140366556112096): torch.Size([2, 144, 56, 56])\n",
      "Conv2dNormActivation (140366556112336): torch.Size([2, 144, 56, 56])\n",
      "AdaptiveAvgPool2d (140366556111616): torch.Size([2, 144, 1, 1])\n",
      "Conv2d (140366556553104): torch.Size([2, 6, 1, 1])\n",
      "SiLU (140366556111856): torch.Size([2, 6, 1, 1])\n",
      "Conv2d (140366556552784): torch.Size([2, 144, 1, 1])\n",
      "Sigmoid (140366552370320): torch.Size([2, 144, 1, 1])\n",
      "SqueezeExcitation (140366552369424): torch.Size([2, 144, 56, 56])\n",
      "Conv2d (140366556552464): torch.Size([2, 24, 56, 56])\n",
      "BatchNorm2d (140366551523648): torch.Size([2, 24, 56, 56])\n",
      "Conv2dNormActivation (140366556111136): torch.Size([2, 24, 56, 56])\n",
      "Sequential (140366552371888): torch.Size([2, 24, 56, 56])\n",
      "StochasticDepth (140366556111376): torch.Size([2, 24, 56, 56])\n",
      "MBConv (140366556110896): torch.Size([2, 24, 56, 56])\n",
      "Sequential (140366552372560): torch.Size([2, 24, 56, 56])\n",
      "Conv2d (140366556552144): torch.Size([2, 144, 56, 56])\n",
      "BatchNorm2d (140366551523376): torch.Size([2, 144, 56, 56])\n",
      "SiLU (140366556110416): torch.Size([2, 144, 56, 56])\n",
      "Conv2dNormActivation (140366556110656): torch.Size([2, 144, 56, 56])\n",
      "Conv2d (140366556551824): torch.Size([2, 144, 28, 28])\n",
      "BatchNorm2d (140366551523104): torch.Size([2, 144, 28, 28])\n",
      "SiLU (140366556109936): torch.Size([2, 144, 28, 28])\n",
      "Conv2dNormActivation (140366556110176): torch.Size([2, 144, 28, 28])\n",
      "AdaptiveAvgPool2d (140366556109456): torch.Size([2, 144, 1, 1])\n",
      "Conv2d (140366556551504): torch.Size([2, 6, 1, 1])\n",
      "SiLU (140366556109696): torch.Size([2, 6, 1, 1])\n",
      "Conv2d (140366556551184): torch.Size([2, 144, 1, 1])\n",
      "Sigmoid (140366552375920): torch.Size([2, 144, 1, 1])\n",
      "SqueezeExcitation (140366552375024): torch.Size([2, 144, 28, 28])\n",
      "Conv2d (140366556550864): torch.Size([2, 40, 28, 28])\n",
      "BatchNorm2d (140366551522832): torch.Size([2, 40, 28, 28])\n",
      "Conv2dNormActivation (140366556108976): torch.Size([2, 40, 28, 28])\n",
      "Sequential (140366552377488): torch.Size([2, 40, 28, 28])\n",
      "MBConv (140366556108736): torch.Size([2, 40, 28, 28])\n",
      "Conv2d (140366556550544): torch.Size([2, 240, 28, 28])\n",
      "BatchNorm2d (140366551522560): torch.Size([2, 240, 28, 28])\n",
      "SiLU (140366556108256): torch.Size([2, 240, 28, 28])\n",
      "Conv2dNormActivation (140366556108496): torch.Size([2, 240, 28, 28])\n",
      "Conv2d (140366556550224): torch.Size([2, 240, 28, 28])\n",
      "BatchNorm2d (140366551522288): torch.Size([2, 240, 28, 28])\n",
      "SiLU (140366556107776): torch.Size([2, 240, 28, 28])\n",
      "Conv2dNormActivation (140366556108016): torch.Size([2, 240, 28, 28])\n",
      "AdaptiveAvgPool2d (140366556107296): torch.Size([2, 240, 1, 1])\n",
      "Conv2d (140366556549904): torch.Size([2, 10, 1, 1])\n",
      "SiLU (140366556107536): torch.Size([2, 10, 1, 1])\n",
      "Conv2d (140366556549584): torch.Size([2, 240, 1, 1])\n",
      "Sigmoid (140366552381072): torch.Size([2, 240, 1, 1])\n",
      "SqueezeExcitation (140366552380176): torch.Size([2, 240, 28, 28])\n",
      "Conv2d (140366556549264): torch.Size([2, 40, 28, 28])\n",
      "BatchNorm2d (140366551522016): torch.Size([2, 40, 28, 28])\n",
      "Conv2dNormActivation (140366556106816): torch.Size([2, 40, 28, 28])\n",
      "Sequential (140366552382640): torch.Size([2, 40, 28, 28])\n",
      "StochasticDepth (140366556107056): torch.Size([2, 40, 28, 28])\n",
      "MBConv (140366556106576): torch.Size([2, 40, 28, 28])\n",
      "Sequential (140366556299344): torch.Size([2, 40, 28, 28])\n",
      "Conv2d (140366556548944): torch.Size([2, 240, 28, 28])\n",
      "BatchNorm2d (140366551521744): torch.Size([2, 240, 28, 28])\n",
      "SiLU (140366556106096): torch.Size([2, 240, 28, 28])\n",
      "Conv2dNormActivation (140366556106336): torch.Size([2, 240, 28, 28])\n",
      "Conv2d (140366556548624): torch.Size([2, 240, 14, 14])\n",
      "BatchNorm2d (140366551521472): torch.Size([2, 240, 14, 14])\n",
      "SiLU (140366556105616): torch.Size([2, 240, 14, 14])\n",
      "Conv2dNormActivation (140366556105856): torch.Size([2, 240, 14, 14])\n",
      "AdaptiveAvgPool2d (140366556105136): torch.Size([2, 240, 1, 1])\n",
      "Conv2d (140366556548304): torch.Size([2, 10, 1, 1])\n",
      "SiLU (140366556105376): torch.Size([2, 10, 1, 1])\n",
      "Conv2d (140366556547984): torch.Size([2, 240, 1, 1])\n",
      "Sigmoid (140366556302704): torch.Size([2, 240, 1, 1])\n",
      "SqueezeExcitation (140366556301808): torch.Size([2, 240, 14, 14])\n",
      "Conv2d (140366556547664): torch.Size([2, 80, 14, 14])\n",
      "BatchNorm2d (140366551521200): torch.Size([2, 80, 14, 14])\n",
      "Conv2dNormActivation (140366556104656): torch.Size([2, 80, 14, 14])\n",
      "Sequential (140366556304272): torch.Size([2, 80, 14, 14])\n",
      "MBConv (140366556104416): torch.Size([2, 80, 14, 14])\n",
      "Conv2d (140366556547344): torch.Size([2, 480, 14, 14])\n",
      "BatchNorm2d (140366551520928): torch.Size([2, 480, 14, 14])\n",
      "SiLU (140366556103696): torch.Size([2, 480, 14, 14])\n",
      "Conv2dNormActivation (140366556103936): torch.Size([2, 480, 14, 14])\n",
      "Conv2d (140366556547024): torch.Size([2, 480, 14, 14])\n",
      "BatchNorm2d (140366551520656): torch.Size([2, 480, 14, 14])\n",
      "SiLU (140366556102976): torch.Size([2, 480, 14, 14])\n",
      "Conv2dNormActivation (140366556103456): torch.Size([2, 480, 14, 14])\n",
      "AdaptiveAvgPool2d (140366556103216): torch.Size([2, 480, 1, 1])\n",
      "Conv2d (140366556546704): torch.Size([2, 20, 1, 1])\n",
      "SiLU (140366556102736): torch.Size([2, 20, 1, 1])\n",
      "Conv2d (140366556546384): torch.Size([2, 480, 1, 1])\n",
      "Sigmoid (140366556307856): torch.Size([2, 480, 1, 1])\n",
      "SqueezeExcitation (140366556306960): torch.Size([2, 480, 14, 14])\n",
      "Conv2d (140366556546064): torch.Size([2, 80, 14, 14])\n",
      "BatchNorm2d (140366551520384): torch.Size([2, 80, 14, 14])\n",
      "Conv2dNormActivation (140366552101376): torch.Size([2, 80, 14, 14])\n",
      "Sequential (140366556309424): torch.Size([2, 80, 14, 14])\n",
      "StochasticDepth (140366552103536): torch.Size([2, 80, 14, 14])\n",
      "MBConv (140366556104176): torch.Size([2, 80, 14, 14])\n",
      "Conv2d (140366556545744): torch.Size([2, 480, 14, 14])\n",
      "BatchNorm2d (140366551520112): torch.Size([2, 480, 14, 14])\n",
      "SiLU (140366552104496): torch.Size([2, 480, 14, 14])\n",
      "Conv2dNormActivation (140366552104256): torch.Size([2, 480, 14, 14])\n",
      "Conv2d (140366556545424): torch.Size([2, 480, 14, 14])\n",
      "BatchNorm2d (140366551519840): torch.Size([2, 480, 14, 14])\n",
      "SiLU (140366552104016): torch.Size([2, 480, 14, 14])\n",
      "Conv2dNormActivation (140366552103776): torch.Size([2, 480, 14, 14])\n",
      "AdaptiveAvgPool2d (140366552101616): torch.Size([2, 480, 1, 1])\n",
      "Conv2d (140366556545104): torch.Size([2, 20, 1, 1])\n",
      "SiLU (140366552103296): torch.Size([2, 20, 1, 1])\n",
      "Conv2d (140366552334032): torch.Size([2, 480, 1, 1])\n",
      "Sigmoid (140366556313008): torch.Size([2, 480, 1, 1])\n",
      "SqueezeExcitation (140366556312112): torch.Size([2, 480, 14, 14])\n",
      "Conv2d (140366552333712): torch.Size([2, 80, 14, 14])\n",
      "BatchNorm2d (140366551519568): torch.Size([2, 80, 14, 14])\n",
      "Conv2dNormActivation (140366552103056): torch.Size([2, 80, 14, 14])\n",
      "Sequential (140366556314576): torch.Size([2, 80, 14, 14])\n",
      "StochasticDepth (140366552102816): torch.Size([2, 80, 14, 14])\n",
      "MBConv (140366552104736): torch.Size([2, 80, 14, 14])\n",
      "Sequential (140366556315248): torch.Size([2, 80, 14, 14])\n",
      "Conv2d (140366552333392): torch.Size([2, 480, 14, 14])\n",
      "BatchNorm2d (140366551519296): torch.Size([2, 480, 14, 14])\n",
      "SiLU (140366552102336): torch.Size([2, 480, 14, 14])\n",
      "Conv2dNormActivation (140366552101136): torch.Size([2, 480, 14, 14])\n",
      "Conv2d (140366552333072): torch.Size([2, 480, 14, 14])\n",
      "BatchNorm2d (140366551519024): torch.Size([2, 480, 14, 14])\n",
      "SiLU (140366552102096): torch.Size([2, 480, 14, 14])\n",
      "Conv2dNormActivation (140366552101856): torch.Size([2, 480, 14, 14])\n",
      "AdaptiveAvgPool2d (140366547929040): torch.Size([2, 480, 1, 1])\n",
      "Conv2d (140366552332752): torch.Size([2, 20, 1, 1])\n",
      "SiLU (140366547929280): torch.Size([2, 20, 1, 1])\n",
      "Conv2d (140366552332432): torch.Size([2, 480, 1, 1])\n",
      "Sigmoid (140366537067664): torch.Size([2, 480, 1, 1])\n",
      "SqueezeExcitation (140366537066768): torch.Size([2, 480, 14, 14])\n",
      "Conv2d (140366552332112): torch.Size([2, 112, 14, 14])\n",
      "BatchNorm2d (140366551518752): torch.Size([2, 112, 14, 14])\n",
      "Conv2dNormActivation (140366547929520): torch.Size([2, 112, 14, 14])\n",
      "Sequential (140366537069232): torch.Size([2, 112, 14, 14])\n",
      "MBConv (140366552102576): torch.Size([2, 112, 14, 14])\n",
      "Conv2d (140366552331792): torch.Size([2, 672, 14, 14])\n",
      "BatchNorm2d (140366551518480): torch.Size([2, 672, 14, 14])\n",
      "SiLU (140366547930480): torch.Size([2, 672, 14, 14])\n",
      "Conv2dNormActivation (140366547930240): torch.Size([2, 672, 14, 14])\n",
      "Conv2d (140366552331472): torch.Size([2, 672, 14, 14])\n",
      "BatchNorm2d (140366551518208): torch.Size([2, 672, 14, 14])\n",
      "SiLU (140366547930960): torch.Size([2, 672, 14, 14])\n",
      "Conv2dNormActivation (140366547930720): torch.Size([2, 672, 14, 14])\n",
      "AdaptiveAvgPool2d (140366547931200): torch.Size([2, 672, 1, 1])\n",
      "Conv2d (140366552331152): torch.Size([2, 28, 1, 1])\n",
      "SiLU (140366547931440): torch.Size([2, 28, 1, 1])\n",
      "Conv2d (140366552330832): torch.Size([2, 672, 1, 1])\n",
      "Sigmoid (140366537072816): torch.Size([2, 672, 1, 1])\n",
      "SqueezeExcitation (140366537071920): torch.Size([2, 672, 14, 14])\n",
      "Conv2d (140366552330512): torch.Size([2, 112, 14, 14])\n",
      "BatchNorm2d (140366551517936): torch.Size([2, 112, 14, 14])\n",
      "Conv2dNormActivation (140366547931680): torch.Size([2, 112, 14, 14])\n",
      "Sequential (140366537074384): torch.Size([2, 112, 14, 14])\n",
      "StochasticDepth (140366547931920): torch.Size([2, 112, 14, 14])\n",
      "MBConv (140366547930000): torch.Size([2, 112, 14, 14])\n",
      "Conv2d (140366552330192): torch.Size([2, 672, 14, 14])\n",
      "BatchNorm2d (140366551517664): torch.Size([2, 672, 14, 14])\n",
      "SiLU (140366547932640): torch.Size([2, 672, 14, 14])\n",
      "Conv2dNormActivation (140366547932400): torch.Size([2, 672, 14, 14])\n",
      "Conv2d (140366552329872): torch.Size([2, 672, 14, 14])\n",
      "BatchNorm2d (140366551517392): torch.Size([2, 672, 14, 14])\n",
      "SiLU (140366547933120): torch.Size([2, 672, 14, 14])\n",
      "Conv2dNormActivation (140366547932880): torch.Size([2, 672, 14, 14])\n",
      "AdaptiveAvgPool2d (140366547933360): torch.Size([2, 672, 1, 1])\n",
      "Conv2d (140366552329552): torch.Size([2, 28, 1, 1])\n",
      "SiLU (140366547933600): torch.Size([2, 28, 1, 1])\n",
      "Conv2d (140366552329232): torch.Size([2, 672, 1, 1])\n",
      "Sigmoid (140366537077968): torch.Size([2, 672, 1, 1])\n",
      "SqueezeExcitation (140366537077072): torch.Size([2, 672, 14, 14])\n",
      "Conv2d (140366552328912): torch.Size([2, 112, 14, 14])\n",
      "BatchNorm2d (140366551517120): torch.Size([2, 112, 14, 14])\n",
      "Conv2dNormActivation (140366547933840): torch.Size([2, 112, 14, 14])\n",
      "Sequential (140366537079536): torch.Size([2, 112, 14, 14])\n",
      "StochasticDepth (140366547934080): torch.Size([2, 112, 14, 14])\n",
      "MBConv (140366547932160): torch.Size([2, 112, 14, 14])\n",
      "Sequential (140366537080208): torch.Size([2, 112, 14, 14])\n",
      "Conv2d (140366552328592): torch.Size([2, 672, 14, 14])\n",
      "BatchNorm2d (140366551516848): torch.Size([2, 672, 14, 14])\n",
      "SiLU (140366547934800): torch.Size([2, 672, 14, 14])\n",
      "Conv2dNormActivation (140366547934560): torch.Size([2, 672, 14, 14])\n",
      "Conv2d (140366552328272): torch.Size([2, 672, 7, 7])\n",
      "BatchNorm2d (140366551516576): torch.Size([2, 672, 7, 7])\n",
      "SiLU (140366547935280): torch.Size([2, 672, 7, 7])\n",
      "Conv2dNormActivation (140366547935040): torch.Size([2, 672, 7, 7])\n",
      "AdaptiveAvgPool2d (140366547935520): torch.Size([2, 672, 1, 1])\n",
      "Conv2d (140366552327952): torch.Size([2, 28, 1, 1])\n",
      "SiLU (140366547935760): torch.Size([2, 28, 1, 1])\n",
      "Conv2d (140366552327632): torch.Size([2, 672, 1, 1])\n",
      "Sigmoid (140366537214896): torch.Size([2, 672, 1, 1])\n",
      "SqueezeExcitation (140366537214000): torch.Size([2, 672, 7, 7])\n",
      "Conv2d (140366552327312): torch.Size([2, 192, 7, 7])\n",
      "BatchNorm2d (140366551516304): torch.Size([2, 192, 7, 7])\n",
      "Conv2dNormActivation (140366547936000): torch.Size([2, 192, 7, 7])\n",
      "Sequential (140366537216464): torch.Size([2, 192, 7, 7])\n",
      "MBConv (140366547934320): torch.Size([2, 192, 7, 7])\n",
      "Conv2d (140366552326992): torch.Size([2, 1152, 7, 7])\n",
      "BatchNorm2d (140366551516032): torch.Size([2, 1152, 7, 7])\n",
      "SiLU (140366547936960): torch.Size([2, 1152, 7, 7])\n",
      "Conv2dNormActivation (140366547936720): torch.Size([2, 1152, 7, 7])\n",
      "Conv2d (140366552326672): torch.Size([2, 1152, 7, 7])\n",
      "BatchNorm2d (140366551515760): torch.Size([2, 1152, 7, 7])\n",
      "SiLU (140366547937440): torch.Size([2, 1152, 7, 7])\n",
      "Conv2dNormActivation (140366547937200): torch.Size([2, 1152, 7, 7])\n",
      "AdaptiveAvgPool2d (140366547937680): torch.Size([2, 1152, 1, 1])\n",
      "Conv2d (140366552326352): torch.Size([2, 48, 1, 1])\n",
      "SiLU (140366547937920): torch.Size([2, 48, 1, 1])\n",
      "Conv2d (140366552326032): torch.Size([2, 1152, 1, 1])\n",
      "Sigmoid (140366537220048): torch.Size([2, 1152, 1, 1])\n",
      "SqueezeExcitation (140366537219152): torch.Size([2, 1152, 7, 7])\n",
      "Conv2d (140366552325712): torch.Size([2, 192, 7, 7])\n",
      "BatchNorm2d (140366551515488): torch.Size([2, 192, 7, 7])\n",
      "Conv2dNormActivation (140366547938160): torch.Size([2, 192, 7, 7])\n",
      "Sequential (140366537221616): torch.Size([2, 192, 7, 7])\n",
      "StochasticDepth (140366547938400): torch.Size([2, 192, 7, 7])\n",
      "MBConv (140366547936480): torch.Size([2, 192, 7, 7])\n",
      "Conv2d (140366552325392): torch.Size([2, 1152, 7, 7])\n",
      "BatchNorm2d (140366551515216): torch.Size([2, 1152, 7, 7])\n",
      "SiLU (140366547939120): torch.Size([2, 1152, 7, 7])\n",
      "Conv2dNormActivation (140366547938880): torch.Size([2, 1152, 7, 7])\n",
      "Conv2d (140366552325072): torch.Size([2, 1152, 7, 7])\n",
      "BatchNorm2d (140366557429504): torch.Size([2, 1152, 7, 7])\n",
      "SiLU (140366547939600): torch.Size([2, 1152, 7, 7])\n",
      "Conv2dNormActivation (140366547939360): torch.Size([2, 1152, 7, 7])\n",
      "AdaptiveAvgPool2d (140366547939840): torch.Size([2, 1152, 1, 1])\n",
      "Conv2d (140366552324752): torch.Size([2, 48, 1, 1])\n",
      "SiLU (140366547940080): torch.Size([2, 48, 1, 1])\n",
      "Conv2d (140366552324432): torch.Size([2, 1152, 1, 1])\n",
      "Sigmoid (140366537225200): torch.Size([2, 1152, 1, 1])\n",
      "SqueezeExcitation (140366537224304): torch.Size([2, 1152, 7, 7])\n",
      "Conv2d (140366552324112): torch.Size([2, 192, 7, 7])\n",
      "BatchNorm2d (140366557428960): torch.Size([2, 192, 7, 7])\n",
      "Conv2dNormActivation (140366547940320): torch.Size([2, 192, 7, 7])\n",
      "Sequential (140366537226768): torch.Size([2, 192, 7, 7])\n",
      "StochasticDepth (140366547940560): torch.Size([2, 192, 7, 7])\n",
      "MBConv (140366547938640): torch.Size([2, 192, 7, 7])\n",
      "Conv2d (140366552323792): torch.Size([2, 1152, 7, 7])\n",
      "BatchNorm2d (140366557429232): torch.Size([2, 1152, 7, 7])\n",
      "SiLU (140366547941280): torch.Size([2, 1152, 7, 7])\n",
      "Conv2dNormActivation (140366547941040): torch.Size([2, 1152, 7, 7])\n",
      "Conv2d (140366552323472): torch.Size([2, 1152, 7, 7])\n",
      "BatchNorm2d (140366869592224): torch.Size([2, 1152, 7, 7])\n",
      "SiLU (140366547941760): torch.Size([2, 1152, 7, 7])\n",
      "Conv2dNormActivation (140366547941520): torch.Size([2, 1152, 7, 7])\n",
      "AdaptiveAvgPool2d (140366547942000): torch.Size([2, 1152, 1, 1])\n",
      "Conv2d (140366552323152): torch.Size([2, 48, 1, 1])\n",
      "SiLU (140366547942240): torch.Size([2, 48, 1, 1])\n",
      "Conv2d (140366552322832): torch.Size([2, 1152, 1, 1])\n",
      "Sigmoid (140366536902928): torch.Size([2, 1152, 1, 1])\n",
      "SqueezeExcitation (140366536902032): torch.Size([2, 1152, 7, 7])\n",
      "Conv2d (140366552322512): torch.Size([2, 192, 7, 7])\n",
      "BatchNorm2d (140366869592496): torch.Size([2, 192, 7, 7])\n",
      "Conv2dNormActivation (140366547942480): torch.Size([2, 192, 7, 7])\n",
      "Sequential (140366536904496): torch.Size([2, 192, 7, 7])\n",
      "StochasticDepth (140366547942720): torch.Size([2, 192, 7, 7])\n",
      "MBConv (140366547940800): torch.Size([2, 192, 7, 7])\n",
      "Sequential (140366536905168): torch.Size([2, 192, 7, 7])\n",
      "Conv2d (140366552322192): torch.Size([2, 1152, 7, 7])\n",
      "BatchNorm2d (140366869585152): torch.Size([2, 1152, 7, 7])\n",
      "SiLU (140366537195600): torch.Size([2, 1152, 7, 7])\n",
      "Conv2dNormActivation (140366547943200): torch.Size([2, 1152, 7, 7])\n",
      "Conv2d (140366552321872): torch.Size([2, 1152, 7, 7])\n",
      "BatchNorm2d (140366869587600): torch.Size([2, 1152, 7, 7])\n",
      "SiLU (140366537196080): torch.Size([2, 1152, 7, 7])\n",
      "Conv2dNormActivation (140366537195840): torch.Size([2, 1152, 7, 7])\n",
      "AdaptiveAvgPool2d (140366537196320): torch.Size([2, 1152, 1, 1])\n",
      "Conv2d (140366558714832): torch.Size([2, 48, 1, 1])\n",
      "SiLU (140366537196560): torch.Size([2, 48, 1, 1])\n",
      "Conv2d (140366558711952): torch.Size([2, 1152, 1, 1])\n",
      "Sigmoid (140366536908528): torch.Size([2, 1152, 1, 1])\n",
      "SqueezeExcitation (140366536907632): torch.Size([2, 1152, 7, 7])\n",
      "Conv2d (140366558708752): torch.Size([2, 320, 7, 7])\n",
      "BatchNorm2d (140366537745520): torch.Size([2, 320, 7, 7])\n",
      "Conv2dNormActivation (140366537196800): torch.Size([2, 320, 7, 7])\n",
      "Sequential (140366536910096): torch.Size([2, 320, 7, 7])\n",
      "MBConv (140366547942960): torch.Size([2, 320, 7, 7])\n",
      "Sequential (140366536910768): torch.Size([2, 320, 7, 7])\n",
      "Conv2d (140366558714512): torch.Size([2, 1280, 7, 7])\n",
      "BatchNorm2d (140366537745792): torch.Size([2, 1280, 7, 7])\n",
      "SiLU (140366537197520): torch.Size([2, 1280, 7, 7])\n",
      "Conv2dNormActivation (140366537197280): torch.Size([2, 1280, 7, 7])\n",
      "Sequential (140366536912112): torch.Size([2, 1280, 7, 7])\n",
      "AdaptiveAvgPool2d (140366537197760): torch.Size([2, 1280, 1, 1])\n",
      "Dropout (140366558514832): torch.Size([2, 1280])\n",
      "Linear (140366558514528): torch.Size([2, 1000])\n",
      "Sequential (140366536913232): torch.Size([2, 1000])\n",
      "EfficientNet (140366558514224): torch.Size([2, 1000])\n",
      "\n",
      "--- Layer Activation Shapes ---\n",
      "Conv2d (140366556556944): torch.Size([2, 32, 112, 112])\n",
      "BatchNorm2d (140366551526368): torch.Size([2, 32, 112, 112])\n",
      "SiLU (140366556117136): torch.Size([2, 32, 112, 112])\n",
      "Conv2dNormActivation (140366556116896): torch.Size([2, 32, 112, 112])\n",
      "Conv2d (140366556556624): torch.Size([2, 32, 112, 112])\n",
      "BatchNorm2d (140366551526096): torch.Size([2, 32, 112, 112])\n",
      "SiLU (140366556116416): torch.Size([2, 32, 112, 112])\n",
      "Conv2dNormActivation (140366556117376): torch.Size([2, 32, 112, 112])\n",
      "AdaptiveAvgPool2d (140366556115936): torch.Size([2, 32, 1, 1])\n",
      "Conv2d (140366556556304): torch.Size([2, 8, 1, 1])\n",
      "SiLU (140366556116176): torch.Size([2, 8, 1, 1])\n",
      "Conv2d (140366556555984): torch.Size([2, 32, 1, 1])\n",
      "Sigmoid (140366556563504): torch.Size([2, 32, 1, 1])\n",
      "SqueezeExcitation (140366556571344): torch.Size([2, 32, 112, 112])\n",
      "Conv2d (140366556555664): torch.Size([2, 16, 112, 112])\n",
      "BatchNorm2d (140366551525280): torch.Size([2, 16, 112, 112])\n",
      "Conv2dNormActivation (140366556115456): torch.Size([2, 16, 112, 112])\n",
      "Sequential (140366556563952): torch.Size([2, 16, 112, 112])\n",
      "MBConv (140366556117856): torch.Size([2, 16, 112, 112])\n",
      "Sequential (140366556564624): torch.Size([2, 16, 112, 112])\n",
      "Conv2d (140366556555344): torch.Size([2, 96, 112, 112])\n",
      "BatchNorm2d (140366551525008): torch.Size([2, 96, 112, 112])\n",
      "SiLU (140366556114736): torch.Size([2, 96, 112, 112])\n",
      "Conv2dNormActivation (140366556114976): torch.Size([2, 96, 112, 112])\n",
      "Conv2d (140366556555024): torch.Size([2, 96, 56, 56])\n",
      "BatchNorm2d (140366551524736): torch.Size([2, 96, 56, 56])\n",
      "SiLU (140366556114256): torch.Size([2, 96, 56, 56])\n",
      "Conv2dNormActivation (140366556114496): torch.Size([2, 96, 56, 56])\n",
      "AdaptiveAvgPool2d (140366556113776): torch.Size([2, 96, 1, 1])\n",
      "Conv2d (140366556554704): torch.Size([2, 4, 1, 1])\n",
      "SiLU (140366556114016): torch.Size([2, 4, 1, 1])\n",
      "Conv2d (140366556554384): torch.Size([2, 96, 1, 1])\n",
      "Sigmoid (140366547646320): torch.Size([2, 96, 1, 1])\n",
      "SqueezeExcitation (140366547645424): torch.Size([2, 96, 56, 56])\n",
      "Conv2d (140366556554064): torch.Size([2, 24, 56, 56])\n",
      "BatchNorm2d (140366551524464): torch.Size([2, 24, 56, 56])\n",
      "Conv2dNormActivation (140366556113296): torch.Size([2, 24, 56, 56])\n",
      "Sequential (140366547647888): torch.Size([2, 24, 56, 56])\n",
      "MBConv (140366556113056): torch.Size([2, 24, 56, 56])\n",
      "Conv2d (140366556553744): torch.Size([2, 144, 56, 56])\n",
      "BatchNorm2d (140366551524192): torch.Size([2, 144, 56, 56])\n",
      "SiLU (140366556112576): torch.Size([2, 144, 56, 56])\n",
      "Conv2dNormActivation (140366556112816): torch.Size([2, 144, 56, 56])\n",
      "Conv2d (140366556553424): torch.Size([2, 144, 56, 56])\n",
      "BatchNorm2d (140366551523920): torch.Size([2, 144, 56, 56])\n",
      "SiLU (140366556112096): torch.Size([2, 144, 56, 56])\n",
      "Conv2dNormActivation (140366556112336): torch.Size([2, 144, 56, 56])\n",
      "AdaptiveAvgPool2d (140366556111616): torch.Size([2, 144, 1, 1])\n",
      "Conv2d (140366556553104): torch.Size([2, 6, 1, 1])\n",
      "SiLU (140366556111856): torch.Size([2, 6, 1, 1])\n",
      "Conv2d (140366556552784): torch.Size([2, 144, 1, 1])\n",
      "Sigmoid (140366552370320): torch.Size([2, 144, 1, 1])\n",
      "SqueezeExcitation (140366552369424): torch.Size([2, 144, 56, 56])\n",
      "Conv2d (140366556552464): torch.Size([2, 24, 56, 56])\n",
      "BatchNorm2d (140366551523648): torch.Size([2, 24, 56, 56])\n",
      "Conv2dNormActivation (140366556111136): torch.Size([2, 24, 56, 56])\n",
      "Sequential (140366552371888): torch.Size([2, 24, 56, 56])\n",
      "StochasticDepth (140366556111376): torch.Size([2, 24, 56, 56])\n",
      "MBConv (140366556110896): torch.Size([2, 24, 56, 56])\n",
      "Sequential (140366552372560): torch.Size([2, 24, 56, 56])\n",
      "Conv2d (140366556552144): torch.Size([2, 144, 56, 56])\n",
      "BatchNorm2d (140366551523376): torch.Size([2, 144, 56, 56])\n",
      "SiLU (140366556110416): torch.Size([2, 144, 56, 56])\n",
      "Conv2dNormActivation (140366556110656): torch.Size([2, 144, 56, 56])\n",
      "Conv2d (140366556551824): torch.Size([2, 144, 28, 28])\n",
      "BatchNorm2d (140366551523104): torch.Size([2, 144, 28, 28])\n",
      "SiLU (140366556109936): torch.Size([2, 144, 28, 28])\n",
      "Conv2dNormActivation (140366556110176): torch.Size([2, 144, 28, 28])\n",
      "AdaptiveAvgPool2d (140366556109456): torch.Size([2, 144, 1, 1])\n",
      "Conv2d (140366556551504): torch.Size([2, 6, 1, 1])\n",
      "SiLU (140366556109696): torch.Size([2, 6, 1, 1])\n",
      "Conv2d (140366556551184): torch.Size([2, 144, 1, 1])\n",
      "Sigmoid (140366552375920): torch.Size([2, 144, 1, 1])\n",
      "SqueezeExcitation (140366552375024): torch.Size([2, 144, 28, 28])\n",
      "Conv2d (140366556550864): torch.Size([2, 40, 28, 28])\n",
      "BatchNorm2d (140366551522832): torch.Size([2, 40, 28, 28])\n",
      "Conv2dNormActivation (140366556108976): torch.Size([2, 40, 28, 28])\n",
      "Sequential (140366552377488): torch.Size([2, 40, 28, 28])\n",
      "MBConv (140366556108736): torch.Size([2, 40, 28, 28])\n",
      "Conv2d (140366556550544): torch.Size([2, 240, 28, 28])\n",
      "BatchNorm2d (140366551522560): torch.Size([2, 240, 28, 28])\n",
      "SiLU (140366556108256): torch.Size([2, 240, 28, 28])\n",
      "Conv2dNormActivation (140366556108496): torch.Size([2, 240, 28, 28])\n",
      "Conv2d (140366556550224): torch.Size([2, 240, 28, 28])\n",
      "BatchNorm2d (140366551522288): torch.Size([2, 240, 28, 28])\n",
      "SiLU (140366556107776): torch.Size([2, 240, 28, 28])\n",
      "Conv2dNormActivation (140366556108016): torch.Size([2, 240, 28, 28])\n",
      "AdaptiveAvgPool2d (140366556107296): torch.Size([2, 240, 1, 1])\n",
      "Conv2d (140366556549904): torch.Size([2, 10, 1, 1])\n",
      "SiLU (140366556107536): torch.Size([2, 10, 1, 1])\n",
      "Conv2d (140366556549584): torch.Size([2, 240, 1, 1])\n",
      "Sigmoid (140366552381072): torch.Size([2, 240, 1, 1])\n",
      "SqueezeExcitation (140366552380176): torch.Size([2, 240, 28, 28])\n",
      "Conv2d (140366556549264): torch.Size([2, 40, 28, 28])\n",
      "BatchNorm2d (140366551522016): torch.Size([2, 40, 28, 28])\n",
      "Conv2dNormActivation (140366556106816): torch.Size([2, 40, 28, 28])\n",
      "Sequential (140366552382640): torch.Size([2, 40, 28, 28])\n",
      "StochasticDepth (140366556107056): torch.Size([2, 40, 28, 28])\n",
      "MBConv (140366556106576): torch.Size([2, 40, 28, 28])\n",
      "Sequential (140366556299344): torch.Size([2, 40, 28, 28])\n",
      "Conv2d (140366556548944): torch.Size([2, 240, 28, 28])\n",
      "BatchNorm2d (140366551521744): torch.Size([2, 240, 28, 28])\n",
      "SiLU (140366556106096): torch.Size([2, 240, 28, 28])\n",
      "Conv2dNormActivation (140366556106336): torch.Size([2, 240, 28, 28])\n",
      "Conv2d (140366556548624): torch.Size([2, 240, 14, 14])\n",
      "BatchNorm2d (140366551521472): torch.Size([2, 240, 14, 14])\n",
      "SiLU (140366556105616): torch.Size([2, 240, 14, 14])\n",
      "Conv2dNormActivation (140366556105856): torch.Size([2, 240, 14, 14])\n",
      "AdaptiveAvgPool2d (140366556105136): torch.Size([2, 240, 1, 1])\n",
      "Conv2d (140366556548304): torch.Size([2, 10, 1, 1])\n",
      "SiLU (140366556105376): torch.Size([2, 10, 1, 1])\n",
      "Conv2d (140366556547984): torch.Size([2, 240, 1, 1])\n",
      "Sigmoid (140366556302704): torch.Size([2, 240, 1, 1])\n",
      "SqueezeExcitation (140366556301808): torch.Size([2, 240, 14, 14])\n",
      "Conv2d (140366556547664): torch.Size([2, 80, 14, 14])\n",
      "BatchNorm2d (140366551521200): torch.Size([2, 80, 14, 14])\n",
      "Conv2dNormActivation (140366556104656): torch.Size([2, 80, 14, 14])\n",
      "Sequential (140366556304272): torch.Size([2, 80, 14, 14])\n",
      "MBConv (140366556104416): torch.Size([2, 80, 14, 14])\n",
      "Conv2d (140366556547344): torch.Size([2, 480, 14, 14])\n",
      "BatchNorm2d (140366551520928): torch.Size([2, 480, 14, 14])\n",
      "SiLU (140366556103696): torch.Size([2, 480, 14, 14])\n",
      "Conv2dNormActivation (140366556103936): torch.Size([2, 480, 14, 14])\n",
      "Conv2d (140366556547024): torch.Size([2, 480, 14, 14])\n",
      "BatchNorm2d (140366551520656): torch.Size([2, 480, 14, 14])\n",
      "SiLU (140366556102976): torch.Size([2, 480, 14, 14])\n",
      "Conv2dNormActivation (140366556103456): torch.Size([2, 480, 14, 14])\n",
      "AdaptiveAvgPool2d (140366556103216): torch.Size([2, 480, 1, 1])\n",
      "Conv2d (140366556546704): torch.Size([2, 20, 1, 1])\n",
      "SiLU (140366556102736): torch.Size([2, 20, 1, 1])\n",
      "Conv2d (140366556546384): torch.Size([2, 480, 1, 1])\n",
      "Sigmoid (140366556307856): torch.Size([2, 480, 1, 1])\n",
      "SqueezeExcitation (140366556306960): torch.Size([2, 480, 14, 14])\n",
      "Conv2d (140366556546064): torch.Size([2, 80, 14, 14])\n",
      "BatchNorm2d (140366551520384): torch.Size([2, 80, 14, 14])\n",
      "Conv2dNormActivation (140366552101376): torch.Size([2, 80, 14, 14])\n",
      "Sequential (140366556309424): torch.Size([2, 80, 14, 14])\n",
      "StochasticDepth (140366552103536): torch.Size([2, 80, 14, 14])\n",
      "MBConv (140366556104176): torch.Size([2, 80, 14, 14])\n",
      "Conv2d (140366556545744): torch.Size([2, 480, 14, 14])\n",
      "BatchNorm2d (140366551520112): torch.Size([2, 480, 14, 14])\n",
      "SiLU (140366552104496): torch.Size([2, 480, 14, 14])\n",
      "Conv2dNormActivation (140366552104256): torch.Size([2, 480, 14, 14])\n",
      "Conv2d (140366556545424): torch.Size([2, 480, 14, 14])\n",
      "BatchNorm2d (140366551519840): torch.Size([2, 480, 14, 14])\n",
      "SiLU (140366552104016): torch.Size([2, 480, 14, 14])\n",
      "Conv2dNormActivation (140366552103776): torch.Size([2, 480, 14, 14])\n",
      "AdaptiveAvgPool2d (140366552101616): torch.Size([2, 480, 1, 1])\n",
      "Conv2d (140366556545104): torch.Size([2, 20, 1, 1])\n",
      "SiLU (140366552103296): torch.Size([2, 20, 1, 1])\n",
      "Conv2d (140366552334032): torch.Size([2, 480, 1, 1])\n",
      "Sigmoid (140366556313008): torch.Size([2, 480, 1, 1])\n",
      "SqueezeExcitation (140366556312112): torch.Size([2, 480, 14, 14])\n",
      "Conv2d (140366552333712): torch.Size([2, 80, 14, 14])\n",
      "BatchNorm2d (140366551519568): torch.Size([2, 80, 14, 14])\n",
      "Conv2dNormActivation (140366552103056): torch.Size([2, 80, 14, 14])\n",
      "Sequential (140366556314576): torch.Size([2, 80, 14, 14])\n",
      "StochasticDepth (140366552102816): torch.Size([2, 80, 14, 14])\n",
      "MBConv (140366552104736): torch.Size([2, 80, 14, 14])\n",
      "Sequential (140366556315248): torch.Size([2, 80, 14, 14])\n",
      "Conv2d (140366552333392): torch.Size([2, 480, 14, 14])\n",
      "BatchNorm2d (140366551519296): torch.Size([2, 480, 14, 14])\n",
      "SiLU (140366552102336): torch.Size([2, 480, 14, 14])\n",
      "Conv2dNormActivation (140366552101136): torch.Size([2, 480, 14, 14])\n",
      "Conv2d (140366552333072): torch.Size([2, 480, 14, 14])\n",
      "BatchNorm2d (140366551519024): torch.Size([2, 480, 14, 14])\n",
      "SiLU (140366552102096): torch.Size([2, 480, 14, 14])\n",
      "Conv2dNormActivation (140366552101856): torch.Size([2, 480, 14, 14])\n",
      "AdaptiveAvgPool2d (140366547929040): torch.Size([2, 480, 1, 1])\n",
      "Conv2d (140366552332752): torch.Size([2, 20, 1, 1])\n",
      "SiLU (140366547929280): torch.Size([2, 20, 1, 1])\n",
      "Conv2d (140366552332432): torch.Size([2, 480, 1, 1])\n",
      "Sigmoid (140366537067664): torch.Size([2, 480, 1, 1])\n",
      "SqueezeExcitation (140366537066768): torch.Size([2, 480, 14, 14])\n",
      "Conv2d (140366552332112): torch.Size([2, 112, 14, 14])\n",
      "BatchNorm2d (140366551518752): torch.Size([2, 112, 14, 14])\n",
      "Conv2dNormActivation (140366547929520): torch.Size([2, 112, 14, 14])\n",
      "Sequential (140366537069232): torch.Size([2, 112, 14, 14])\n",
      "MBConv (140366552102576): torch.Size([2, 112, 14, 14])\n",
      "Conv2d (140366552331792): torch.Size([2, 672, 14, 14])\n",
      "BatchNorm2d (140366551518480): torch.Size([2, 672, 14, 14])\n",
      "SiLU (140366547930480): torch.Size([2, 672, 14, 14])\n",
      "Conv2dNormActivation (140366547930240): torch.Size([2, 672, 14, 14])\n",
      "Conv2d (140366552331472): torch.Size([2, 672, 14, 14])\n",
      "BatchNorm2d (140366551518208): torch.Size([2, 672, 14, 14])\n",
      "SiLU (140366547930960): torch.Size([2, 672, 14, 14])\n",
      "Conv2dNormActivation (140366547930720): torch.Size([2, 672, 14, 14])\n",
      "AdaptiveAvgPool2d (140366547931200): torch.Size([2, 672, 1, 1])\n",
      "Conv2d (140366552331152): torch.Size([2, 28, 1, 1])\n",
      "SiLU (140366547931440): torch.Size([2, 28, 1, 1])\n",
      "Conv2d (140366552330832): torch.Size([2, 672, 1, 1])\n",
      "Sigmoid (140366537072816): torch.Size([2, 672, 1, 1])\n",
      "SqueezeExcitation (140366537071920): torch.Size([2, 672, 14, 14])\n",
      "Conv2d (140366552330512): torch.Size([2, 112, 14, 14])\n",
      "BatchNorm2d (140366551517936): torch.Size([2, 112, 14, 14])\n",
      "Conv2dNormActivation (140366547931680): torch.Size([2, 112, 14, 14])\n",
      "Sequential (140366537074384): torch.Size([2, 112, 14, 14])\n",
      "StochasticDepth (140366547931920): torch.Size([2, 112, 14, 14])\n",
      "MBConv (140366547930000): torch.Size([2, 112, 14, 14])\n",
      "Conv2d (140366552330192): torch.Size([2, 672, 14, 14])\n",
      "BatchNorm2d (140366551517664): torch.Size([2, 672, 14, 14])\n",
      "SiLU (140366547932640): torch.Size([2, 672, 14, 14])\n",
      "Conv2dNormActivation (140366547932400): torch.Size([2, 672, 14, 14])\n",
      "Conv2d (140366552329872): torch.Size([2, 672, 14, 14])\n",
      "BatchNorm2d (140366551517392): torch.Size([2, 672, 14, 14])\n",
      "SiLU (140366547933120): torch.Size([2, 672, 14, 14])\n",
      "Conv2dNormActivation (140366547932880): torch.Size([2, 672, 14, 14])\n",
      "AdaptiveAvgPool2d (140366547933360): torch.Size([2, 672, 1, 1])\n",
      "Conv2d (140366552329552): torch.Size([2, 28, 1, 1])\n",
      "SiLU (140366547933600): torch.Size([2, 28, 1, 1])\n",
      "Conv2d (140366552329232): torch.Size([2, 672, 1, 1])\n",
      "Sigmoid (140366537077968): torch.Size([2, 672, 1, 1])\n",
      "SqueezeExcitation (140366537077072): torch.Size([2, 672, 14, 14])\n",
      "Conv2d (140366552328912): torch.Size([2, 112, 14, 14])\n",
      "BatchNorm2d (140366551517120): torch.Size([2, 112, 14, 14])\n",
      "Conv2dNormActivation (140366547933840): torch.Size([2, 112, 14, 14])\n",
      "Sequential (140366537079536): torch.Size([2, 112, 14, 14])\n",
      "StochasticDepth (140366547934080): torch.Size([2, 112, 14, 14])\n",
      "MBConv (140366547932160): torch.Size([2, 112, 14, 14])\n",
      "Sequential (140366537080208): torch.Size([2, 112, 14, 14])\n",
      "Conv2d (140366552328592): torch.Size([2, 672, 14, 14])\n",
      "BatchNorm2d (140366551516848): torch.Size([2, 672, 14, 14])\n",
      "SiLU (140366547934800): torch.Size([2, 672, 14, 14])\n",
      "Conv2dNormActivation (140366547934560): torch.Size([2, 672, 14, 14])\n",
      "Conv2d (140366552328272): torch.Size([2, 672, 7, 7])\n",
      "BatchNorm2d (140366551516576): torch.Size([2, 672, 7, 7])\n",
      "SiLU (140366547935280): torch.Size([2, 672, 7, 7])\n",
      "Conv2dNormActivation (140366547935040): torch.Size([2, 672, 7, 7])\n",
      "AdaptiveAvgPool2d (140366547935520): torch.Size([2, 672, 1, 1])\n",
      "Conv2d (140366552327952): torch.Size([2, 28, 1, 1])\n",
      "SiLU (140366547935760): torch.Size([2, 28, 1, 1])\n",
      "Conv2d (140366552327632): torch.Size([2, 672, 1, 1])\n",
      "Sigmoid (140366537214896): torch.Size([2, 672, 1, 1])\n",
      "SqueezeExcitation (140366537214000): torch.Size([2, 672, 7, 7])\n",
      "Conv2d (140366552327312): torch.Size([2, 192, 7, 7])\n",
      "BatchNorm2d (140366551516304): torch.Size([2, 192, 7, 7])\n",
      "Conv2dNormActivation (140366547936000): torch.Size([2, 192, 7, 7])\n",
      "Sequential (140366537216464): torch.Size([2, 192, 7, 7])\n",
      "MBConv (140366547934320): torch.Size([2, 192, 7, 7])\n",
      "Conv2d (140366552326992): torch.Size([2, 1152, 7, 7])\n",
      "BatchNorm2d (140366551516032): torch.Size([2, 1152, 7, 7])\n",
      "SiLU (140366547936960): torch.Size([2, 1152, 7, 7])\n",
      "Conv2dNormActivation (140366547936720): torch.Size([2, 1152, 7, 7])\n",
      "Conv2d (140366552326672): torch.Size([2, 1152, 7, 7])\n",
      "BatchNorm2d (140366551515760): torch.Size([2, 1152, 7, 7])\n",
      "SiLU (140366547937440): torch.Size([2, 1152, 7, 7])\n",
      "Conv2dNormActivation (140366547937200): torch.Size([2, 1152, 7, 7])\n",
      "AdaptiveAvgPool2d (140366547937680): torch.Size([2, 1152, 1, 1])\n",
      "Conv2d (140366552326352): torch.Size([2, 48, 1, 1])\n",
      "SiLU (140366547937920): torch.Size([2, 48, 1, 1])\n",
      "Conv2d (140366552326032): torch.Size([2, 1152, 1, 1])\n",
      "Sigmoid (140366537220048): torch.Size([2, 1152, 1, 1])\n",
      "SqueezeExcitation (140366537219152): torch.Size([2, 1152, 7, 7])\n",
      "Conv2d (140366552325712): torch.Size([2, 192, 7, 7])\n",
      "BatchNorm2d (140366551515488): torch.Size([2, 192, 7, 7])\n",
      "Conv2dNormActivation (140366547938160): torch.Size([2, 192, 7, 7])\n",
      "Sequential (140366537221616): torch.Size([2, 192, 7, 7])\n",
      "StochasticDepth (140366547938400): torch.Size([2, 192, 7, 7])\n",
      "MBConv (140366547936480): torch.Size([2, 192, 7, 7])\n",
      "Conv2d (140366552325392): torch.Size([2, 1152, 7, 7])\n",
      "BatchNorm2d (140366551515216): torch.Size([2, 1152, 7, 7])\n",
      "SiLU (140366547939120): torch.Size([2, 1152, 7, 7])\n",
      "Conv2dNormActivation (140366547938880): torch.Size([2, 1152, 7, 7])\n",
      "Conv2d (140366552325072): torch.Size([2, 1152, 7, 7])\n",
      "BatchNorm2d (140366557429504): torch.Size([2, 1152, 7, 7])\n",
      "SiLU (140366547939600): torch.Size([2, 1152, 7, 7])\n",
      "Conv2dNormActivation (140366547939360): torch.Size([2, 1152, 7, 7])\n",
      "AdaptiveAvgPool2d (140366547939840): torch.Size([2, 1152, 1, 1])\n",
      "Conv2d (140366552324752): torch.Size([2, 48, 1, 1])\n",
      "SiLU (140366547940080): torch.Size([2, 48, 1, 1])\n",
      "Conv2d (140366552324432): torch.Size([2, 1152, 1, 1])\n",
      "Sigmoid (140366537225200): torch.Size([2, 1152, 1, 1])\n",
      "SqueezeExcitation (140366537224304): torch.Size([2, 1152, 7, 7])\n",
      "Conv2d (140366552324112): torch.Size([2, 192, 7, 7])\n",
      "BatchNorm2d (140366557428960): torch.Size([2, 192, 7, 7])\n",
      "Conv2dNormActivation (140366547940320): torch.Size([2, 192, 7, 7])\n",
      "Sequential (140366537226768): torch.Size([2, 192, 7, 7])\n",
      "StochasticDepth (140366547940560): torch.Size([2, 192, 7, 7])\n",
      "MBConv (140366547938640): torch.Size([2, 192, 7, 7])\n",
      "Conv2d (140366552323792): torch.Size([2, 1152, 7, 7])\n",
      "BatchNorm2d (140366557429232): torch.Size([2, 1152, 7, 7])\n",
      "SiLU (140366547941280): torch.Size([2, 1152, 7, 7])\n",
      "Conv2dNormActivation (140366547941040): torch.Size([2, 1152, 7, 7])\n",
      "Conv2d (140366552323472): torch.Size([2, 1152, 7, 7])\n",
      "BatchNorm2d (140366869592224): torch.Size([2, 1152, 7, 7])\n",
      "SiLU (140366547941760): torch.Size([2, 1152, 7, 7])\n",
      "Conv2dNormActivation (140366547941520): torch.Size([2, 1152, 7, 7])\n",
      "AdaptiveAvgPool2d (140366547942000): torch.Size([2, 1152, 1, 1])\n",
      "Conv2d (140366552323152): torch.Size([2, 48, 1, 1])\n",
      "SiLU (140366547942240): torch.Size([2, 48, 1, 1])\n",
      "Conv2d (140366552322832): torch.Size([2, 1152, 1, 1])\n",
      "Sigmoid (140366536902928): torch.Size([2, 1152, 1, 1])\n",
      "SqueezeExcitation (140366536902032): torch.Size([2, 1152, 7, 7])\n",
      "Conv2d (140366552322512): torch.Size([2, 192, 7, 7])\n",
      "BatchNorm2d (140366869592496): torch.Size([2, 192, 7, 7])\n",
      "Conv2dNormActivation (140366547942480): torch.Size([2, 192, 7, 7])\n",
      "Sequential (140366536904496): torch.Size([2, 192, 7, 7])\n",
      "StochasticDepth (140366547942720): torch.Size([2, 192, 7, 7])\n",
      "MBConv (140366547940800): torch.Size([2, 192, 7, 7])\n",
      "Sequential (140366536905168): torch.Size([2, 192, 7, 7])\n",
      "Conv2d (140366552322192): torch.Size([2, 1152, 7, 7])\n",
      "BatchNorm2d (140366869585152): torch.Size([2, 1152, 7, 7])\n",
      "SiLU (140366537195600): torch.Size([2, 1152, 7, 7])\n",
      "Conv2dNormActivation (140366547943200): torch.Size([2, 1152, 7, 7])\n",
      "Conv2d (140366552321872): torch.Size([2, 1152, 7, 7])\n",
      "BatchNorm2d (140366869587600): torch.Size([2, 1152, 7, 7])\n",
      "SiLU (140366537196080): torch.Size([2, 1152, 7, 7])\n",
      "Conv2dNormActivation (140366537195840): torch.Size([2, 1152, 7, 7])\n",
      "AdaptiveAvgPool2d (140366537196320): torch.Size([2, 1152, 1, 1])\n",
      "Conv2d (140366558714832): torch.Size([2, 48, 1, 1])\n",
      "SiLU (140366537196560): torch.Size([2, 48, 1, 1])\n",
      "Conv2d (140366558711952): torch.Size([2, 1152, 1, 1])\n",
      "Sigmoid (140366536908528): torch.Size([2, 1152, 1, 1])\n",
      "SqueezeExcitation (140366536907632): torch.Size([2, 1152, 7, 7])\n",
      "Conv2d (140366558708752): torch.Size([2, 320, 7, 7])\n",
      "BatchNorm2d (140366537745520): torch.Size([2, 320, 7, 7])\n",
      "Conv2dNormActivation (140366537196800): torch.Size([2, 320, 7, 7])\n",
      "Sequential (140366536910096): torch.Size([2, 320, 7, 7])\n",
      "MBConv (140366547942960): torch.Size([2, 320, 7, 7])\n",
      "Sequential (140366536910768): torch.Size([2, 320, 7, 7])\n",
      "Conv2d (140366558714512): torch.Size([2, 1280, 7, 7])\n",
      "BatchNorm2d (140366537745792): torch.Size([2, 1280, 7, 7])\n",
      "SiLU (140366537197520): torch.Size([2, 1280, 7, 7])\n",
      "Conv2dNormActivation (140366537197280): torch.Size([2, 1280, 7, 7])\n",
      "Sequential (140366536912112): torch.Size([2, 1280, 7, 7])\n",
      "AdaptiveAvgPool2d (140366537197760): torch.Size([2, 1280, 1, 1])\n",
      "Dropout (140366558514832): torch.Size([2, 1280])\n",
      "Linear (140366558514528): torch.Size([2, 1000])\n",
      "Sequential (140366536913232): torch.Size([2, 1000])\n",
      "EfficientNet (140366558514224): torch.Size([2, 1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "activation_values = {}  # Stores activation values\n",
    "\n",
    "# 10. Define the evaluation function\n",
    "def evaluate_model(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    i = 0\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for images, labels in tqdm(dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)  \n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)  \n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            print(\"\\n--- Layer Activation Shapes ---\")\n",
    "            for layer, val in activation_values.items():\n",
    "                print(f\"{layer}: {val.shape}\")\n",
    "\n",
    "            if i ==1:\n",
    "                break  \n",
    "            i += 1\n",
    "\n",
    "    accuracy = (correct / total) * 100\n",
    "    return accuracy\n",
    "\n",
    "# 11. Evaluate the model on ImageNet validation set\n",
    "accuracy = evaluate_model(model, imagenet_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_modules(module):\n",
    "\n",
    "    for child in module.named_children():\n",
    "        print_modules(child)\n",
    "\n",
    "print_modules(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (EfficientNet()\n",
      "  features (Sequential()\n",
      "    features.0 (Conv2dNormActivation()\n",
      "      features.0.0 (Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False))\n",
      "      features.0.1 (BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      features.0.2 (SiLU(inplace=True))\n",
      "    features.1 (Sequential()\n",
      "      features.1.0 (MBConv()\n",
      "        features.1.0.block (Sequential()\n",
      "          features.1.0.block.0 (Conv2dNormActivation()\n",
      "            features.1.0.block.0.0 (Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False))\n",
      "            features.1.0.block.0.1 (BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.1.0.block.0.2 (SiLU(inplace=True))\n",
      "          features.1.0.block.1 (SqueezeExcitation()\n",
      "            features.1.0.block.1.avgpool (AdaptiveAvgPool2d(output_size=1))\n",
      "            features.1.0.block.1.fc1 (Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.1.0.block.1.fc2 (Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.1.0.block.1.activation (SiLU(inplace=True))\n",
      "            features.1.0.block.1.scale_activation (Sigmoid())\n",
      "          features.1.0.block.2 (Conv2dNormActivation()\n",
      "            features.1.0.block.2.0 (Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.1.0.block.2.1 (BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "        features.1.0.stochastic_depth (StochasticDepth(p=0.0, mode=row))\n",
      "    features.2 (Sequential()\n",
      "      features.2.0 (MBConv()\n",
      "        features.2.0.block (Sequential()\n",
      "          features.2.0.block.0 (Conv2dNormActivation()\n",
      "            features.2.0.block.0.0 (Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.2.0.block.0.1 (BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.2.0.block.0.2 (SiLU(inplace=True))\n",
      "          features.2.0.block.1 (Conv2dNormActivation()\n",
      "            features.2.0.block.1.0 (Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False))\n",
      "            features.2.0.block.1.1 (BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.2.0.block.1.2 (SiLU(inplace=True))\n",
      "          features.2.0.block.2 (SqueezeExcitation()\n",
      "            features.2.0.block.2.avgpool (AdaptiveAvgPool2d(output_size=1))\n",
      "            features.2.0.block.2.fc1 (Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.2.0.block.2.fc2 (Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.2.0.block.2.activation (SiLU(inplace=True))\n",
      "            features.2.0.block.2.scale_activation (Sigmoid())\n",
      "          features.2.0.block.3 (Conv2dNormActivation()\n",
      "            features.2.0.block.3.0 (Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.2.0.block.3.1 (BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "        features.2.0.stochastic_depth (StochasticDepth(p=0.0125, mode=row))\n",
      "      features.2.1 (MBConv()\n",
      "        features.2.1.block (Sequential()\n",
      "          features.2.1.block.0 (Conv2dNormActivation()\n",
      "            features.2.1.block.0.0 (Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.2.1.block.0.1 (BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.2.1.block.0.2 (SiLU(inplace=True))\n",
      "          features.2.1.block.1 (Conv2dNormActivation()\n",
      "            features.2.1.block.1.0 (Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False))\n",
      "            features.2.1.block.1.1 (BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.2.1.block.1.2 (SiLU(inplace=True))\n",
      "          features.2.1.block.2 (SqueezeExcitation()\n",
      "            features.2.1.block.2.avgpool (AdaptiveAvgPool2d(output_size=1))\n",
      "            features.2.1.block.2.fc1 (Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.2.1.block.2.fc2 (Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.2.1.block.2.activation (SiLU(inplace=True))\n",
      "            features.2.1.block.2.scale_activation (Sigmoid())\n",
      "          features.2.1.block.3 (Conv2dNormActivation()\n",
      "            features.2.1.block.3.0 (Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.2.1.block.3.1 (BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "        features.2.1.stochastic_depth (StochasticDepth(p=0.025, mode=row))\n",
      "    features.3 (Sequential()\n",
      "      features.3.0 (MBConv()\n",
      "        features.3.0.block (Sequential()\n",
      "          features.3.0.block.0 (Conv2dNormActivation()\n",
      "            features.3.0.block.0.0 (Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.3.0.block.0.1 (BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.3.0.block.0.2 (SiLU(inplace=True))\n",
      "          features.3.0.block.1 (Conv2dNormActivation()\n",
      "            features.3.0.block.1.0 (Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False))\n",
      "            features.3.0.block.1.1 (BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.3.0.block.1.2 (SiLU(inplace=True))\n",
      "          features.3.0.block.2 (SqueezeExcitation()\n",
      "            features.3.0.block.2.avgpool (AdaptiveAvgPool2d(output_size=1))\n",
      "            features.3.0.block.2.fc1 (Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.3.0.block.2.fc2 (Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.3.0.block.2.activation (SiLU(inplace=True))\n",
      "            features.3.0.block.2.scale_activation (Sigmoid())\n",
      "          features.3.0.block.3 (Conv2dNormActivation()\n",
      "            features.3.0.block.3.0 (Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.3.0.block.3.1 (BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "        features.3.0.stochastic_depth (StochasticDepth(p=0.037500000000000006, mode=row))\n",
      "      features.3.1 (MBConv()\n",
      "        features.3.1.block (Sequential()\n",
      "          features.3.1.block.0 (Conv2dNormActivation()\n",
      "            features.3.1.block.0.0 (Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.3.1.block.0.1 (BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.3.1.block.0.2 (SiLU(inplace=True))\n",
      "          features.3.1.block.1 (Conv2dNormActivation()\n",
      "            features.3.1.block.1.0 (Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False))\n",
      "            features.3.1.block.1.1 (BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.3.1.block.1.2 (SiLU(inplace=True))\n",
      "          features.3.1.block.2 (SqueezeExcitation()\n",
      "            features.3.1.block.2.avgpool (AdaptiveAvgPool2d(output_size=1))\n",
      "            features.3.1.block.2.fc1 (Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.3.1.block.2.fc2 (Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.3.1.block.2.activation (SiLU(inplace=True))\n",
      "            features.3.1.block.2.scale_activation (Sigmoid())\n",
      "          features.3.1.block.3 (Conv2dNormActivation()\n",
      "            features.3.1.block.3.0 (Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.3.1.block.3.1 (BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "        features.3.1.stochastic_depth (StochasticDepth(p=0.05, mode=row))\n",
      "    features.4 (Sequential()\n",
      "      features.4.0 (MBConv()\n",
      "        features.4.0.block (Sequential()\n",
      "          features.4.0.block.0 (Conv2dNormActivation()\n",
      "            features.4.0.block.0.0 (Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.4.0.block.0.1 (BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.4.0.block.0.2 (SiLU(inplace=True))\n",
      "          features.4.0.block.1 (Conv2dNormActivation()\n",
      "            features.4.0.block.1.0 (Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False))\n",
      "            features.4.0.block.1.1 (BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.4.0.block.1.2 (SiLU(inplace=True))\n",
      "          features.4.0.block.2 (SqueezeExcitation()\n",
      "            features.4.0.block.2.avgpool (AdaptiveAvgPool2d(output_size=1))\n",
      "            features.4.0.block.2.fc1 (Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.4.0.block.2.fc2 (Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.4.0.block.2.activation (SiLU(inplace=True))\n",
      "            features.4.0.block.2.scale_activation (Sigmoid())\n",
      "          features.4.0.block.3 (Conv2dNormActivation()\n",
      "            features.4.0.block.3.0 (Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.4.0.block.3.1 (BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "        features.4.0.stochastic_depth (StochasticDepth(p=0.0625, mode=row))\n",
      "      features.4.1 (MBConv()\n",
      "        features.4.1.block (Sequential()\n",
      "          features.4.1.block.0 (Conv2dNormActivation()\n",
      "            features.4.1.block.0.0 (Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.4.1.block.0.1 (BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.4.1.block.0.2 (SiLU(inplace=True))\n",
      "          features.4.1.block.1 (Conv2dNormActivation()\n",
      "            features.4.1.block.1.0 (Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False))\n",
      "            features.4.1.block.1.1 (BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.4.1.block.1.2 (SiLU(inplace=True))\n",
      "          features.4.1.block.2 (SqueezeExcitation()\n",
      "            features.4.1.block.2.avgpool (AdaptiveAvgPool2d(output_size=1))\n",
      "            features.4.1.block.2.fc1 (Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.4.1.block.2.fc2 (Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.4.1.block.2.activation (SiLU(inplace=True))\n",
      "            features.4.1.block.2.scale_activation (Sigmoid())\n",
      "          features.4.1.block.3 (Conv2dNormActivation()\n",
      "            features.4.1.block.3.0 (Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.4.1.block.3.1 (BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "        features.4.1.stochastic_depth (StochasticDepth(p=0.07500000000000001, mode=row))\n",
      "      features.4.2 (MBConv()\n",
      "        features.4.2.block (Sequential()\n",
      "          features.4.2.block.0 (Conv2dNormActivation()\n",
      "            features.4.2.block.0.0 (Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.4.2.block.0.1 (BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.4.2.block.0.2 (SiLU(inplace=True))\n",
      "          features.4.2.block.1 (Conv2dNormActivation()\n",
      "            features.4.2.block.1.0 (Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False))\n",
      "            features.4.2.block.1.1 (BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.4.2.block.1.2 (SiLU(inplace=True))\n",
      "          features.4.2.block.2 (SqueezeExcitation()\n",
      "            features.4.2.block.2.avgpool (AdaptiveAvgPool2d(output_size=1))\n",
      "            features.4.2.block.2.fc1 (Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.4.2.block.2.fc2 (Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.4.2.block.2.activation (SiLU(inplace=True))\n",
      "            features.4.2.block.2.scale_activation (Sigmoid())\n",
      "          features.4.2.block.3 (Conv2dNormActivation()\n",
      "            features.4.2.block.3.0 (Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.4.2.block.3.1 (BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "        features.4.2.stochastic_depth (StochasticDepth(p=0.08750000000000001, mode=row))\n",
      "    features.5 (Sequential()\n",
      "      features.5.0 (MBConv()\n",
      "        features.5.0.block (Sequential()\n",
      "          features.5.0.block.0 (Conv2dNormActivation()\n",
      "            features.5.0.block.0.0 (Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.5.0.block.0.1 (BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.5.0.block.0.2 (SiLU(inplace=True))\n",
      "          features.5.0.block.1 (Conv2dNormActivation()\n",
      "            features.5.0.block.1.0 (Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False))\n",
      "            features.5.0.block.1.1 (BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.5.0.block.1.2 (SiLU(inplace=True))\n",
      "          features.5.0.block.2 (SqueezeExcitation()\n",
      "            features.5.0.block.2.avgpool (AdaptiveAvgPool2d(output_size=1))\n",
      "            features.5.0.block.2.fc1 (Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.5.0.block.2.fc2 (Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.5.0.block.2.activation (SiLU(inplace=True))\n",
      "            features.5.0.block.2.scale_activation (Sigmoid())\n",
      "          features.5.0.block.3 (Conv2dNormActivation()\n",
      "            features.5.0.block.3.0 (Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.5.0.block.3.1 (BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "        features.5.0.stochastic_depth (StochasticDepth(p=0.1, mode=row))\n",
      "      features.5.1 (MBConv()\n",
      "        features.5.1.block (Sequential()\n",
      "          features.5.1.block.0 (Conv2dNormActivation()\n",
      "            features.5.1.block.0.0 (Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.5.1.block.0.1 (BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.5.1.block.0.2 (SiLU(inplace=True))\n",
      "          features.5.1.block.1 (Conv2dNormActivation()\n",
      "            features.5.1.block.1.0 (Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False))\n",
      "            features.5.1.block.1.1 (BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.5.1.block.1.2 (SiLU(inplace=True))\n",
      "          features.5.1.block.2 (SqueezeExcitation()\n",
      "            features.5.1.block.2.avgpool (AdaptiveAvgPool2d(output_size=1))\n",
      "            features.5.1.block.2.fc1 (Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.5.1.block.2.fc2 (Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.5.1.block.2.activation (SiLU(inplace=True))\n",
      "            features.5.1.block.2.scale_activation (Sigmoid())\n",
      "          features.5.1.block.3 (Conv2dNormActivation()\n",
      "            features.5.1.block.3.0 (Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.5.1.block.3.1 (BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "        features.5.1.stochastic_depth (StochasticDepth(p=0.1125, mode=row))\n",
      "      features.5.2 (MBConv()\n",
      "        features.5.2.block (Sequential()\n",
      "          features.5.2.block.0 (Conv2dNormActivation()\n",
      "            features.5.2.block.0.0 (Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.5.2.block.0.1 (BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.5.2.block.0.2 (SiLU(inplace=True))\n",
      "          features.5.2.block.1 (Conv2dNormActivation()\n",
      "            features.5.2.block.1.0 (Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False))\n",
      "            features.5.2.block.1.1 (BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.5.2.block.1.2 (SiLU(inplace=True))\n",
      "          features.5.2.block.2 (SqueezeExcitation()\n",
      "            features.5.2.block.2.avgpool (AdaptiveAvgPool2d(output_size=1))\n",
      "            features.5.2.block.2.fc1 (Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.5.2.block.2.fc2 (Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.5.2.block.2.activation (SiLU(inplace=True))\n",
      "            features.5.2.block.2.scale_activation (Sigmoid())\n",
      "          features.5.2.block.3 (Conv2dNormActivation()\n",
      "            features.5.2.block.3.0 (Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.5.2.block.3.1 (BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "        features.5.2.stochastic_depth (StochasticDepth(p=0.125, mode=row))\n",
      "    features.6 (Sequential()\n",
      "      features.6.0 (MBConv()\n",
      "        features.6.0.block (Sequential()\n",
      "          features.6.0.block.0 (Conv2dNormActivation()\n",
      "            features.6.0.block.0.0 (Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.6.0.block.0.1 (BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.6.0.block.0.2 (SiLU(inplace=True))\n",
      "          features.6.0.block.1 (Conv2dNormActivation()\n",
      "            features.6.0.block.1.0 (Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False))\n",
      "            features.6.0.block.1.1 (BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.6.0.block.1.2 (SiLU(inplace=True))\n",
      "          features.6.0.block.2 (SqueezeExcitation()\n",
      "            features.6.0.block.2.avgpool (AdaptiveAvgPool2d(output_size=1))\n",
      "            features.6.0.block.2.fc1 (Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.6.0.block.2.fc2 (Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.6.0.block.2.activation (SiLU(inplace=True))\n",
      "            features.6.0.block.2.scale_activation (Sigmoid())\n",
      "          features.6.0.block.3 (Conv2dNormActivation()\n",
      "            features.6.0.block.3.0 (Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.6.0.block.3.1 (BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "        features.6.0.stochastic_depth (StochasticDepth(p=0.1375, mode=row))\n",
      "      features.6.1 (MBConv()\n",
      "        features.6.1.block (Sequential()\n",
      "          features.6.1.block.0 (Conv2dNormActivation()\n",
      "            features.6.1.block.0.0 (Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.6.1.block.0.1 (BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.6.1.block.0.2 (SiLU(inplace=True))\n",
      "          features.6.1.block.1 (Conv2dNormActivation()\n",
      "            features.6.1.block.1.0 (Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False))\n",
      "            features.6.1.block.1.1 (BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.6.1.block.1.2 (SiLU(inplace=True))\n",
      "          features.6.1.block.2 (SqueezeExcitation()\n",
      "            features.6.1.block.2.avgpool (AdaptiveAvgPool2d(output_size=1))\n",
      "            features.6.1.block.2.fc1 (Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.6.1.block.2.fc2 (Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.6.1.block.2.activation (SiLU(inplace=True))\n",
      "            features.6.1.block.2.scale_activation (Sigmoid())\n",
      "          features.6.1.block.3 (Conv2dNormActivation()\n",
      "            features.6.1.block.3.0 (Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.6.1.block.3.1 (BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "        features.6.1.stochastic_depth (StochasticDepth(p=0.15000000000000002, mode=row))\n",
      "      features.6.2 (MBConv()\n",
      "        features.6.2.block (Sequential()\n",
      "          features.6.2.block.0 (Conv2dNormActivation()\n",
      "            features.6.2.block.0.0 (Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.6.2.block.0.1 (BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.6.2.block.0.2 (SiLU(inplace=True))\n",
      "          features.6.2.block.1 (Conv2dNormActivation()\n",
      "            features.6.2.block.1.0 (Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False))\n",
      "            features.6.2.block.1.1 (BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.6.2.block.1.2 (SiLU(inplace=True))\n",
      "          features.6.2.block.2 (SqueezeExcitation()\n",
      "            features.6.2.block.2.avgpool (AdaptiveAvgPool2d(output_size=1))\n",
      "            features.6.2.block.2.fc1 (Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.6.2.block.2.fc2 (Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.6.2.block.2.activation (SiLU(inplace=True))\n",
      "            features.6.2.block.2.scale_activation (Sigmoid())\n",
      "          features.6.2.block.3 (Conv2dNormActivation()\n",
      "            features.6.2.block.3.0 (Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.6.2.block.3.1 (BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "        features.6.2.stochastic_depth (StochasticDepth(p=0.1625, mode=row))\n",
      "      features.6.3 (MBConv()\n",
      "        features.6.3.block (Sequential()\n",
      "          features.6.3.block.0 (Conv2dNormActivation()\n",
      "            features.6.3.block.0.0 (Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.6.3.block.0.1 (BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.6.3.block.0.2 (SiLU(inplace=True))\n",
      "          features.6.3.block.1 (Conv2dNormActivation()\n",
      "            features.6.3.block.1.0 (Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False))\n",
      "            features.6.3.block.1.1 (BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.6.3.block.1.2 (SiLU(inplace=True))\n",
      "          features.6.3.block.2 (SqueezeExcitation()\n",
      "            features.6.3.block.2.avgpool (AdaptiveAvgPool2d(output_size=1))\n",
      "            features.6.3.block.2.fc1 (Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.6.3.block.2.fc2 (Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.6.3.block.2.activation (SiLU(inplace=True))\n",
      "            features.6.3.block.2.scale_activation (Sigmoid())\n",
      "          features.6.3.block.3 (Conv2dNormActivation()\n",
      "            features.6.3.block.3.0 (Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.6.3.block.3.1 (BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "        features.6.3.stochastic_depth (StochasticDepth(p=0.17500000000000002, mode=row))\n",
      "    features.7 (Sequential()\n",
      "      features.7.0 (MBConv()\n",
      "        features.7.0.block (Sequential()\n",
      "          features.7.0.block.0 (Conv2dNormActivation()\n",
      "            features.7.0.block.0.0 (Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.7.0.block.0.1 (BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.7.0.block.0.2 (SiLU(inplace=True))\n",
      "          features.7.0.block.1 (Conv2dNormActivation()\n",
      "            features.7.0.block.1.0 (Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False))\n",
      "            features.7.0.block.1.1 (BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "            features.7.0.block.1.2 (SiLU(inplace=True))\n",
      "          features.7.0.block.2 (SqueezeExcitation()\n",
      "            features.7.0.block.2.avgpool (AdaptiveAvgPool2d(output_size=1))\n",
      "            features.7.0.block.2.fc1 (Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.7.0.block.2.fc2 (Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1)))\n",
      "            features.7.0.block.2.activation (SiLU(inplace=True))\n",
      "            features.7.0.block.2.scale_activation (Sigmoid())\n",
      "          features.7.0.block.3 (Conv2dNormActivation()\n",
      "            features.7.0.block.3.0 (Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "            features.7.0.block.3.1 (BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "        features.7.0.stochastic_depth (StochasticDepth(p=0.1875, mode=row))\n",
      "    features.8 (Conv2dNormActivation()\n",
      "      features.8.0 (Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      "      features.8.1 (BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      features.8.2 (SiLU(inplace=True))\n",
      "  avgpool (AdaptiveAvgPool2d(output_size=1))\n",
      "  classifier (Sequential()\n",
      "    classifier.0 (Dropout(p=0.2, inplace=True))\n",
      "    classifier.1 (Linear(in_features=1280, out_features=1000, bias=True))\n"
     ]
    }
   ],
   "source": [
    "def print_modules(module, name=\"\", indent=0):\n",
    "    indent_str = \"  \" * indent\n",
    "    module_info = str(module).split(\"\\n\")[0]  # Get only the first line of module info\n",
    "\n",
    "    print(f\"{indent_str}{name} ({module_info})\")  # Print module with full info\n",
    "\n",
    "    # Recursively print child modules\n",
    "    for child_name, child in module.named_children():\n",
    "        full_child_name = f\"{name}.{child_name}\" if name else child_name\n",
    "        print_modules(child, full_child_name, indent=indent + 1)\n",
    "\n",
    "print_modules(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_conv2d_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 112, 112])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_shapes['Conv2d (134778391459024)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "features\n",
      "features.0\n",
      "features.0.0\n",
      "features.0.1\n",
      "features.0.2\n",
      "features.1\n",
      "features.1.0\n",
      "features.1.0.block\n",
      "features.1.0.block.0\n",
      "features.1.0.block.0.0\n",
      "features.1.0.block.0.1\n",
      "features.1.0.block.0.2\n",
      "features.1.0.block.1\n",
      "features.1.0.block.1.avgpool\n",
      "features.1.0.block.1.fc1\n",
      "features.1.0.block.1.fc2\n",
      "features.1.0.block.1.activation\n",
      "features.1.0.block.1.scale_activation\n",
      "features.1.0.block.2\n",
      "features.1.0.block.2.0\n",
      "features.1.0.block.2.1\n",
      "features.1.0.stochastic_depth\n",
      "features.2\n",
      "features.2.0\n",
      "features.2.0.block\n",
      "features.2.0.block.0\n",
      "features.2.0.block.0.0\n",
      "features.2.0.block.0.1\n",
      "features.2.0.block.0.2\n",
      "features.2.0.block.1\n",
      "features.2.0.block.1.0\n",
      "features.2.0.block.1.1\n",
      "features.2.0.block.1.2\n",
      "features.2.0.block.2\n",
      "features.2.0.block.2.avgpool\n",
      "features.2.0.block.2.fc1\n",
      "features.2.0.block.2.fc2\n",
      "features.2.0.block.2.activation\n",
      "features.2.0.block.2.scale_activation\n",
      "features.2.0.block.3\n",
      "features.2.0.block.3.0\n",
      "features.2.0.block.3.1\n",
      "features.2.0.stochastic_depth\n",
      "features.2.1\n",
      "features.2.1.block\n",
      "features.2.1.block.0\n",
      "features.2.1.block.0.0\n",
      "features.2.1.block.0.1\n",
      "features.2.1.block.0.2\n",
      "features.2.1.block.1\n",
      "features.2.1.block.1.0\n",
      "features.2.1.block.1.1\n",
      "features.2.1.block.1.2\n",
      "features.2.1.block.2\n",
      "features.2.1.block.2.avgpool\n",
      "features.2.1.block.2.fc1\n",
      "features.2.1.block.2.fc2\n",
      "features.2.1.block.2.activation\n",
      "features.2.1.block.2.scale_activation\n",
      "features.2.1.block.3\n",
      "features.2.1.block.3.0\n",
      "features.2.1.block.3.1\n",
      "features.2.1.stochastic_depth\n",
      "features.3\n",
      "features.3.0\n",
      "features.3.0.block\n",
      "features.3.0.block.0\n",
      "features.3.0.block.0.0\n",
      "features.3.0.block.0.1\n",
      "features.3.0.block.0.2\n",
      "features.3.0.block.1\n",
      "features.3.0.block.1.0\n",
      "features.3.0.block.1.1\n",
      "features.3.0.block.1.2\n",
      "features.3.0.block.2\n",
      "features.3.0.block.2.avgpool\n",
      "features.3.0.block.2.fc1\n",
      "features.3.0.block.2.fc2\n",
      "features.3.0.block.2.activation\n",
      "features.3.0.block.2.scale_activation\n",
      "features.3.0.block.3\n",
      "features.3.0.block.3.0\n",
      "features.3.0.block.3.1\n",
      "features.3.0.stochastic_depth\n",
      "features.3.1\n",
      "features.3.1.block\n",
      "features.3.1.block.0\n",
      "features.3.1.block.0.0\n",
      "features.3.1.block.0.1\n",
      "features.3.1.block.0.2\n",
      "features.3.1.block.1\n",
      "features.3.1.block.1.0\n",
      "features.3.1.block.1.1\n",
      "features.3.1.block.1.2\n",
      "features.3.1.block.2\n",
      "features.3.1.block.2.avgpool\n",
      "features.3.1.block.2.fc1\n",
      "features.3.1.block.2.fc2\n",
      "features.3.1.block.2.activation\n",
      "features.3.1.block.2.scale_activation\n",
      "features.3.1.block.3\n",
      "features.3.1.block.3.0\n",
      "features.3.1.block.3.1\n",
      "features.3.1.stochastic_depth\n",
      "features.4\n",
      "features.4.0\n",
      "features.4.0.block\n",
      "features.4.0.block.0\n",
      "features.4.0.block.0.0\n",
      "features.4.0.block.0.1\n",
      "features.4.0.block.0.2\n",
      "features.4.0.block.1\n",
      "features.4.0.block.1.0\n",
      "features.4.0.block.1.1\n",
      "features.4.0.block.1.2\n",
      "features.4.0.block.2\n",
      "features.4.0.block.2.avgpool\n",
      "features.4.0.block.2.fc1\n",
      "features.4.0.block.2.fc2\n",
      "features.4.0.block.2.activation\n",
      "features.4.0.block.2.scale_activation\n",
      "features.4.0.block.3\n",
      "features.4.0.block.3.0\n",
      "features.4.0.block.3.1\n",
      "features.4.0.stochastic_depth\n",
      "features.4.1\n",
      "features.4.1.block\n",
      "features.4.1.block.0\n",
      "features.4.1.block.0.0\n",
      "features.4.1.block.0.1\n",
      "features.4.1.block.0.2\n",
      "features.4.1.block.1\n",
      "features.4.1.block.1.0\n",
      "features.4.1.block.1.1\n",
      "features.4.1.block.1.2\n",
      "features.4.1.block.2\n",
      "features.4.1.block.2.avgpool\n",
      "features.4.1.block.2.fc1\n",
      "features.4.1.block.2.fc2\n",
      "features.4.1.block.2.activation\n",
      "features.4.1.block.2.scale_activation\n",
      "features.4.1.block.3\n",
      "features.4.1.block.3.0\n",
      "features.4.1.block.3.1\n",
      "features.4.1.stochastic_depth\n",
      "features.4.2\n",
      "features.4.2.block\n",
      "features.4.2.block.0\n",
      "features.4.2.block.0.0\n",
      "features.4.2.block.0.1\n",
      "features.4.2.block.0.2\n",
      "features.4.2.block.1\n",
      "features.4.2.block.1.0\n",
      "features.4.2.block.1.1\n",
      "features.4.2.block.1.2\n",
      "features.4.2.block.2\n",
      "features.4.2.block.2.avgpool\n",
      "features.4.2.block.2.fc1\n",
      "features.4.2.block.2.fc2\n",
      "features.4.2.block.2.activation\n",
      "features.4.2.block.2.scale_activation\n",
      "features.4.2.block.3\n",
      "features.4.2.block.3.0\n",
      "features.4.2.block.3.1\n",
      "features.4.2.stochastic_depth\n",
      "features.5\n",
      "features.5.0\n",
      "features.5.0.block\n",
      "features.5.0.block.0\n",
      "features.5.0.block.0.0\n",
      "features.5.0.block.0.1\n",
      "features.5.0.block.0.2\n",
      "features.5.0.block.1\n",
      "features.5.0.block.1.0\n",
      "features.5.0.block.1.1\n",
      "features.5.0.block.1.2\n",
      "features.5.0.block.2\n",
      "features.5.0.block.2.avgpool\n",
      "features.5.0.block.2.fc1\n",
      "features.5.0.block.2.fc2\n",
      "features.5.0.block.2.activation\n",
      "features.5.0.block.2.scale_activation\n",
      "features.5.0.block.3\n",
      "features.5.0.block.3.0\n",
      "features.5.0.block.3.1\n",
      "features.5.0.stochastic_depth\n",
      "features.5.1\n",
      "features.5.1.block\n",
      "features.5.1.block.0\n",
      "features.5.1.block.0.0\n",
      "features.5.1.block.0.1\n",
      "features.5.1.block.0.2\n",
      "features.5.1.block.1\n",
      "features.5.1.block.1.0\n",
      "features.5.1.block.1.1\n",
      "features.5.1.block.1.2\n",
      "features.5.1.block.2\n",
      "features.5.1.block.2.avgpool\n",
      "features.5.1.block.2.fc1\n",
      "features.5.1.block.2.fc2\n",
      "features.5.1.block.2.activation\n",
      "features.5.1.block.2.scale_activation\n",
      "features.5.1.block.3\n",
      "features.5.1.block.3.0\n",
      "features.5.1.block.3.1\n",
      "features.5.1.stochastic_depth\n",
      "features.5.2\n",
      "features.5.2.block\n",
      "features.5.2.block.0\n",
      "features.5.2.block.0.0\n",
      "features.5.2.block.0.1\n",
      "features.5.2.block.0.2\n",
      "features.5.2.block.1\n",
      "features.5.2.block.1.0\n",
      "features.5.2.block.1.1\n",
      "features.5.2.block.1.2\n",
      "features.5.2.block.2\n",
      "features.5.2.block.2.avgpool\n",
      "features.5.2.block.2.fc1\n",
      "features.5.2.block.2.fc2\n",
      "features.5.2.block.2.activation\n",
      "features.5.2.block.2.scale_activation\n",
      "features.5.2.block.3\n",
      "features.5.2.block.3.0\n",
      "features.5.2.block.3.1\n",
      "features.5.2.stochastic_depth\n",
      "features.6\n",
      "features.6.0\n",
      "features.6.0.block\n",
      "features.6.0.block.0\n",
      "features.6.0.block.0.0\n",
      "features.6.0.block.0.1\n",
      "features.6.0.block.0.2\n",
      "features.6.0.block.1\n",
      "features.6.0.block.1.0\n",
      "features.6.0.block.1.1\n",
      "features.6.0.block.1.2\n",
      "features.6.0.block.2\n",
      "features.6.0.block.2.avgpool\n",
      "features.6.0.block.2.fc1\n",
      "features.6.0.block.2.fc2\n",
      "features.6.0.block.2.activation\n",
      "features.6.0.block.2.scale_activation\n",
      "features.6.0.block.3\n",
      "features.6.0.block.3.0\n",
      "features.6.0.block.3.1\n",
      "features.6.0.stochastic_depth\n",
      "features.6.1\n",
      "features.6.1.block\n",
      "features.6.1.block.0\n",
      "features.6.1.block.0.0\n",
      "features.6.1.block.0.1\n",
      "features.6.1.block.0.2\n",
      "features.6.1.block.1\n",
      "features.6.1.block.1.0\n",
      "features.6.1.block.1.1\n",
      "features.6.1.block.1.2\n",
      "features.6.1.block.2\n",
      "features.6.1.block.2.avgpool\n",
      "features.6.1.block.2.fc1\n",
      "features.6.1.block.2.fc2\n",
      "features.6.1.block.2.activation\n",
      "features.6.1.block.2.scale_activation\n",
      "features.6.1.block.3\n",
      "features.6.1.block.3.0\n",
      "features.6.1.block.3.1\n",
      "features.6.1.stochastic_depth\n",
      "features.6.2\n",
      "features.6.2.block\n",
      "features.6.2.block.0\n",
      "features.6.2.block.0.0\n",
      "features.6.2.block.0.1\n",
      "features.6.2.block.0.2\n",
      "features.6.2.block.1\n",
      "features.6.2.block.1.0\n",
      "features.6.2.block.1.1\n",
      "features.6.2.block.1.2\n",
      "features.6.2.block.2\n",
      "features.6.2.block.2.avgpool\n",
      "features.6.2.block.2.fc1\n",
      "features.6.2.block.2.fc2\n",
      "features.6.2.block.2.activation\n",
      "features.6.2.block.2.scale_activation\n",
      "features.6.2.block.3\n",
      "features.6.2.block.3.0\n",
      "features.6.2.block.3.1\n",
      "features.6.2.stochastic_depth\n",
      "features.6.3\n",
      "features.6.3.block\n",
      "features.6.3.block.0\n",
      "features.6.3.block.0.0\n",
      "features.6.3.block.0.1\n",
      "features.6.3.block.0.2\n",
      "features.6.3.block.1\n",
      "features.6.3.block.1.0\n",
      "features.6.3.block.1.1\n",
      "features.6.3.block.1.2\n",
      "features.6.3.block.2\n",
      "features.6.3.block.2.avgpool\n",
      "features.6.3.block.2.fc1\n",
      "features.6.3.block.2.fc2\n",
      "features.6.3.block.2.activation\n",
      "features.6.3.block.2.scale_activation\n",
      "features.6.3.block.3\n",
      "features.6.3.block.3.0\n",
      "features.6.3.block.3.1\n",
      "features.6.3.stochastic_depth\n",
      "features.7\n",
      "features.7.0\n",
      "features.7.0.block\n",
      "features.7.0.block.0\n",
      "features.7.0.block.0.0\n",
      "features.7.0.block.0.1\n",
      "features.7.0.block.0.2\n",
      "features.7.0.block.1\n",
      "features.7.0.block.1.0\n",
      "features.7.0.block.1.1\n",
      "features.7.0.block.1.2\n",
      "features.7.0.block.2\n",
      "features.7.0.block.2.avgpool\n",
      "features.7.0.block.2.fc1\n",
      "features.7.0.block.2.fc2\n",
      "features.7.0.block.2.activation\n",
      "features.7.0.block.2.scale_activation\n",
      "features.7.0.block.3\n",
      "features.7.0.block.3.0\n",
      "features.7.0.block.3.1\n",
      "features.7.0.stochastic_depth\n",
      "features.8\n",
      "features.8.0\n",
      "features.8.1\n",
      "features.8.2\n",
      "avgpool\n",
      "classifier\n",
      "classifier.0\n",
      "classifier.1\n"
     ]
    }
   ],
   "source": [
    "for name, layer in model.named_modules():\n",
    "    print(name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for name, layer in model.named_modules():\n",
    "    print(name)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (8): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=True)\n",
       "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 3, 3])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features[0][0].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1216,  0.6563,  0.4567],\n",
       "        [-0.1109, -0.6100, -0.3334],\n",
       "        [ 0.0280, -0.1031, -0.1032]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features[0][0].weight[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
